{"posts":[{"title":"kind","text":"kind 는 Docker 컨테이너 노드를 사용하여 로컬 쿠버네티스 클러스터를 실행하기 위한 도구이다. 테스트환경: m1 아래는 kind의 k8s가 DIND 구조로 동작함을 도식화 한 것 kind 및 툴 설치 1234567# Install Kindbrew install kindkind --version# Install kubectlbrew install kubernetes-clikubectl version --client=true kind 기본사용 cluster 생성 (control-plane 1ea) 12345678910111213141516171819202122232425262728293031323334353637# 클러스터 배포 전 확인docker ps# Create a clusterkind create cluster# 클러스터 배포 확인kind get clusters# 컨트롤플레인 노드 1개 확인kind get nodeskubectl cluster-info# 노드 정보 확인kubectl get node -o wide# 파드 정보 확인kubectl get pod -A# controller-manager, scheduler, etcd-0kubectl get componentstatuses# kube config 파일 확인cat ~/.kube/config# nginx 파드 배포 및 확인kubectl run nginx --image=nginx:alpinekubectl get pod -o wide# 노드에 Taints 정보 확인kubectl describe node | grep TaintsTaints: &lt;none&gt;# 클러스터 삭제kind delete cluster# kube config 삭제 확인cat ~/.kube/config cluster 생성 (control-plane, worker node) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 클러스터 배포 전 확인docker ps# kind 는 별도 도커 네트워크 생성 후 사용 : 기본값 172.18.0.0/16docker network lsdocker inspect kind | jq# Create a cluster with kindcat &lt;&lt; EOT &gt; kind-2node.yaml # two node (one workers) cluster configkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane- role: workerEOTkind create cluster --config kind-2node.yaml --name myk8s# 확인kind get nodes --name myk8s# k8s api 주소 확인 : 어떻게 로컬에서 접속이 되는 걸까?kubectl cluster-infodocker ps # 포트 포워딩 정보 확인docker exec -it myk8s-control-plane ss -tnlp | grep 6443kubectl get pod -n kube-system -l component=kube-apiserver -owide # 파드 IP 확인kubectl describe pod -n kube-system -l component=kube-apiserverdocker exec -it myk8s-control-plane curl -k https://localhost:6443/livez ;echodocker exec -it myk8s-control-plane curl -k https://localhost:6443/readyz ;echo# 노드 정보 확인 : CRI 는 containerd 사용kubectl get node -o wide# 파드 정보 확인 : CNI 는 kindnet 사용kubectl get pod -A -owide# 디버그용 내용 출력에 ~/.kube/config 권한 인증 로드kubectl get pod -v6# kube config 파일 확인cat ~/.kube/config# local-path 라는 StorageClass 가 설치, local-path 는 노드의 로컬 저장소를 활용함# 로컬 호스트의 path 를 지정할 필요 없이 local-path provisioner 이 볼륨을 관리kubectl get sckubectl get deploy -n local-path-storage# 툴 설치docker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'docker exec -it myk8s-worker sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y' pod 생성 12345678910111213141516171819202122232425262728293031# 파드 생성cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: netpodspec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx-pod image: nginx:alpine terminationGracePeriodSeconds: 0EOF# 파드 확인kubectl get pod -owide# netpod 파드에서 nginx 웹 접속kubectl exec -it netpod -- curl -s $(kubectl get pod nginx -o jsonpath={.status.podIP}) | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;&lt;title&gt;Welcome to nginx!&lt;/title&gt; 도커 컨테이너 확인 k8s node인 docker container 는 init process 를 포함하는 Entrypoint 가 정의되어 있음 123456789# 도커 컨테이너 확인docker psdocker inspect myk8s-control-plane | jq... &quot;Entrypoint&quot;: [ &quot;/usr/local/bin/entrypoint&quot;, &quot;/sbin/init&quot; ],... 컨트롤플레인 컨테이너 bash 접속 후 확인 각 노드는 cpu type에 호환되는 모델로 구동됨 1234567891011docker exec -it myk8s-control-plane bash-------------------------------------------# CPU 정보 확인archaarch64 # mac m1~m3혹은x86_64 # intel/호환 cpu# 기본 사용자 확인whoamiroot 네트워크 정보 확인 123ip -br -c -4 addrip -c routecat /etc/resolv.conf Pod정보 확인 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Entrypoint 정보 확인cat /usr/local/bin/entrypoint# 프로세스 확인 : PID 1 은 /sbin/initps -ef# 컨테이터 런타임 정보 확인systemctl status containerd# DinD 컨테이너 확인 : crictl 사용crictl versioncrictl infocrictl ps -o json | jq -r '.containers[] | {NAME: .metadata.name, POD: .labels[&quot;io.kubernetes.pod.name&quot;]}'{ &quot;NAME&quot;: &quot;netshoot-pod&quot;, &quot;POD&quot;: &quot;netpod&quot;}{ &quot;NAME&quot;: &quot;nginx-pod&quot;, &quot;POD&quot;: &quot;nginx&quot;}{ &quot;NAME&quot;: &quot;kindnet-cni&quot;, &quot;POD&quot;: &quot;kindnet-fvtvf&quot;}{ &quot;NAME&quot;: &quot;kube-proxy&quot;, &quot;POD&quot;: &quot;kube-proxy-5pmzr&quot;}# 파드 이미지 확인crictl imagesIMAGE TAG IMAGE ID SIZEdocker.io/library/nginx alpine b887aca7aed61 19.6MBdocker.io/nicolaka/netshoot latest e286c635d1232 189MB# kubectl 확인kubectl get node -v6cat /etc/kubernetes/admin.confexit-------------------------------------------# 도커 컨테이너 확인 : 다시 한번 자신의 호스트PC에서 도커 컨테이너 확인, DinD 컨테이너가 호스트에서 보이는지 확인docker psdocker port myk8s-control-plane# kubectl 확인 : k8s api 호출 주소 확인kubectl get node -v6 ---------------------I0907 20:43:02.142592 2766 loader.go:395] Config loaded from file: /etc/kubernetes/admin.confI0907 20:43:02.151778 2766 round_trippers.go:553] GET https://myk8s-control-plane:6443/api/v1/nodes?limit=500 200 OK in 5 milliseconds 클러스터 삭제 1kind delete cluster --name myk8s Multi-Node Cluster (Control-plane, Nodes)1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &lt;&lt;EOT&gt; kind-2node.yaml# two node (one workers) cluster configkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane- role: worker extraPortMappings: - containerPort: 31000 hostPort: 31000 listenAddress: &quot;0.0.0.0&quot; # Optional, defaults to &quot;0.0.0.0&quot; protocol: tcp # Optional, defaults to tcp - containerPort: 31001 hostPort: 31001EOTCLUSTERNAME=myk8skind create cluster --config kind-2node.yaml --name $CLUSTERNAME# 배포 확인kind get clusterskind get nodes --name $CLUSTERNAME# 노드 확인kubectl get nodes -o wide# 노드에 Taints 정보 확인kubectl describe node $CLUSTERNAME-control-plane | grep TaintsTaints: node-role.kubernetes.io/control-plane:NoSchedulekubectl describe node $CLUSTERNAME-worker | grep TaintsTaints: &lt;none&gt;# 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인# kind manifests 파일에 정의한 nodePort로 접근 가능.docker psdocker port $CLUSTERNAME-worker31000/tcp -&gt; 0.0.0.0:3100031001/tcp -&gt; 0.0.0.0:31001# 컨테이너 내부 정보 확인 : 필요 시 각각의 노드에 bash로 접속하여 확인.docker exec -it $CLUSTERNAME-control-plane ip -br -c -4 addrdocker exec -it $CLUSTERNAME-worker ip -br -c -4 addr kube-ops-view 설치 호스트에서 노트포트로 접근가능 1234567891011# kube-ops-view# helm show values geek-cookbook/kube-ops-viewhelm repo add geek-cookbook https://geek-cookbook.github.io/charts/helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=31000 --set env.TZ=&quot;Asia/Seoul&quot; --namespace kube-system# 설치 확인kubectl get deploy,pod,svc,ep -n kube-system -l app.kubernetes.io/instance=kube-ops-view# kube-ops-view 접속 URL 확인 (1.5 , 2 배율)echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:31000/#scale=1.5&quot;echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:31000/#scale=2&quot; 클러스터 삭제 1kind delete cluster --name $CLUSTERNAME","link":"/blog/2024/09/08/docs/kind/"},{"title":"cgroups","text":"Cgroups, Control Groups 컨테이너 별로 자원을 분배하고 limit 내에서 운용 하나 또는 복수의 장치를 묶어서 그룹 프로세스가 사용하는 리소스 통제 Cgroup 파일시스템 자원할당과 제어를 파일시스템으로 제공 cgroup 네임스페이스로 격리할 수 있다. 1234567891011seongtki@seongtki:~$ tree -L 1 /sys/fs/cgroup/cpu/sys/fs/cgroup/cpu├── cgroup.clone_children├── cgroup.procs├── cgroup.sane_behavior├── cpuacct.stat├── cpuacct.usage├── cpuacct.usage_all├── cpuacct.usage_percpu├── cpuacct.usage_percpu_sys├── cpuacct.usage_percpu_user cgroup-tools, stress 설치12root@seongtki:~# apt install -y cgroup-toolsroot@seongtki:~# apt install -y stress 프로세스 실행 및 cpu 사용률 확인 top으로 CPU 100% 확인 12root@seongtki:~# stress -c 1stress: info: [2328] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd 자원제한을 위한 cgroup 제어그룹 생성 -a : 소유자 정의 (:) -g : 추가할 그룹을 정의한다. (:) 1root@seongtki:~# cgcreate -a root -g cpu:mycgroup 디렉토리만 만들면 내용은 커널이 다만들어준다 (mycgroup이 관리할 내용을) 123456789101112131415root@seongtki:~# tree /sys/fs/cgroup/cpu/mycgroup//sys/fs/cgroup/cpu/mycgroup/├── cgroup.clone_children├── cgroup.procs├── cpuacct.stat├── cpuacct.usage├── cpuacct.usage_all├── cpuacct.usage_percpu├── cpuacct.usage_percpu_sys├── cpuacct.usage_percpu_user├── cpuacct.usage_sys├── cpuacct.usage_user├── cpu.cfs_period_us├── cpu.cfs_quota_us... 리소스 설정 및 프로세스 할당 cpu.cfs_period_us : CPU 할당량을 사용할 수 있는 기간을 마이크로초(µs) 단위로 설정 기본값이 100000 cpu.cfs_quota_us : 프로세스 그룹에 할당되는 CPU 시간의 한계를 마이크로초(µs) 단위로 설정 cpu.cfs_quota_us / cfs_period_us * 100 =&gt; 30000 / 100000 * 100 = 30% 30%가 넘지 않게 커널이 cgroup정보를 활용해서 쓰로틀링을 걸어준다 123root@seongtki:~# cgset -r cpu.cfs_quota_us=30000 mycgroup;root@seongtki:~# cgexec -g cpu:mycgroup stress -c 1stress: info: [2352] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd cgroups 삭제1cgdelete cpu:mycgroup Cgroup 파일 시스템으로 리소스 관리 요약 제어그룹(controller group) 생성 우리 예제는 mycgroup이라는 제어그룹을 생성 제어그룹 리소스 설정 cpu.cfs_quota_us 을 통해 사용률을 설정 제어그룹 프로세스 할당 cgexec 를사용해서 mycgroup 에 stress 프로세스를 할당해서 실행.","link":"/blog/2024/08/31/docs/cgroup/cgroup/"},{"title":"Flannel CNI Concept","text":"Flannel은 단일 바이너리 에이전트인 flanneld 가 각 노드에서 동작하여 작은 규모의 클러스터 환경에서 파드들 간 통신 환경을 구성하는 CNI이다. Flannel은 노드간 통신에 VXLAN을 사용하며, UDP 8472 port를 사용한다. kind &amp; Flannel 배포 disableDefaultCNI : true 기본 CNI를 사용하지 않는다. control-plane, worker node 2ea 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081cat &lt;&lt;EOF&gt; kind-cni.yamlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane labels: mynode: control-plane extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 kubeadmConfigPatches: - | kind: ClusterConfiguration controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0- role: worker labels: mynode: worker- role: worker labels: mynode: worker2networking: disableDefaultCNI: trueEOFkind create cluster --config kind-cni.yaml --name myk8s --image kindest/node:v1.30.4# 배포 확인kind get clusterskind get nodes --name myk8skubectl cluster-info# 네트워크 확인kubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot;-----------------------&quot;--service-cluster-ip-range=10.96.0.0/16&quot;, # service ip 대역&quot;--cluster-cidr=10.244.0.0/16&quot;, # # 노드 확인 : CRIkubectl get nodes -o wide# 노드 라벨 확인kubectl get nodes myk8s-control-plane -o jsonpath={.metadata.labels} | jq...&quot;mynode&quot;: &quot;control-plane&quot;,...kubectl get nodes myk8s-worker -o jsonpath={.metadata.labels} | jqkubectl get nodes myk8s-worker2 -o jsonpath={.metadata.labels} | jq# 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인docker psdocker port myk8s-control-planedocker port myk8s-workerdocker port myk8s-worker2# 컨테이너 내부 정보 확인docker exec -it myk8s-control-plane ip -br -c -4 addrdocker exec -it myk8s-worker ip -br -c -4 addrdocker exec -it myk8s-worker2 ip -br -c -4 addr# tool installdocker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping htop git nano -y'docker exec -it myk8s-worker sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y'docker exec -it myk8s-worker2 sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y' CNI 를 disable 했기 때문에 kindnet 이 없고, coredns, sc가 실행되지 못한다. kubectl describe pod -n kube-system -l k8s-app=kube-dns flannel net-config flannel 을 설치했으나 bridge 플러그인이 /opt/cni/bin 에 없어서 추가해 주어야 한다. 권한에 유의하여 복사한다. 12345678910111213141516171819docker cp bridge myk8s-control-plane:/opt/cni/bin/bridgedocker cp bridge myk8s-worker:/opt/cni/bin/bridgedocker cp bridge myk8s-worker2:/opt/cni/bin/bridgedocker exec -it myk8s-control-plane chmod 755 /opt/cni/bin/bridgedocker exec -it myk8s-worker chmod 755 /opt/cni/bin/bridgedocker exec -it myk8s-worker2 chmod 755 /opt/cni/bin/bridgedocker exec -it myk8s-control-plane chown root root /opt/cni/bin/bridgedocker exec -it myk8s-worker chown root root /opt/cni/bin/bridgedocker exec -it myk8s-worker2 chown root root /opt/cni/bin/bridge#docker exec -it myk8s-control-plane ls -l /opt/cni/bin/docker exec -it myk8s-worker ls -l /opt/cni/bin/docker exec -it myk8s-worker2 ls -l /opt/cni/bin/for i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i ls /opt/cni/bin/; echo; donebridge flannel host-local loopback portmap ptp#kubectl get pod -A -owide kubelet, pstree 확인 1234567891011121314151617181920212223242526272829303132333435363738# kubelet config 정보kubectl describe cm -n kube-system kubelet-configdocker exec -it myk8s-control-plane cat /var/lib/kubelet/config.yaml...clusterDNS:- 10.96.0.10clusterDomain: cluster.localstaticPodPath: /etc/kubernetes/manifests...#docker exec -it myk8s-control-plane cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS=&quot;--container-runtime-endpoint=unix:///run/containerd/containerd.sock --node-ip=172.18.0.2 --node-labels= --pod-infra-container-image=registry.k8s.io/pause:3.9 --provider-id=kind://docker/myk8s/myk8s-control-plane&quot;for i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i cat /var/lib/kubelet/kubeadm-flags.env; echo; done#for i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i ls /etc/kubernetes/manifests; echo; done&gt;&gt; node myk8s-control-plane &lt;&lt;etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml&gt;&gt; node myk8s-worker &lt;&lt;&gt;&gt; node myk8s-worker2 &lt;&lt;#kubectl describe pod -n kube-system kube-apiserver-myk8s-control-planekubectl describe pod -n kube-system kube-controller-manager-myk8s-control-planekubectl describe pod -n kube-system kube-scheduler-myk8s-control-plane#for i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i pstree -aT; echo; donefor i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i pstree -atl; echo; done#docker exec -it myk8s-control-plane systemctl status kubelet -l --no-pagerdocker exec -it myk8s-worker systemctl status kubelet -l --no-pagerdocker exec -it myk8s-worker2 systemctl status kubelet -l --no-pager flannel 정보 확인1234567kubectl get ds,pod,cm -n kube-flannel -owidekubectl describe cm -n kube-flannel kube-flannel-cfg# iptables 정보 확인for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; done FLANNEL_MTU 1450 1500byte 에서 vxlan header 50byte 제외 12345678910111213141516171819for i in myk8s-control-plane myk8s-worker myk8s-worker2; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it $i cat /run/flannel/subnet.env ; echo; done&gt;&gt; node myk8s-control-plane &lt;&lt;FLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24 # 파드 네트워크 대역 정의, CNI inetFLANNEL_MTU=1450FLANNEL_IPMASQ=true # 파드가 외부(인터넷) 통신 시 해당 노드의 마스커레이딩을 사용&gt;&gt; node myk8s-worker &lt;&lt;FLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.2.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true&gt;&gt; node myk8s-worker2 &lt;&lt;FLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.1.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true 테스트 pod 생성123456789101112131415161718192021222324252627282930313233343536cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: pod-1 labels: app: podspec: nodeSelector: kubernetes.io/hostname: myk8s-worker containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: pod-2 labels: app: podspec: nodeSelector: kubernetes.io/hostname: myk8s-worker2 containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0EOF# 파드 확인 : IP 확인kubectl get pod -o wide worker node 의 여러 네트워크 정보를 파악할 수 있다. 1234ip -c -br addrip -c route # 다른 노드의 파드 대역(podCIDR)의 라우팅 정보ip -c neigh show dev flannel.1 # flannel.1 인터페이스를 통한 ARP 테이블 정보 확인bridge fdb show dev flannel.1 # 브리지 fdb 정보에서 해당 MAC 주소와 통신 시 각 노드의 en0 dst bridge, cni 정보 확인1234567891011121314151617181920212223# [터미널1,2] 워커 노드1,2docker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bash-----------------------------# 브리지 정보 확인brctl show cni0# 브리지 연결 링크(veth) 확인bridge link# 브리지 VLAN 정보 확인bridge vlan# cbr(custom bridge) 정보 : kubenet CNI의 bridge - 링크tree /var/lib/cni/networks/cbr0 /var/lib/cni/networks/cbr0 ├── 10.244.2.4. ├── last_reserved_ip.0 └── lock# 네트워크 관련 정보들 확인ip -c addr | grep veth -A3----------------------------- 패킷 캡처123tcpdump -i cni0 -nn icmptcpdump -i flannel.1 -nn icmptcpdump -i eth0 -nn icmp 123456789101112131415# [터미널1,2] 워커 노드1,2docker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bash-----------------------------tcpdump -i eth0 -nn udp port 8472 -w /root/vxlan.pcap # CTRL+C 취소 후 확인 : ls -l /root/vxlan.pcapconntrack -L | grep -i icmp-----------------------------# [터미널3]# 패킷캡처 후 호스트로 파일 이동 및 wireshark를 통해 패킷을 쉽게 확인할 수 있다.docker cp myk8s-worker:/root/vxlan.pcap .wireshark vxlan.pcap 패킷 캡처 SRC, DST IP 확인","link":"/blog/2024/11/24/docs/flannel/"},{"title":"Calico Concept","text":"aws k8s 배포123456789101112131415161718192021222324aws cloudformation deploy --template-file kans-3w.yaml --stack-name mylab --parameter-overrides KeyName=container SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2# CloudFormation 스택 배포# aws cloudformation deploy --template-file kans-3w.yaml --stack-name mylab --parameter-overrides KeyName=&lt;My SSH Keyname&gt; SgIngressSshCidr=&lt;My Home Public IP Address&gt;/32 --region ap-northeast-2aws cloudformation deploy --template-file kans-3w.yaml --stack-name mylab --parameter-overrides KeyName=kp-gasida SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2## Tip. 인스턴스 타입 변경 : MyInstanceType=t2.micro예시) aws cloudformation deploy --template-file kans-3w.yaml --stack-name mylab --parameter-overrides MyInstanceType=t2.micro KeyName=kp-gasida SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2# CloudFormation 스택 배포 완료 후 k8s-m EC2 IP 출력aws cloudformation describe-stacks --stack-name mylab --query 'Stacks[*].Outputs[0].OutputValue' --output text --region ap-northeast-2# [모니터링] CloudFormation 스택 상태 : 생성 완료 확인while true; do date AWS_PAGER=&quot;&quot; aws cloudformation list-stacks \\ --stack-status-filter CREATE_IN_PROGRESS CREATE_COMPLETE CREATE_FAILED DELETE_IN_PROGRESS DELETE_FAILED \\ --query &quot;StackSummaries[*].{StackName:StackName, StackStatus:StackStatus}&quot; \\ --output table sleep 1done# k8s-m EC2 SSH 접속 ssh -i ~/.ssh/container.pem ubuntu@$(aws cloudformation describe-stacks --stack-name mylab --query 'Stacks[*].Outputs[0].OutputValue' --output text --region ap-northeast-2) 12kubectl config rename-context &quot;kubernetes-admin@kubernetes&quot; &quot;HomeLab&quot;kubectl config get-contexts Calico CNI 설치123456789101112131415161718192021222324252627282930# 모니터링watch -d 'kubectl get pod -A -owide'# calico cni install## kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml - 서브넷 24bit 추가# 기본 yaml 에 4946줄 이동 후 아래 내용 추가 해둠vi calico.yaml... # Block size to use for the IPv4 POOL created at startup. Block size for IPv4 should be in the range 20-32. default 24 - name: CALICO_IPV4POOL_BLOCK_SIZE value: &quot;24&quot;kubectl apply -f https://raw.githubusercontent.com/gasida/KANS/main/kans3/calico-kans.yaml#tree /opt/cni/bin/ls -l /opt/cni/bin/ip -c routeip -c addriptables -t filter -Liptables -t nat -Liptables -t filter -L | wc -liptables -t nat -L | wc -l# calicoctl installcurl -L https://github.com/projectcalico/calico/releases/download/v3.28.1/calicoctl-linux-amd64 -o calicoctlchmod +x calicoctl &amp;&amp; mv calicoctl /usr/bincalicoctl version# CNI 설치 후 파드 상태 확인kubectl get pod -A -o wide 구성요소 확인 12345678910111213141516171819202122232425262728293031323334353637383940# 버전 확인 - 링크## kdd 의미는 쿠버네티스 API 를 데이터저장소로 사용 : k8s API datastore(kdd)calicoctl version# calico 관련 정보 확인kubectl get daemonset -n kube-systemkubectl get pod -n kube-system -l k8s-app=calico-node -owidekubectl get deploy -n kube-system calico-kube-controllerskubectl get pod -n kube-system -l k8s-app=calico-kube-controllers -owide# 칼리코 IPAM 정보 확인 : 칼리코 CNI 를 사용한 파드가 생성된 노드에 podCIDR 네트워크 대역 확인 - 링크calicoctl ipam show# Block 는 각 노드에 할당된 podCIDR 정보calicoctl ipam show --show-blockscalicoctl ipam show --show-borrowedcalicoctl ipam show --show-configurationps axf 4405 ? Sl 0:09 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id dd532e5efaad436ebe7d10cdd3bb2ffe5a2873a0604ce3b 4425 ? Ss 0:00 \\_ /pause 4740 ? Ss 0:00 \\_ /usr/local/bin/runsvdir -P /etc/service/enabled 4811 ? Ss 0:00 \\_ runsv allocate-tunnel-addrs 4819 ? Sl 0:00 | \\_ calico-node -allocate-tunnel-addrs 4812 ? Ss 0:00 \\_ runsv bird 4994 ? S 0:00 | \\_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg 4813 ? Ss 0:00 \\_ runsv cni 4822 ? Sl 0:00 | \\_ calico-node -monitor-token 4814 ? Ss 0:00 \\_ runsv bird6 4993 ? S 0:00 | \\_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 4815 ? Ss 0:00 \\_ runsv confd 4820 ? Sl 0:00 | \\_ calico-node -confd 4816 ? Ss 0:00 \\_ runsv felix 4824 ? Sl 0:54 | \\_ calico-node -felix 4817 ? Ss 0:00 \\_ runsv node-status-reporter 4823 ? Sl 0:00 | \\_ calico-node -status-reporter 4818 ? Ss 0:00 \\_ runsv monitor-addresses 4825 ? Sl 0:00 \\_ calico-node -monitor-addresses IPAM IP를 할당해주고 관리해주는 시스템 기본적으로 OS에는 local-IPAM이 존재함 host-local IPAM 정보 확인 : k8s-m 노드의 podCIDR 은 host-local 대신 칼리코 IPAM 를 사용함 워커 노드마다 할당된 dedicated subnet (podCIDR) 확인 felix felix (필릭스) : Host의 Network Inteface, Route Table, iptables을 관리 12iptables -t filter -S | grep caliiptables -t nat -S | grep cali bird bird (버드) : bird는 라우팅 프로토콜 동작을 하는 데몬이며, 각 노드들의 파드 네트워크 대역 정보를 전파 및 전달 받음. confd 가벼운 오픈소스 설정 변경 관리 툴 BGP 설정 등으로 칼리코 데이터 저장소에 변경이 발생하면 버드의 변경된 설정 파일을 만들고, 변경된 설정 파일을 반영","link":"/blog/2024/09/22/docs/calico/calico-concept/"},{"title":"gatewayapi concept","text":"GateWay API 주요기능 개선된 리소스 모델 : API는 GatewayClass, Gateway 및 Route(HTTPRoute, TCPRoute 등)와 같은 새로운 사용자 정의 리소스를 도입하여 라우팅 규칙을 정의하는 보다 세부적이고 표현력 있는 방법을 제공합니다. 프로토콜 독립적 : 주로 HTTP용으로 설계된 Ingress와 달리 Gateway API는 TCP, UDP, TLS를 포함한 여러 프로토콜을 지원합니다. 강화된 보안 : TLS 구성 및 보다 세부적인 액세스 제어에 대한 기본 제공 지원. 교차 네임스페이스 지원 : 서로 다른 네임스페이스의 서비스로 트래픽을 라우팅하여 보다 유연한 아키텍처를 구축할 수 있는 기능을 제공합니다. 확장성 : API는 사용자 정의 리소스 및 정책으로 쉽게 확장할 수 있도록 설계되었습니다. 역할 지향 : 클러스터 운영자, 애플리케이션 개발자, 보안 팀 간의 우려를 명확하게 분리합니다. 이것은 제 겸손한 의견으로는 Gateway API의 가장 흥미로운 기능 중 하나일 것입니다. Gateway API 소개 : 기존의 Ingress 에 좀 더 기능을 추가, 역할 분리(role-oriented) - Docs 서비스 메시(istio)에서 제공하는 Rich 한 기능 중 일부 기능들과 혹은 운영 관리에 필요한 기능들을 추가 추가 기능 : 헤더 기반 라우팅, 헤더 변조, 트래픽 미러링(쉽게 트래픽 복제), 역할 기반 Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing. Make network services available by using an extensible, role-oriented, protocol-aware configuration mechanism. Gateway API is an add-on containing API kinds that provide dynamic infrastructure provisioning and advanced traffic routing. 구성 요소 (Resource) GatewayClass,Gateway, HTTPRoute, TCPRoute, Service","link":"/blog/2024/12/01/docs/gateway/gateway-api/"},{"title":"Calico Network Mode","text":"Network Mode1. IPIP 모드 파드 간 통신이 노드와 노드 구간에서는 IPIP 인캡슐레이션을 통해서 이루어 집니다 calicoctl 및 네트워크 정보 확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 모드 정보 확인calicoctl get ippool -o wide# 노드(BGP) Peer 정보 확인calicoctl node statusIPv4 BGP status+----------------+-------------------+-------+------------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+-------------------+-------+------------+-------------+| 192.168.20.100 | node-to-node mesh | up | 2024-09-15 | Established || 192.168.10.101 | node-to-node mesh | up | 2024-09-15 | Established || 192.168.10.102 | node-to-node mesh | up | 2024-09-15 | Established |+----------------+-------------------+-------+------------+-------------+# BGP 로 전달 받은 파드 네트워크 대역이 호스트 라우팅 테이블에 적용되었는지 확인 route -n | egrep '(Destination|tunl0)'Destination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.158.0 192.168.10.101 255.255.255.0 UG 0 0 0 tunl0172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 tunl0calicoctl ipam show --show-blocks+----------+-----------------+-----------+------------+--------------+| GROUPING | CIDR | IPS TOTAL | IPS IN USE | IPS FREE |+----------+-----------------+-----------+------------+--------------+| IP Pool | 172.16.0.0/16 | 65536 | 7 (0%) | 65529 (100%) || Block | 172.16.116.0/24 | 256 | 1 (0%) | 255 (100%) || Block | 172.16.158.0/24 | 256 | 1 (0%) | 255 (100%) || Block | 172.16.184.0/24 | 256 | 1 (0%) | 255 (100%) || Block | 172.16.34.0/24 | 256 | 4 (2%) | 252 (98%) |# 워커 노드마다 할당된 dedicated subnet (podCIDR) 확인kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'kubectl get node k8s-m -o json | jq '.spec.podCIDR'&quot;172.16.0.0/24&quot;kubectl get node k8s-w1 -o json | jq '.spec.podCIDR'&quot;172.16.2.0/24&quot;...# 기본제공 cat /etc/cni/net.d/10-calico.conflist...&quot;ipam&quot;: { &quot;type&quot;: &quot;calico-ipam&quot; },...# IPAM 우선순위 - 링크 링크21. Kubernetes annotations2. CNI configuration &lt;&lt; Calico IPAM3. IP pool node selectors &lt;&lt; host-local IPAM# IPIP 인캡슐레이션 동작을 수행하는 터널 인터페이스 확인ip -c -d addr show tunl03: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 8981 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 promiscuity 0 minmtu 0 maxmtu 0 ipip any remote any local any ttl inherit nopmtudisc numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 172.16.116.0/32 scope global tunl0 valid_lft forever preferred_lft forever 동작확인1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: pod1 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: pod2 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: pod3spec: containers: - name: pod3 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0 1234567891011121314151617181920(⎈|HomeLab:default) root@k8s-m:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod1 1/1 Running 0 18s 172.16.158.5 k8s-w1 &lt;none&gt; &lt;none&gt;pod2 1/1 Running 0 18s 172.16.184.2 k8s-w2 &lt;none&gt; &lt;none&gt;pod3 1/1 Running 0 18s 172.16.34.4 k8s-w0 &lt;none&gt; &lt;none&gt;# 파드 Shell 접속(zsh)kubectl exec -it pod1 -- zsh## 파드 Shell 에서 아래 입력 pod1  ~  ping 172.16.184.2PING 172.16.184.2 (172.16.184.2) 56(84) bytes of data.64 bytes from 172.16.184.2: icmp_seq=1 ttl=62 time=0.466 ms64 bytes from 172.16.184.2: icmp_seq=2 ttl=62 time=0.318 ms# 파드가 동작하는 워커노드1의 eth0(예시)에서 IPIP 패킷(proto 4) 덤프tcpdump -i &lt;eth0&gt; -nn proto 4# 혹은 아래 처럼 파일로 저장 후 해당 파일을 다운받아서 확인(wireshark 등 사용)tcpdump -i &lt;eth0&gt; proto 4 -w /tmp/calico-ipip.pcap 12345678910111213root@k8s-w1:~# tcpdump -i ens5 -nn proto 4tcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on ens5, link-type EN10MB (Ethernet), snapshot length 262144 bytes05:28:15.092929 IP 192.168.10.101 &gt; 192.168.20.100: IP 172.16.158.5 &gt; 172.16.34.4: ICMP echo request, id 105, seq 1, length 6405:28:15.093903 IP 192.168.20.100 &gt; 192.168.10.101: IP 172.16.34.4 &gt; 172.16.158.5: ICMP echo reply, id 105, seq 1, length 6405:28:16.094082 IP 192.168.10.101 &gt; 192.168.20.100: IP 172.16.158.5 &gt; 172.16.34.4: ICMP echo request, id 105, seq 2, length 6405:28:16.094944 IP 192.168.20.100 &gt; 192.168.10.101: IP 172.16.34.4 &gt; 172.16.158.5: ICMP echo reply, id 105, seq 2, length 6405:28:17.723065 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.5 &gt; 172.16.184.2: ICMP echo request, id 109, seq 1, length 6405:28:17.723282 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.2 &gt; 172.16.158.5: ICMP echo reply, id 109, seq 1, length 6405:28:18.752143 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.5 &gt; 172.16.184.2: ICMP echo request, id 109, seq 2, length 6405:28:18.752374 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.2 &gt; 172.16.158.5: ICMP echo reply, id 109, seq 2, length 6405:28:19.776067 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.5 &gt; 172.16.184.2: ICMP echo request, id 109, seq 3, length 6405:28:19.776393 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.2 &gt; 172.16.158.5: ICMP echo reply, id 109, seq 3, length 64 2. Direct 모드통신 흐름파드 통신 패킷이 출발지 노드의 라우팅 정보를 보고 목적지 노드로 원본 패킷 그대로 전달합니다 Direct 모드 설정123(⎈|HomeLab:default) root@k8s-m:~# calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED DISABLEBGPEXPORT SELECTORdefault-ipv4-ippool 172.16.0.0/16 true Never Never false false all() 123456789101112131415161718192021222324252627# Calico 모드 정보 확인calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTORdefault-ipv4-ippool 172.16.0.0/16 true Always Never false all()# (옵션) 모니터링watch -d &quot;route -n | egrep '(Destination|UG)'&quot;# 설정calicoctl get ippool default-ipv4-ippool -o yamlcalicoctl get ippool default-ipv4-ippool -o yaml | sed -e &quot;s/ipipMode: Always/ipipMode: Never/&quot; | calicoctl apply -f -# 모드 정보 확인 : IPIPMODE 가 Never 로 변경!calicoctl get ippool -o wideroot@k8s-m:~/yaml# calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED DISABLEBGPEXPORT SELECTORdefault-ipv4-ippool 172.16.0.0/16 true Never Never false false all()# BGP 로 전달 받은 파드 네트워크 대역이 호스트 라우팅 테이블에 적용되었는지 확인 : Iface 가 tunl0 에서 ens5 혹은 enp0s8 로 변경!route -n | egrep '(Destination|UG)'root@k8s-w1:~# route -n | egrep '(Destination|UG)'Destination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.10.1 0.0.0.0 UG 100 0 0 ens5172.16.34.0 192.168.10.1 255.255.255.0 UG 0 0 0 ens5172.16.158.0 192.168.10.101 255.255.255.0 UG 0 0 0 ens5172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 ens5192.168.0.2 192.168.10.1 255.255.255.255 UGH 100 0 0 ens5 동작 확인1234567891011# 파드 생성curl -s -O https://raw.githubusercontent.com/gasida/NDKS/main/5/node3-pod3.yamlkubectl apply -f node3-pod3.yaml# 파드 IP 정보 확인kubectl get pod -o widecalicoctl get wepWORKLOAD NODE NETWORKS INTERFACEpod1 k8s-w1 172.16.158.6/32 calice0906292e2pod2 k8s-w2 172.16.184.3/32 calibd2348b4f67pod3 k8s-w0 172.16.34.5/32 cali49778cadcf1 aws 환경에서 테스트하기 위해 pod 네트워크 대역에 대한 Eni를 Route table에 추가해준다. 1234567891011# 파드 Shell 접속(zsh)kubectl exec -it pod1 -- zsh## 파드 Shell 에서 아래 입력ping &lt;pod2 혹은 pod3 IP&gt;# 파드가 동작하는 노드의 eth0(예시)에서 패킷 덤프tcpdump -i ens5 -nn icmp # [실습환경 A Type]# 파일로 저장 후 해당 파일을 다운받아서 확인(wireshark 등 사용)tcpdump -i &lt;eth0&gt; icmp -w /tmp/calico-direct.pcaptcpdump -i ens5 icmp -w /tmp/calico-direct.pcap # [실습환경 A Type] pod1 -&gt; pod2 통신 1234567891011(⎈|HomeLab:default) root@k8s-m:~# calicoctl get wepWORKLOAD NODE NETWORKS INTERFACEpod1 k8s-w1 172.16.158.6/32 calice0906292e2pod2 k8s-w2 172.16.184.3/32 calibd2348b4f67pod3 k8s-w0 172.16.34.5/32 cali49778cadcf1(⎈|HomeLab:default) root@k8s-m:~# k exec -it pod1 -- bashpod1:~# ping 172.16.184.3PING 172.16.184.3 (172.16.184.3) 56(84) bytes of data.64 bytes from 172.16.184.3: icmp_seq=1 ttl=62 time=0.349 ms64 bytes from 172.16.184.3: icmp_seq=2 ttl=62 time=0.292 ms 1234567891011root@k8s-w2:~# tcpdump -i ens5 -nn icmptcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on ens5, link-type EN10MB (Ethernet), snapshot length 262144 bytes05:57:52.600373 IP 172.16.158.6 &gt; 172.16.184.3: ICMP echo request, id 15, seq 1, length 6405:57:52.600465 IP 172.16.184.3 &gt; 172.16.158.6: ICMP echo reply, id 15, seq 1, length 6405:57:53.632116 IP 172.16.158.6 &gt; 172.16.184.3: ICMP echo request, id 15, seq 2, length 6405:57:53.632178 IP 172.16.184.3 &gt; 172.16.158.6: ICMP echo reply, id 15, seq 2, length 6405:57:54.663943 IP 172.16.158.6 &gt; 172.16.184.3: ICMP echo request, id 15, seq 3, length 6405:57:54.664012 IP 172.16.184.3 &gt; 172.16.158.6: ICMP echo reply, id 15, seq 3, length 6405:57:55.661415 IP 172.16.158.6 &gt; 172.16.184.3: ICMP echo request, id 15, seq 4, length 6405:57:55.661479 IP 172.16.184.3 &gt; 172.16.158.6: ICMP echo reply, id 15, seq 4, length 64 CrossSubnet 모드동작 : 노드 간 같은 네트워크 대역(Direct 모드로 동작) , 노드 간 다른 네트워크 대역(IPIP 모드로 동작) 1234567891011121314151617181920212223242526272829303132333435# CrossSubnet 모드 설정calicoctl patch ippool default-ipv4-ippool -p '{&quot;spec&quot;:{&quot;ipipMode&quot;:&quot;CrossSubnet&quot;}}'# 모드 확인calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTORdefault-ipv4-ippool 172.16.0.0/16 true CrossSubnet Never false all()# 파드 생성kubectl apply -f node3-pod3.yamlcalicoctl get wep(⎈|HomeLab:default) root@k8s-m:~# route -n | egrep '(Destination|UG)'Destination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.10.1 0.0.0.0 UG 100 0 0 ens5172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0 # 노드간 다른 네트워크 대역 - IPIP 모드172.16.158.0 192.168.10.101 255.255.255.0 UG 0 0 0 ens5 # 노드간 같은 네트워크 대역 - Direct 모드172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 ens5 # 노드간 같은 네트워크 대역 - Direct 모드192.168.0.2 192.168.10.1 255.255.255.255 UGH 100 0 0 ens5# 파드 Shell 접속(zsh)kubectl exec -it pod1 -- zsh## 파드 Shell 에서 아래 입력 pod1  ~  ping 172.16.184.3PING 172.16.184.3 (172.16.184.3) 56(84) bytes of data.64 bytes from 172.16.184.3: icmp_seq=1 ttl=62 time=0.301 ms64 bytes from 172.16.184.3: icmp_seq=2 ttl=62 time=1.15 ms64 bytes from 172.16.184.3: icmp_seq=3 ttl=62 time=0.299 ms pod1  ~  ping 172.16.34.5PING 172.16.34.5 (172.16.34.5) 56(84) bytes of data.64 bytes from 172.16.34.5: icmp_seq=1 ttl=62 time=1.03 ms64 bytes from 172.16.34.5: icmp_seq=2 ttl=62 time=0.949 ms64 bytes from 172.16.34.5: icmp_seq=3 ttl=62 time=0.943 ms 3. VXLAN 모드 모드 변경 12345678910111213141516# VXLAN 설정 적용kubectl apply -f https://raw.githubusercontent.com/gasida/KANS/main/3/calico-vxlan-kans.yaml# 모드 설정 변경 : VXLAN 모드 사용 시, ipipMode 와 BGP(Bird)는 반드시 Never 비활성화 되어야 합니다# ipipMode 현재 모드(Always, CrossSubnet)를 확인하고, 해당 모드를 Never 로 변경합니다calicoctl get ippool default-ipv4-ippool -o widecalicoctl get ippool default-ipv4-ippool -o yaml | sed -e &quot;s/ipipMode: CrossSubnet/ipipMode: Never/&quot; | calicoctl apply -f -#calicoctl get ippool default-ipv4-ippool -o yaml | sed -e &quot;s/ipipMode: Always/ipipMode: Never/&quot; | calicoctl apply -f -calicoctl get ippool default-ipv4-ippool -o yaml | sed -e &quot;s/vxlanMode: Never/vxlanMode: Always/&quot; | calicoctl apply -f -calicoctl get ippool default-ipv4-ippool -o wide# 변경 설정 적용을 위해서 calico 파드 삭제 후 재생성kubectl delete pod -n kube-system -l k8s-app=calico-node# 모든 노드에서 enp0s8 down 후 up (라우팅 테이블 갱신을 위함)route -n &amp;&amp; echo &amp;&amp; ip link set ens5 down &amp;&amp; sleep 1 &amp;&amp; ip link set ens5 up &amp;&amp; route -n 설정확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 모드 정보 확인calicoctl get ippool -o wideroot@k8s-m:~/yaml# calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTORdefault-ipv4-ippool 172.16.0.0/16 false Never Always false all()# 노드(BGP) Peer 정보 확인 &gt;&gt; BGP 동작하지 않는다!calicoctl node statusroot@k8s-m:~/yaml# calicoctl node statusCalico process is running.None of the BGP backend processes (BIRD or GoBGP) are running.# 파드 네트워크 대역이 호스트 라우팅 테이블에 적용되었는지 확인 &gt;&gt; BGP 미사용으로 다른 방식으로 파드 대역 전달 받음route -n | egrep '(Destination|vxlan)'root@k8s-m:~/yaml# route -n | egrep '(Destination|vxlan)'Destination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 172.16.34.7 255.255.255.0 UG 0 0 0 vxlan.calico172.16.158.0 172.16.158.9 255.255.255.0 UG 0 0 0 vxlan.calico172.16.184.0 172.16.184.6 255.255.255.0 UG 0 0 0 vxlan.calico# VXLAN 인캡슐레이션 동작을 수행하는 인터페이스 확인 (참고로 tunl0 의 IP는 삭제됨)ifconfig vxlan.calicoroot@k8s-m:~/yaml# ifconfig vxlan.calicovxlan.calico: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 8951 inet 172.16.116.2 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::6404:3ff:fe49:b11e prefixlen 64 scopeid 0x20&lt;link&gt; ether 66:04:03:49:b1:1e txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 13 overruns 0 carrier 0 collisions 0 ip -c -d addr show vxlan.calico# VXLAN Peer 정보 확인bridge fdb show | grep vxlanroot@k8s-m:~/yaml# bridge fdb show | grep vxlan66:55:c4:6b:81:ed dev vxlan.calico dst 192.168.10.102 self permanent66:74:91:32:7c:04 dev vxlan.calico dst 192.168.10.101 self permanent66:8d:da:b4:58:1a dev vxlan.calico dst 192.168.20.100 self permanentip neighbor show | grep vxlanroot@k8s-m:~/yaml# ip neighbor show | grep vxlan172.16.158.9 dev vxlan.calico lladdr 66:74:91:32:7c:04 PERMANENT172.16.34.7 dev vxlan.calico lladdr 66:8d:da:b4:58:1a PERMANENT172.16.184.6 dev vxlan.calico lladdr 66:55:c4:6b:81:ed PERMANENT tcpdump 확인 1234567891011# 파드 Shell 접속(zsh)kubectl exec -it pod1 -- zsh## 파드 Shell 에서 아래 입력ping 172.16.184.7# 파드가 동작하는 노드의 eth0(예시)에서 패킷 덤프tcpdump -i ens5 -nn udp port 4789혹은 아래 처럼 파일로 저장 후 해당 파일을 다운받아서 확인(wireshark 등 사용)tcpdump -i ens5 udp port 4789 -w /tmp/calico-vxlan.pcap# downloadscp -i ~/.ssh/container.pem ubuntu@3.39.253.154:/tmp/calico-vxlan.pcap .","link":"/blog/2024/09/22/docs/calico/network-mode/"},{"title":"gloo","text":"Install KinD Cluster 123456789101112131415161718192021222324#**cat &lt;&lt;EOT&gt; kind-1node.yaml**kind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: **control-plane** extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002**EOT**# Install KinD Clusterkind create cluster --image **kindest/node:v1.30.0** --config **kind-1node.yaml** --name **myk8s**# 노드에 기본 툴 설치docker exec -it **myk8s-control-plane sh -c '**apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils tcpdump ngrep iputils-ping **git vim** -y'# 노드/파드 확인kubectl get nodes -o widekubectl get pod -A Install Gateway API CRDs : The Kubernetes Gateway API abstractions are expressed using Kubernetes CRDs. 123# CRDs 설치 및 확인**kubectl apply -f &lt;https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml**&gt;kubectl get crd Install Glooctl Utility : GLOOCTL is a command-line utility that allows users to view, manage, and debug Gloo Gateway deployments - Link 1234567891011121314151617181920212223242526# [신규 터미널] 아래 bash 진입 후 glooctl 툴 사용**docker exec -it myk8s-control-plane bash**----------------------------------------# Install Glooctl Utility## glooctl install gateway # install gloo's function gateway functionality into the 'gloo-system' namespace## glooctl install ingress # install very basic Kubernetes Ingress support with Gloo into namespace gloo-system## glooctl install knative # install Knative serving with Gloo configured as the default cluster ingress## curl -sL &lt;https://run.solo.io/gloo/install&gt; | sh**curl -sL &lt;https://run.solo.io/gloo/install&gt; | GLOO_VERSION=v1.17.7 sh**export PATH=$HOME/.gloo/bin:$PATH# 버전 확인**glooctl version**----------------------------------------{ &quot;client&quot;: { &quot;version&quot;: &quot;1.17.7&quot; }, &quot;kubernetesCluster&quot;: { &quot;major&quot;: &quot;1&quot;, &quot;minor&quot;: &quot;30&quot;, &quot;gitVersion&quot;: &quot;v1.30.0&quot;, &quot;buildDate&quot;: &quot;2024-05-13T22:02:25Z&quot;, &quot;platform&quot;: &quot;linux/arm64&quot; } gloo 설치 123456789101112131415161718192021222324252627282930313233343536373839404142# [신규 터미널] 모니터링watch -d kubectl get pod,svc,endpointslices,ep -n gloo-system# Install Gloo Gateway## --set kubeGateway.enabled=true: Kubernetes Gateway 기능을 활성화합니다.## --set gloo.disableLeaderElection=true: Gloo의 리더 선출 기능을 비활성화합니다. (단일 인스턴스에서 Gloo를 실행 시 유용)## --set discovery.enabled=false: 서비스 디스커버리 기능을 비활성화합니다.helm repo add gloo https://storage.googleapis.com/solo-public-helmhelm repo updatehelm install -n gloo-system gloo-gateway gloo/gloo \\--create-namespace \\--version 1.17.7 \\--set kubeGateway.enabled=true \\--set gloo.disableLeaderElection=true \\--set discovery.enabled=false# Confirm that the Gloo control plane has successfully been deployed using this commandkubectl rollout status deployment/gloo -n gloo-system# 설치 확인kubectl get crd | grep 'networking.k8s.io'kubectl get crd | grep -v 'networking.k8s.io'kubectl get pod,svc,endpointslices -n gloo-system#kubectl explain gatewayclasseskubectl get gatewayclassesNAME CONTROLLER ACCEPTED AGEgloo-gateway solo.io/gloo-gateway True 21mkubectl get gatewayclasses -o yamlapiVersion: v1items:- apiVersion: gateway.networking.k8s.io/v1 kind: GatewayClass metadata: labels: app: gloo name: gloo-gateway spec: controllerName: solo.io/gloo-gateway... 접속확인 1234567891011121314151617181920212223242526272829303132333435#watch -d kubectl get pod,svc,endpointslices,ep -n httpbin# Install Httpbin Applicationkubectl apply -f https://raw.githubusercontent.com/solo-io/solo-blog/main/gateway-api-tutorial/01-httpbin-svc.yaml# 설치 확인kubectl get deploy,pod,svc,endpointslices,sa -n httpbinkubectl rollout status deploy/httpbin -n httpbin# (옵션) NodePort 설정cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: labels: app: httpbin service: httpbin name: httpbin namespace: httpbinspec: type: NodePort ports: - name: http port: 8000 targetPort: 80 nodePort: 30000 selector: app: httpbinEOF# (옵션) 로컬 접속 확인echo &quot;httpbin web - http://localhost:30000&quot; # macOS 사용자httpbin web - http://localhost:30000 Configure a Gateway Listener Let’s begin by establishing a Gateway resource that sets up an HTTP listener on port 8080 to expose routes from all our namespaces. Gateway custom resources like this are part of the Gateway API standard. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 02-gateway.yamlkind: GatewayapiVersion: gateway.networking.k8s.io/v1metadata: name: httpspec: gatewayClassName: gloo-gateway listeners: - protocol: HTTP port: 8080 name: http allowedRoutes: namespaces: from: All# gateway 리소스 생성kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/02-gateway.yaml# 확인 : Now we can confirm that the Gateway has been activatedkubectl get gateway -n gloo-system# You can also confirm that Gloo Gateway has spun up an Envoy proxy instance in response to the creation of this Gateway object by deploying gloo-proxy-http:kubectl get deployment gloo-proxy-http -n gloo-systemNAME READY UP-TO-DATE AVAILABLE AGEgloo-proxy-http 1/1 1 1 5m22s# envoy 사용 확인kubectl get pod -n gloo-systemkubectl describe pod -n gloo-system |grep Image: Image: quay.io/solo-io/gloo-envoy-wrapper:1.17.7 Image: quay.io/solo-io/gloo:1.17.7 Image: quay.io/solo-io/gloo-envoy-wrapper:1.17.7# gloo-proxy-http 서비스는 External-IP는 Pending 상태kubectl get svc -n gloo-system gloo-proxy-httpNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgloo-proxy-http LoadBalancer 10.96.71.22 &lt;pending&gt; 8080:31555/TCP 2m4s# gloo-proxy-http NodePort 30001 설정cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: labels: app.kubernetes.io/instance: http app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: gloo-proxy-http app.kubernetes.io/version: 1.17.7 gateway.networking.k8s.io/gateway-name: http gloo: kube-gateway helm.sh/chart: gloo-gateway-1.17.7 name: gloo-proxy-http namespace: gloo-systemspec: ports: - name: http nodePort: 30001 port: 8080 selector: app.kubernetes.io/instance: http app.kubernetes.io/name: gloo-proxy-http gateway.networking.k8s.io/gateway-name: http type: LoadBalancerEOFkubectl get svc -n gloo-system gloo-proxy-httpNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgloo-proxy-http LoadBalancer 10.96.227.35 &lt;pending&gt; 8080:30001/TCP 51s Configure Simple Routing with an HTTPRoute HTTPRoute is one of the new Kubernetes CRDs introduced by the Gateway API, as documented here. We’ll start by introducing a simple HTTPRoute for our service. 123456789101112131415161718192021apiVersion: gateway.networking.k8s.io/v1beta1kind: HTTPRoutemetadata: name: httpbin namespace: httpbin labels: example: httpbin-routespec: parentRefs: - name: http namespace: gloo-system hostnames: - &quot;api.example.com&quot; rules: - matches: - path: type: Exact value: /get backendRefs: - name: httpbin port: 8000 This example attaches to the default Gateway object created for us when we installed Gloo Gateway earlier.See the gloo-system/http reference in the parentRefs stanza. 12345678910# Our route watches for HTTP requests directed at the host api.example.com with the request path /get and then forwards the request to the httpbin service on port 8000.# Let’s establish this route now:kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/03-httpbin-route.yaml#kubectl get httproute -n httpbinNAME HOSTNAMES AGEhttpbin [&quot;api.example.com&quot;] 3m15skubectl describe httproute -n httpbin Test the Simple Route with Curl12345# let’s use curl to display the response with the -i option to additionally show the HTTP response code and headers.echo &quot;127.0.0.1 api.example.com&quot; | sudo tee -a /etc/hostsecho &quot;httproute - http://api.example.com:30001/get&quot; # 웹브라우저혹은curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/get # kubectl port-forward 사용 시 12345678910# 호출 응답 왜 그럴까?curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/delay/1HTTP/1.1 404 Not Founddate: Wed, 03 Jul 2024 07:19:21 GMTserver: envoycontent-length: 0# nodeport 직접 접속echo &quot;httproute - http://api.example.com:30000/delay/1&quot; # 1초 후 응답echo &quot;httproute - http://api.example.com:30000/delay/5&quot; # 5초 후 응답 [정규식 패턴 매칭] Explore Routing with Regex Matching Patterns1234567891011121314151617181920212223242526272829#echo &quot;httproute - http://api.example.com:30001/api/httpbin/get&quot; # 웹브라우저혹은curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/api/httpbin/get # kubectl port-forward 사용 시HTTP/1.1 200 OKserver: envoydate: Sun, 06 Oct 2024 08:08:09 GMTcontent-type: application/jsoncontent-length: 289access-control-allow-origin: *access-control-allow-credentials: truex-envoy-upstream-service-time: 18 # envoy 가 업스트림 httpbin 요청 처리에 걸리 시간 0.018초{ &quot;args&quot;: {}, &quot;headers&quot;: { &quot;Accept&quot;: &quot;*/*&quot;, &quot;Host&quot;: &quot;api.example.com&quot;, &quot;User-Agent&quot;: &quot;curl/8.7.1&quot;, &quot;X-Envoy-Expected-Rq-Timeout-Ms&quot;: &quot;15000&quot;, &quot;X-Envoy-Original-Path&quot;: &quot;/api/httpbin/get&quot; }, &quot;origin&quot;: &quot;10.244.0.11&quot;, &quot;url&quot;: &quot;http://api.example.com/get&quot;}# 아래 NodePort 와 GW API 통한 접속 비교echo &quot;httproute - http://api.example.com:30001/api/httpbin/get&quot;echo &quot;httproute - http://api.example.com:30000/api/httpbin/get&quot; # NodePort 직접 접근 123456789101112131415161718192021222324252627282930---#echo &quot;httproute - http://api.example.com:30001/api/httpbin/delay/1&quot; # 웹브라우저혹은curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/api/httpbin/delay/1 # kubectl port-forward 사용 시HTTP/1.1 200 OKserver: envoydate: Wed, 03 Jul 2024 07:31:47 GMTcontent-type: application/jsoncontent-length: 342access-control-allow-origin: *access-control-allow-credentials: truex-envoy-upstream-service-time: 1023 # envoy 가 업스트림 httpbin 요청 처리에 걸리 시간 1초 이상{ &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: {}, &quot;headers&quot;: { &quot;Accept&quot;: &quot;*/*&quot;, &quot;Host&quot;: &quot;api.example.com&quot;, &quot;User-Agent&quot;: &quot;curl/8.6.0&quot;, &quot;X-Envoy-Expected-Rq-Timeout-Ms&quot;: &quot;15000&quot;, &quot;X-Envoy-Original-Path&quot;: &quot;/api/httpbin/delay/1&quot; }, &quot;origin&quot;: &quot;10.244.0.7&quot;, &quot;url&quot;: &quot;http://api.example.com/delay/1&quot;} [업스트림 베어러 토큰을 사용한 변환] Test Transformations with Upstream Bearer Tokens 목적 : 요청을 라우팅하는 백엔드 시스템 중 하나에서 인증해야 하는 요구 사항이 있는 경우는 어떻게 할까요? 이 업스트림 시스템에는 권한 부여를 위한 API 키가 필요하고, 이를 소비하는 클라이언트에 직접 노출하고 싶지 않다고 가정해 보겠습니다. 즉, 프록시 계층에서 요청에 주입할 간단한 베어러 토큰을 구성하고 싶습니다. (정적 API 키 토큰을 직접 주입) 설치 123456789101112131415161718192021222324252627282930#kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/05-httpbin-rewrite-xform.yaml#kubectl describe httproute -n httpbin...Spec: ... Rules: Backend Refs: Group: Kind: Service Name: httpbin Port: 8000 Weight: 1 Filters: Type: URLRewrite URL Rewrite: Path: Replace Prefix Match: / Type: ReplacePrefixMatch Request Header Modifier: Add: Name: Authorization Value: Bearer my-api-key Type: RequestHeaderModifier Matches: Path: Type: PathPrefix Value: /api/httpbin/ 테스트 1234567891011121314151617181920212223242526#echo &quot;httproute - http://api.example.com:30001/api/httpbin/get&quot; # 웹브라우저혹은curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/api/httpbin/get # kubectl port-forward 사용 시HTTP/1.1 200 OKserver: envoydate: Sun, 06 Oct 2024 08:20:00 GMTcontent-type: application/jsoncontent-length: 332access-control-allow-origin: *access-control-allow-credentials: truex-envoy-upstream-service-time: 11{ &quot;args&quot;: {}, &quot;headers&quot;: { &quot;Accept&quot;: &quot;*/*&quot;, &quot;Authorization&quot;: &quot;Bearer my-api-key&quot;, &quot;Host&quot;: &quot;api.example.com&quot;, &quot;User-Agent&quot;: &quot;curl/8.7.1&quot;, &quot;X-Envoy-Expected-Rq-Timeout-Ms&quot;: &quot;15000&quot;, &quot;X-Envoy-Original-Path&quot;: &quot;/api/httpbin/get&quot; }, &quot;origin&quot;: &quot;10.244.0.11&quot;, &quot;url&quot;: &quot;http://api.example.com/get&quot;} Migrate123456# You should see the response below, indicating deployments for both v1 and v2 of my-workload have been created in the my-workload namespace.kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/06-workload-svcs.yaml# v1,v2 2가지 버전 워크로드 확인kubectl get deploy,pod,svc,endpointslices -n my-workload Test Simple V1 Routing 12345678910111213141516171819202122apiVersion: gateway.networking.k8s.io/v1beta1kind: HTTPRoutemetadata: name: my-workload namespace: my-workload labels: example: my-workload-routespec: parentRefs: - name: http namespace: gloo-system hostnames: - &quot;api.example.com&quot; rules: - matches: - path: type: PathPrefix value: /api/my-workload backendRefs: - name: my-workload-v1 namespace: my-workload port: 8080 HTTPRoute 설치 1234567891011121314151617181920212223242526272829303132#kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/07-workload-route.yaml#kubectl get httproute -ANAMESPACE NAME HOSTNAMES AGEhttpbin httpbin [&quot;api.example.com&quot;] 41mmy-workload my-workload [&quot;api.example.com&quot;] 39s#kubectl describe httproute -n my-workload...Spec: Hostnames: api.example.com Parent Refs: Group: gateway.networking.k8s.io Kind: Gateway Name: http Namespace: gloo-system Rules: Backend Refs: Group: Kind: Service Name: my-workload-v1 Namespace: my-workload Port: 8080 Weight: 1 Matches: Path: Type: PathPrefix Value: /api/my-workload workload-v1 에만 접속됨을 확인 Simulate a v2 Dark Launch with Header-Based RoutingDark Launch : 일부 사용자에게 새로운 기능을 출시하여 피드백을 수집하고 잠재적으로 더 큰 사용자 커뮤니티를 방해하기 전에 개선 사항을 실험하는 훌륭한 클라우드 마이그레이션 기술 123456789101112131415161718192021222324rules: - matches: - path: type: PathPrefix value: /api/my-workload # Add a matcher to route requests with a v2 version header to v2 # version=v2 헤더값이 있는 사용자만 v2 라우팅 headers: - name: version value: v2 backendRefs: - name: my-workload-v2 namespace: my-workload port: 8080 - matches: # Route requests without the version header to v1 as before # 대다수 일반 사용자는 기존 처럼 v1 라우팅 - path: type: PathPrefix value: /api/my-workload backendRefs: - name: my-workload-v1 namespace: my-workload port: 8080 123456789#kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/08-workload-route-header.yaml# kubectl describe httproute -n my-workload...Spec: ... 12345678# Now we’ll test the original route, with no special headers supplied, and confirm that traffic still goes to v1:curl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/api/my-workloadcurl -is -H &quot;Host: api.example.com&quot; http://localhost:8080/api/my-workload | grep body&quot;body&quot;: &quot;Hello From My Workload (v1)!&quot;,# But it we supply the version: v2 header, note that our gateway routes the request to v2 as expected:curl -is -H &quot;Host: api.example.com&quot; -H &quot;version: v2&quot; http://localhost:8080/api/my-workloadcurl -is -H &quot;Host: api.example.com&quot; -H &quot;version: v2&quot; http://localhost:8080/api/my-workload | grep body 헤더에 version: v1 혹은 version: v2 를 작성하면 해당하는 팟에 접근됨 Expand V2 Testing with Percentage-Based Routing성공적인 다크 런칭 이후, 우리는 점진적으로 이전 버전에서 새 버전으로 사용자 트래픽을 옮기는 블루-그린 전략을 사용하는 기간을 원할 수 있습니다. 트래픽을 균등하게 분할하고 트래픽의 절반을 로 보내고 v1나머지 절반을 로 보내는 라우팅 정책으로 이를 살펴보겠습니다 v2. 123456789101112131415161718192021# Apply this 50-50 routing policy with kubectl:kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/09-workload-route-split.yaml#kubectl describe httproute -n my-workload... rules: - matches: - path: type: PathPrefix value: /api/my-workload # Configure a 50-50 traffic split across v1 and v2 : 버전 1,2 50:50 비율 backendRefs: - name: my-workload-v1 namespace: my-workload port: 8080 weight: 50 - name: my-workload-v2 namespace: my-workload port: 8080 weight: 50 1for i in {1..100}; do curl -s -H &quot;Host: api.example.com&quot; http://localhost:8080/api/my-workload/ | grep body; done | sort | uniq -c | sort -nr 트래픽 에 따라 my-workload-v1, my-workload-v2에 번갈아 접속됨","link":"/blog/2024/12/01/docs/gateway/gloo/"},{"title":"Calico Communication","text":"1. 파드(Pod) ↔ 파드간 통신12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: pod1spec: nodeName: k8s-w1 # 노드의 호스트이름을 직접 지정했습니다 containers: - name: pod1 image: nicolaka/netshoot # 이미지는 네트워크 장애 처리에 유용한 이미지를 사용합니다 command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: pod2spec: nodeName: k8s-w1 containers: - name: pod2 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0 123456789101112131415161718192021222324252627282930313233# 네트워크 인터페이스 정보 확인 : 터널(ipip) 인터페이스가 존재!ip -c -d addr show tunl0# 네트워크 네임스페이스 확인lsns -t net# 네트워크 라우팅 경로 정보 확인# 이중 bird 는 bird 데몬이 BGP 라우팅 프로토콜에 의해 파드 네트워크 대역을 전달받거나 전달하는 경로 → 각각 노드의 파드 대역입니다Quiz. blackhole 라우팅은 왜 있을까요?-&gt; 루핑 방지 를 위해서 blackhole(null) 라우팅이 존재.-&gt; 유효하지 않은 IP 대역으로 들어오는 트래픽을 조기에 차단하여 불필요한 라우팅과 네트워크 자원 낭비를 방지ip -c route | grep bird# 아래 tunl0 Iface 에 목적지 네트워크 대역은 ipip 인캡슐레이션에 의해서 각 노드에 전달됩니다 → 각각 노드의 파드 대역입니다route -nroot@k8s-w1:~# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.10.1 0.0.0.0 UG 100 0 0 ens5172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.116.0 192.168.10.10 255.255.255.0 UG 0 0 0 tunl0172.16.158.0 0.0.0.0 255.255.255.0 U 0 0 0 *172.16.158.1 0.0.0.0 255.255.255.255 UH 0 0 0 calice0906292e2172.16.158.2 0.0.0.0 255.255.255.255 UH 0 0 0 calibd2348b4f67172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 tunl0192.168.0.2 192.168.10.1 255.255.255.255 UGH 100 0 0 ens5192.168.10.0 0.0.0.0 255.255.255.0 U 100 0 0 ens5192.168.10.1 0.0.0.0 255.255.255.255 UH 100 0 0 ens5# (옵션) iptables rule 갯수 확인 &gt;&gt; iptables rule 이 모든 노드에서 똑같나요? 다른가요?-&gt; pod개수에 따라 filter rule이 늘어남을 확인.iptables -t filter -S | grep cali | wc -liptables -t nat -S | grep cali | wc -l pod1, pod2 가상인터페이스 확인. Pod 간 통신 123pod1:~# ip neigh169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE192.168.10.101 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE 라우팅 정보에 169.254.1.1을 디폴트 게이트웨이 주소로 사용 calice 인터페이스(파드에 연결된 가상 인터페이스)에 proxy_arp 파일 (활성화) ARP 정보를 어떻게 가져올까? - proxy_arp 기능을 이용 파드1에서 게이트웨이의 IP인 169.254.1.1 의 MAC 주소를 알기 위해서 ARP Request 를 보낸다 이때 veth 연결된 calice#~ 에 proxy arp 설정이 되어 있고, 자신의 mac 주소를 알려주고, 이후 정상 통신됨 2. 파드 → 외부(인터넷) 통신파드에서 외부(인터넷) 통신 시에는 해당 노드의 네트워크 인터페이스 IP 주소로 MASQUERADE(출발지 IP가 변경) 되어서 외부에 연결됨 파드 배포 전 기본 상태 확인1234567891011121314151617181920212223# 마스터 노드에서 확인 : natOutgoing 의 기본값은 true 이다calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTORdefault-ipv4-ippool 172.16.0.0/16 true Always Never false all()# 노드에서 확인 : 노드에서 외부로 통신 시 MASQUERADE 동작 Rule 확인iptables -n -t nat --list cali-nat-outgoingroot@k8s-w1:~# iptables -n -t nat --list cali-nat-outgoingChain cali-nat-outgoing (1 references)target prot opt source destinationMASQUERADE all -- 0.0.0.0/0 0.0.0.0/0 /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipam-pools dst random-fullyipset listipset list cali40masq-ipam-pools Name: cali40masq-ipam-pools Type: hash:net Revision: 7 Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0x33f4554b Size in memory: 504 References: 1 Number of entries: 1 Members: 172.16.0.0/16 파드 배포 및 외부 통신 확인123456789101112apiVersion: v1kind: Podmetadata: name: pod1spec: nodeName: k8s-w1 containers: - name: pod1 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 파드에서 외부 정상 통신 확인kubectl exec pod1 -it -- zsh----------------------------# 혹은 통신 확인 pod1&gt; ping -c 10 8.8.8.8# The right way to check the weather - 링크curl wttr.in/seoulcurl 'wttr.in/seoul?format=3'curl 'wttr.in/busan?format=3'curl -s 'wttr.in/{London,Busan}'curl v3.wttr.in/Seoul.sxlcurl wttr.in/Mooncurl wttr.in/:help# 패킷 덤프 내용 확인tcpdump -i &lt;각자 실습 환경에 따라 다름&gt; -nn icmproot@k8s-w1:~# tcpdump -i calice0906292e2 -nn icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on calice0906292e2, link-type EN10MB (Ethernet), capture size 262144 bytes09:27:29.236878 IP 172.16.228.82 &gt; 8.8.8.8: ICMP echo request, id 56376, seq 1, length 6409:27:29.311810 IP 8.8.8.8 &gt; 172.16.228.82: ICMP echo reply, id 56376, seq 1, length 64# [실습환경 A Type] 아래 192.168.10.101는 노드의 외부(인터넷) 연결된 네트워크 인터페이스의 IP이며, 출발지 IP가 변경되어서 외부로 나감root@k8s-w1:~# tcpdump -i ens5 -nn icmp15:37:56.579286 IP 192.168.10.101 &gt; 8.8.8.8: ICMP echo request, id 57122, seq 1, length 6415:37:56.610585 IP 8.8.8.8 &gt; 192.168.10.101: ICMP echo reply, id 57122, seq 1, length 64# [실습환경 B Type] 아래 10.0.2.15는 VM의 1번 네트워크 인터페이스의 IP이며, 출발지 IP가 변경되어서 외부로 나감root@k8s-w1:~# tcpdump -i enp0s3 -nn icmp06:05:12.356260 IP 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 15671, seq 5, length 6406:05:12.402586 IP 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 15671, seq 5, length 64# nat MASQUERADE rule 카운트(pkts)가 증가!## 출발지 매칭은 cali40masq-ipam-pools 을 사용watch -d 'iptables -n -v -t nat --list cali-nat-outgoing'root@k8s-w1:~# iptables -n -v -t nat --list cali-nat-outgoingChain cali-nat-outgoing (1 references) pkts bytes target prot opt in out source destination 3 252 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipam-pools dst random-fully# IPSET 으로 의 cali40masq-ipam-pools IP 대역 정보 확인 : 172.16.0.0/16 대역임을 확인ipset list cali40masq-ipam-poolsName: cali40masq-ipam-poolsType: hash:netRevision: 7Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0x97754149Size in memory: 504References: 1Number of entries: 1Members:172.16.0.0/16 3. 다른 노드에서 파드 ↔ 파드간 통신다른 노드 환경에서 파드 간 통신 시에는 IPIP 터널(기본값) 모드를 통해서 이루어 집니다 각 노드에 파드 네트워크 대역은 Bird 에 의해서 BGP 로 광고 전파/전달 되며, Felix 에 의해서 호스트의 라우팅 테이블에 자동으로 추가 및 삭제 됩니다 다른 노드 간의 파드 통신은 tunl0 인터페이스를 통해 IP 헤더에 감싸져서 상대측 노드로 도달 후 tunl0 인터페이스에서 Outer 헤더를 제거하고 내부의 파드와 통신됩니다 - 링크 파드 배포 전 기본 상태 확인 12345678910111213141516171819# 아래 명령어로 확인 시 나머지 노드들의 파드 대역을 자신의 호스트 라우팅 테이블에 가지고 있고, 해당 경로는 tunl0 인터페이스로 보내게 된다route | head -2 ; route -n | grep tunl0# 마스터노드, 노드1~root@k8s-m:~# route | head -2 ; route -n | grep tunl0Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.158.0 192.168.10.101 255.255.255.0 UG 0 0 0 tunl0172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 tunl0# 노드1root@k8s-w1:~# route | head -2 ; route -n | grep tunl0Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.116.0 192.168.10.10 255.255.255.0 UG 0 0 0 tunl0172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 tunl0... 1234567891011121314151617181920212223# 터널 인터페이스에 IP가 할당되어 있고, MTU는 1480(IP헤더 20바이트 추가를 고려)을 사용하며, 현재 TX/RX 카운트는 0 이다# Calico 사용 시 파드의 인터페이스도 기본 MTU 1480 을 사용한다ifconfig tunl0# 노드1root@k8s-w1:~# ifconfig tunl0tunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 8981 inet 172.16.158.0 netmask 255.255.255.255 tunnel txqueuelen 1000 (IPIP Tunnel) RX packets 1 bytes 149 (149.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1 bytes 70 (70.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 노드2root@k8s-w2:~# ifconfig tunl0tunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 8981 inet 172.16.184.0 netmask 255.255.255.255 tunnel txqueuelen 1000 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 파드 배포12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: pod1spec: nodeName: k8s-w1 containers: - name: pod1 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: pod2spec: nodeName: k8s-w2 containers: - name: pod2 image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0 배포 확인 1234(⎈|HomeLab:default) root@k8s-m:~# calicoctl get workloadendpointsWORKLOAD NODE NETWORKS INTERFACEpod1 k8s-w1 172.16.158.4/32 calice0906292e2pod2 k8s-w2 172.16.184.1/32 calibd2348b4f67 파드간 통신을 처리하는 라우팅 정보 확인 : 각각 상대방 노드의 IP를 게이트웨이로 전달하게 됩니다! 1234567891011121314151617181920212223route -n | head -2 ; route -n | grep 172.16.# 노드1root@k8s-w1:~# route -n | head -2 ; route -n | grep 172.16.Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.116.0 192.168.10.10 255.255.255.0 UG 0 0 0 tunl0172.16.158.0 0.0.0.0 255.255.255.0 U 0 0 0 *172.16.158.4 0.0.0.0 255.255.255.255 UH 0 0 0 calice0906292e2172.16.184.0 192.168.10.102 255.255.255.0 UG 0 0 0 tunl0...# 노드2root@k8s-w2:~# route -n | head -2 ; route -n | grep 172.16.Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.34.0 192.168.20.100 255.255.255.0 UG 0 0 0 tunl0172.16.116.0 192.168.10.10 255.255.255.0 UG 0 0 0 tunl0172.16.158.0 192.168.10.101 255.255.255.0 UG 0 0 0 tunl0172.16.184.0 0.0.0.0 255.255.255.0 U 0 0 0 *172.16.184.1 0.0.0.0 255.255.255.255 UH 0 0 0 calibd2348b4f67... 파드간 통신 실행 및 확인지금까지 tunl0 인터페이스가 사용되지 않았는데, 다른 노드의 파드간 통신시에 사용되는지 확인을 해보자 노드1 정보12345678910111213141516171819202122232425262728293031323334353637# tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 실행watch -d 'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'root@k8s-w1:~# watch -d 'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'tunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1480 inet 172.16.228.76 netmask 255.255.255.255 RX packets 0 bytes 0 (0.0 B) TX packets 0 bytes 0 (0.0 B)# (옵션) 패킷 덤프 : calice#tcpdump -i -nn &lt;calice#&gt;# 패킷 덤프 : tunl0tcpdump -i tunl0 -nn혹은 패킷을 파일로 추출시tcpdump -i tunl0 -nn -w /tmp/calico-tunl0.pcap# 패킷 덤프 : IP 헤더에 상위 프로토콜을 IPIP(4)인 패킷만 필터#tcpdump -i &lt;각자 자신의 노드의 eth0&gt; -nn proto 4tcpdump -i ens5 -nn proto 4 # [실습환경 A Type]tcpdump -i enp0s8 -nn proto 4 # [실습환경 B Type]혹은 패킷을 파일로 추출시tcpdump -i ens5 -nn proto 4 -w /tmp/calico-ipip.pcap # [실습환경 A Type]tcpdump -i enp0s8 -nn proto 4 -w /tmp/calico-ipip.pcap # [실습환경 B Type]# [실습환경 A Type] 자신의 PC로 패킷 복사scp ubuntu@&lt;node 유동공인 IP&gt;:/tmp/calico-ipip.pcap .scp ubuntu@3.35.229.92:/tmp/calico-ipip.pcap .# [실습환경 B Type] 자신의 PC로 패킷 복사 : https://github.com/invernizzi/vagrant-scp## vagrant scp 플러그인 설치vagrant plugin install vagrant-scpvagrant plugin list## k8s-w1 VM 내부의 파일을 자신의 PC(호스트)에 복사vagrant scp k8s-w1:/tmp/calico-ipip.pcap ./ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 마스터 노드에서 pod1 Shell 접속kubectl exec pod1 -it -- zsh# pod1 에서 pod2 로 핑 통신 : 정상 통신!ping -c 10 172.16.184.1pod1&gt; ping -c 10 172.16.184.1PING 172.16.184.1 (172.16.184.1) 56(84) bytes of data.64 bytes from 172.16.184.1: icmp_seq=1 ttl=62 time=0.637 ms64 bytes from 172.16.184.1: icmp_seq=2 ttl=62 time=0.279 ms64 bytes from 172.16.184.1: icmp_seq=3 ttl=62 time=0.316 ms# tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 확인 : TX/RX 패킷 카운트가 각각 10개로 증가했다watch -d 'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'Every 2.0s: ifconfig tunl0 | head -2 ; ifconfig tunl0 | ... k8s-w2: Sun Sep 22 05:15:39 2024tunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 8981 inet 172.16.184.0 netmask 255.255.255.255 RX packets 13 bytes 1092 (1.0 KB) TX packets 13 bytes 1092 (1.0 KB)# 패킷 덤프 : tunl0 - 터널 인터페이스에 파드간 IP 패킷 정보 확인!tcpdump -i tunl0 -nnroot@k8s-w2:~# watch -d 'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'root@k8s-w2:~# tcpdump -i tunl0 -nntcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on tunl0, link-type RAW (Raw IP), snapshot length 262144 bytes05:15:53.004279 IP 172.16.158.4 &gt; 172.16.184.1: ICMP echo request, id 99, seq 1, length 6405:15:53.004463 IP 172.16.184.1 &gt; 172.16.158.4: ICMP echo reply, id 99, seq 1, length 6405:15:54.006068 IP 172.16.158.4 &gt; 172.16.184.1: ICMP echo request, id 99, seq 2, length 6405:15:54.006154 IP 172.16.184.1 &gt; 172.16.158.4: ICMP echo reply, id 99, seq 2, length 64# 패킷 덤프 : eth0(enp#~) - IP Outer 헤더 안쪽에 IP 헤더 1개가 더 있음을 알 수 있다!tcpdump -i ens5 -nn proto 4 # [실습환경 A Type]tcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on ens5, link-type EN10MB (Ethernet), snapshot length 262144 bytes05:16:13.505744 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.4 &gt; 172.16.184.1: ICMP echo request, id 103, seq 1, length 6405:16:13.505853 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.1 &gt; 172.16.158.4: ICMP echo reply, id 103, seq 1, length 6405:16:14.506472 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.4 &gt; 172.16.184.1: ICMP echo request, id 103, seq 2, length 6405:16:14.506563 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.1 &gt; 172.16.158.4: ICMP echo reply, id 103, seq 2, length 6405:16:15.520181 IP 192.168.10.101 &gt; 192.168.10.102: IP 172.16.158.4 &gt; 172.16.184.1: ICMP echo request, id 103, seq 3, length 6405:16:15.520273 IP 192.168.10.102 &gt; 192.168.10.101: IP 172.16.184.1 &gt; 172.16.158.4: ICMP echo reply, id 103, seq 3, length 64tcpdump -i ens5 -nn proto 4 -w /tmp/calico-ipip.pcap # [실습환경 A Type]ls -l /tmp# [실습환경 A Type] 자신의 PC로 패킷 복사scp ubuntu@&lt;node 유동공인 IP&gt;:/tmp/calico-ipip.pcap .# [실습환경 B Type] 자신의 PC로 패킷 복사 : https://github.com/invernizzi/vagrant-scp## vagrant scp 플러그인 설치vagrant plugin install vagrant-scpvagrant plugin list## k8s-w1 VM 내부의 파일을 자신의 PC(호스트)에 복사vagrant scp k8s-w1:/tmp/calico-ipip.pcap ./","link":"/blog/2024/09/22/docs/calico/calico-communication/"},{"title":"PAUSE 컨테이너","text":"파드는 1개 이상의 컨테이너로 구성된 컨테이너의 집합이며, PAUSE 컨테이너가 Network/IPC/UTS 네임스페이스를 생성하고 유지/공유한다. 1234567891011121314151617181920212223242526272829303132# '컨트롤플레인, 워커 노드 1대' 클러스터 배포 : 파드에 접속하기 위한 포트 맵핑 설정cat &lt;&lt;EOT&gt; kind-2node.yamlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane- role: worker extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001EOTkind create cluster --config kind-2node.yaml --name myk8s# 툴 설치docker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'docker exec -it myk8s-worker sh -c 'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop -y'# 확인kubectl get nodes -o widedocker psdocker port myk8s-workerdocker exec -it myk8s-control-plane ip -br -c -4 addrdocker exec -it myk8s-worker ip -br -c -4 addr# kube-ops-viewhelm repo add geek-cookbook https://geek-cookbook.github.io/charts/helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 --set env.TZ=&quot;Asia/Seoul&quot; --namespace kube-system# 설치 확인kubectl get deploy,pod,svc,ep -n kube-system -l app.kubernetes.io/instance=kube-ops-view&quot; 격리자원 확인12345678910111213141516171819202122232425262728293031323334353637383940414243# [터미널1] myk8s-worker bash 진입 후 실행 및 확인docker exec -it myk8s-worker bash----------------------------------systemctl list-unit-files | grep 'enabled enabled'containerd.service enabled enabledkubelet.service enabled enabled...# 확인 : 파드내에 pause 컨테이너와 metrics-server 컨테이너, 네임스페이스 정보# pgrep metrics-serverpstree -aclnpsS... \\-containerd-shim,1776 -namespace k8s.io -id 6bd147995b3a6c17384459eb4d3ceab4369329e6b57c009bdc6257b72254e1fb -address /run/containerd/containerd.sock |-{containerd-shim},1777 ... |-pause,1797,ipc,mnt,net,pid,uts |-metrics-server,1896,cgroup,ipc,mnt,net,pid,uts --cert-dir=/tmp --secure-port=10250 --kubelet-insecure-tls --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s | |-{metrics-server},1912 ...# 특정 소켓 파일을 사용하는 프로세스 확인lsof /run/containerd/containerd.sockss -xl | egrep 'Netid|containerd'findmnt -ATARGET SOURCE FSTYPE OPTIONS/ overlay overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/HW4BGGJ4LV6M5...|-/sys sysfs sysfs ro,nosuid,nodev,noexec,relatime| |-/sys/kernel/debug debugfs debugfs rw,nosuid,nodev,noexec,relatime| |-/sys/kernel/tracing tracefs tracefs rw,nosuid,nodev,noexec,relatime| |-/sys/fs/fuse/connections fusectl fusectl rw,nosuid,nodev,noexec,relatime| |-/sys/kernel/config configfs configfs rw,nosuid,nodev,noexec,relatime| \\-/sys/fs/cgroup cgroup cgroup2 rw,nosuid,nodev,noexec,relatimefindmnt -t cgroup2grep cgroup /proc/filesystemsstat -fc %T /sys/fs/cgroup/---------------------------------- 1pstree -aclnpsS containers test123456789101112131415161718apiVersion: v1kind: Podmetadata: name: myweb2spec: containers: - name: myweb2-nginx image: nginx ports: - containerPort: 80 protocol: TCP - name: myweb2-netshoot image: nicolaka/netshoot command: [&quot;/bin/bash&quot;] args: [&quot;-c&quot;, &quot;while true; do sleep 5; curl localhost; done&quot;] # 포드가 종료되지 않도록 유지합니다 terminationGracePeriodSeconds: 0 1kubectl apply -f myweb2.yaml 12345678910kubectl exec myweb2 -c myweb2-netshoot -- ip addrkubectl exec myweb2 -c myweb2-nginx -- apt updatekubectl exec myweb2 -c myweb2-nginx -- apt install -y net-toolskubectl exec myweb2 -c myweb2-nginx -- ifconfigNGINXPID=$(ps -ef | grep 'nginx -g' | grep -v grep | awk '{print $2}')echo $NGINXPIDNETSHPID=$(ps -ef | grep 'curl' | grep -v grep | awk '{print $2}')echo $NETSHPID ns, ip 비교12345678910111213141516# 개별 컨테이너에 명령 실행 : IP 동일 확인crictl pscrictl ps -qcrictl exec -its &lt;myweb2-nginx 컨테이너ID&gt; ifconfigcrictl exec -its &lt;myweb2-netshoot 컨테이너ID&gt; ifconfig# PAUSE 의 NET 네임스페이스 PID 확인 및 IP 정보 확인lsns -t netnsenter -t $PAUSEPID -n ip -c addrnsenter -t $NGINXPID -n ip -c addrnsenter -t $NETSHPID -n ip -c addr# 2개의 네임스페이스 비교 , 아래 2112 프로세스의 정제는?crictl inspect &lt;myweb2-nginx 컨테이너ID&gt; | jqcrictl inspect &lt;myweb2-netshoot 컨테이너ID&gt; | jq 자원정리12kubectl delete pod myweb2kind delete cluster --name myk8s","link":"/blog/2024/09/08/docs/pause-container/"},{"title":"bookinfo","text":"Bookinfo 실습 &amp; Istio 기능Bookinfo 애플리케이션 소개 : 4개의 마이크로서비스로 구성 : Productpage, reviews, ratings, details - 링크 ProductPage 페이지에서 요청을 받으면, 도서 리뷰를 보여주는 Reviews 서비스와 도서 상세 정보를 보여주는 Details 서비스에 접속하고, ProductPage 는 Reviews 와 Details 결과를 사용자에게 응답한다. Reviews 서비스는 v1, v2, v3 세 개의 버전이 있고 v2, v3 버전의 경우 Ratings 서비스에 접소갛여 도서에 대한 5단계 평가를 가져옴. Reviews 서비스의 차이는, v1은 Rating 이 없고, v2는 검은색 별로 Ratings 가 표시되며, v3는 색깔이 있는 별로 Ratings 가 표시됨. 12345678910111213141516# 모니터링watch -d 'kubectl get pod -owide;echo;kubectl get svc'# Bookinfo 애플리케이션 배포echo $ISTIOVcat ~/istio-$ISTIOV/samples/bookinfo/platform/kube/bookinfo.yamlkubectl apply -f ~/istio-$ISTIOV/samples/bookinfo/platform/kube/bookinfo.yaml# 확인kubectl get all,sa# product 웹 접속 확인kubectl exec &quot;$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')&quot; -c ratings -- curl -sS productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;# 로그kubetail -l app=productpage -f Istio 를 통한 인입 기본 설정123456789101112131415161718# Istio Gateway/VirtualService 설정cat ~/istio-$ISTIOV/samples/bookinfo/networking/bookinfo-gateway.yamlkubectl apply -f ~/istio-$ISTIOV/samples/bookinfo/networking/bookinfo-gateway.yaml# 확인kubectl get gw,vsistioctl proxy-statusNAME CLUSTER CDS LDS EDS RDS ECDS ISTIOD VERSIONdetails-v1-65cfcf56f9-4drsk.default Kubernetes SYNCED (7m4s) SYNCED (7m4s) SYNCED (6m57s) SYNCED (7m4s) IGNORED istiod-7f8b586864-mv944 1.23.2istio-ingressgateway-5f9f654d46-c4g7s.istio-system Kubernetes SYNCED (3m7s) SYNCED (3m7s) SYNCED (6m57s) SYNCED (3m7s) IGNORED istiod-7f8b586864-mv944 1.23.2productpage-v1-d5789fdfb-5cr6m.default Kubernetes SYNCED (6m59s) SYNCED (6m59s) SYNCED (6m57s) SYNCED (6m59s) IGNORED istiod-7f8b586864-mv944 1.23.2ratings-v1-7c9bd4b87f-9q4nv.default Kubernetes SYNCED (7m3s) SYNCED (7m3s) SYNCED (6m57s) SYNCED (7m3s) IGNORED istiod-7f8b586864-mv944 1.23.2reviews-v1-6584ddcf65-rqgp7.default Kubernetes SYNCED (7m2s) SYNCED (7m2s) SYNCED (6m57s) SYNCED (7m2s) IGNORED istiod-7f8b586864-mv944 1.23.2reviews-v2-6f85cb9b7c-h6m7p.default Kubernetes SYNCED (7m2s) SYNCED (7m2s) SYNCED (6m57s) SYNCED (7m2s) IGNORED istiod-7f8b586864-mv944 1.23.2reviews-v3-6f5b775685-rprpb.default Kubernetes SYNCED (6m58s) SYNCED (6m58s) SYNCED (6m57s) SYNCED (6m58s) IGNORED istiod-7f8b586864-mv944 1.23.2# productpage 파드의 istio-proxy 로그 확인 Access log 가 출력 - Default access log format : 링크kubetail -l app=productpage -c istio-proxy -f k3s-s NodePort 접속 확인 1234567891011121314151617#export IGWHTTP=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.spec.ports[1].nodePort}')echo $IGWHTTP32759# 접속 확인kubectl get svc -n istio-system istio-ingressgatewaycurl -s http://localhost:$IGWHTTP/productpagecurl -s http://192.168.10.101:$IGWHTTP/productpagecurl -s http://192.168.10.102:$IGWHTTP/productpage# 정보 확인echo $MYDOMAINcat /etc/hosts#curl -s http://$MYDOMAIN:$IGWHTTP/productpage 12345678910#echo $MYDOMAIN $IGWHTTPcat /etc/hosts#curl -v -s $MYDOMAIN:$IGWHTTP/productpageecho -e &quot;http://$MYDOMAIN:$IGWHTTP/productpage&quot;#aws ec2 describe-instances --query &quot;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}&quot; --filters Name=instance-state-name,Values=running --output text 1234# istio ingress gw 를 통한 접속 테스트curl -s $MYDOMAIN:$IGWHTTP/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;while true; do curl -s $MYDOMAIN:$IGWHTTP/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot; ; echo &quot;--------------&quot; ; sleep 1; donefor i in {1..100}; do curl -s $MYDOMAIN:$IGWHTTP/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot; ; done 모니터링 - Blog1234567891011121314151617181920212223242526272829# Install Kiali and the other addons and wait for them to be deployed. : Kiali dashboard, along with Prometheus, Grafana, and Jaeger.tree ~/istio-$ISTIOV/samples/addons/kubectl apply -f ~/istio-$ISTIOV/samples/addons # 디렉터리에 있는 모든 yaml 자원을 생성kubectl rollout status deployment/kiali -n istio-system# 확인kubectl get all,sa,cm -n istio-systemkubectl get svc,ep -n istio-system# kiali 서비스 변경kubectl patch svc -n istio-system kiali -p '{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'# kiali 웹 접속 주소 확인KIALINodePort=$(kubectl get svc -n istio-system kiali -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;KIALI UI URL = http://$(curl -s ipinfo.io/ip):$KIALINodePort&quot;# Grafana 서비스 변경kubectl patch svc -n istio-system grafana -p '{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'# Grafana 웹 접속 주소 확인 : 7개의 대시보드GRAFANANodePort=$(kubectl get svc -n istio-system grafana -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Grafana URL = http://$(curl -s ipinfo.io/ip):$GRAFANANodePort&quot;# Prometheus 서비스 변경kubectl patch svc -n istio-system prometheus -p '{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'# Prometheus 웹 접속 주소 확인PROMENodePort=$(kubectl get svc -n istio-system prometheus -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Prometheus URL = http://$(curl -s ipinfo.io/ip):$PROMENodePort&quot; Traffic ManagementTraffic Managenet : 트래픽 컨트롤은 VirtualService 와 DestinationRule 설정을 통해서 동작한다 Gateway : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정 가능 VirtualService : 인입 처리할 hosts 설정, L7 PATH 별 라우팅, 목적지에 대한 정책 설정 가능 (envoy route config) - 링크 사용 예시 : 헤더 매칭에 따라서, 각기 다른 destination 으로 라우팅 1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: **VirtualService**metadata: name: reviewsspec: hosts: - reviews http: - **match**: - headers: end-user: exact: jason route: - **destination**: host: reviews subset: **v2** - route: - **destination**: host: reviews subset: **v3** VirtualService 는 DestinationRule 에서 설정된 **서브셋(subset)**을 사용하여 트래픽 컨트롤을 할 수 있다 hosts 필드 : 목적지 주소 - IP address, a DNS name (FQDN), 혹은 k8s svc 이름 , wildcard (”*”) prefixes Routing rules : HTTP 경우 - Match 필드(예, 헤더) , Destination(istio/envoy 에 등록된 대상, subnet 에 DestinationRule 활용) HTTPRoute : redirect , rewrite , fault(장애 주입) , mirror(복제, 기본 100%) , corsPolicy(CORS 삽입) , headers(헤더 조작) 등 - 링크 Routing rule precedence : Routing rules are evaluated in sequential order from top to bottom - 위에서 순차적 적용 DestinationRule : 실제 도착지(서비스와 1:1 연결)의 정교한 정책(부하분산, 연결 옵션, 서킷 브레이크, TLS 등)을 설정 - 링크 사용 예시 : 3개의 subsets for the my-svc destination service 에 3개의 subsets 이 있고, 이중 v1/v3 은 RAMDOM 이고 v2 는 ROUND_ROBIN 12345678910111213141516171819202122apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: my-destination-rulespec: host: my-svc trafficPolicy: loadBalancer: simple: RANDOM **subsets**: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v3 labels: version: v3 Load balancing options : Round robin(기본값) , Random , Weighted , Least requests - 링크 Destination Rule : TrafficPolicy , Subset , ConnectionPoolSettings 등 - 링크 서브셋(subsets)을 정의할 수 있어 마이크로서비스 버전별로 라우팅할 때 사용한다","link":"/blog/2024/12/15/docs/istio/bookinfo/"},{"title":"istio-traffic","text":"클라이언트(요청) → 파드(인입)1234567891011# 아래 처럼 정상적으로 웹 서버 접속 정보 출력 확인curl -s -v $MYDOMAIN:$IGWHTTPcurl -s $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;while true; do curl -s $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot; ; echo &quot;--------------&quot; ; sleep 1; donewhile true; do curl -s $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot; ; echo &quot;--------------&quot; ; sleep 0.1; donecurl -s --user-agent &quot;IPHONE&quot; $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;while true; do curl -s $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; echo &quot;--------------&quot; ; sleep 1; done# 로그 확인kubetail -l app=nginx-app -f Istio IngressGateway 파드 → 노드 인입12345678910111213141516171819root@k8s-w2:~# ip -c route | grep 172.16.46.11172.16.46.11 dev cali8b56182ddcf scope link# 노드의 NIC에서 패킷 덤프 시 파드의 목적지 IP로 바로 인입되는 것을 알 수 있다root@k8s-w2:~# tcpdump -i enp0s8 -nn tcp port 80 -q17:31:16.074031 IP 172.16.228.76.54522 &gt; 172.16.46.11.80: tcp 162617:31:16.074096 IP 172.16.46.11.80 &gt; 172.16.228.76.54522: tcp 017:31:16.075258 IP 172.16.46.11.80 &gt; 172.16.228.76.54522: tcp 159117:31:16.075598 IP 172.16.228.76.54522 &gt; 172.16.46.11.80: tcp 0root@k8s-w2:~# ngrep -d enp0s8 -tW byline port 80T 2022/02/11 20:08:17.034879 172.16.158.5:43372 -&gt; 172.16.184.1:80 [AP] #153GET / HTTP/1.1.host: www.gasida.dev:31526.user-agent: curl/7.68.0.accept: */*.x-forwarded-for: 192.168.10.254.x-forwarded-proto: http.x-envoy-internal: true. [파드 내부] IPTables 적용 → Istio-proxy 컨테이너 인입*12345678910111213141516171819202122232425262728# 아래 처럼 'istio-init 컨테이너' 의 로그에 iptables rules 설정을 확인할 수 있습니다.# 참고로, NAT Tables 만 설정되고, 그외(filter, mangle, raw 등)은 설정하지 않습니다.(istio-k8s:default) root@k8s-m:~# kubectl logs nginx-pod -c istio-init* nat-N ISTIO_INBOUND-N ISTIO_REDIRECT-N ISTIO_IN_REDIRECT-N ISTIO_OUTPUT-A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006-A PREROUTING -p tcp -j ISTIO_INBOUND-A ISTIO_INBOUND -p tcp --dport 22 -j RETURN-A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN-A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN-A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT-A OUTPUT -p tcp -j ISTIO_OUTPUT-A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN-A ISTIO_OUTPUT -j ISTIO_REDIRECTCOMMIT ‘Istio-proxy 컨테이너’의 15006 Listener 확인123456789101112131415161718192021# 변수 지정 : C1(Istio-proxy, Envoy , 단축키 지정lsns -t netps -ef |grep istio1337 347173 347155 0 18:52 ? 00:00:01 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --drain-strategy immediate --local-address-ip-version v4 --file-flush-interval-msec 1000 --disable-hot-restart --allow-unknown-static-fields -l warning --component-log-level misc:error --concurrency 2C1PID=347173alias c1=&quot;nsenter -t $C1PID -n&quot;# ss (socket statistics)로 시스템 소켓 상태 확인 : 15006 은 envoy 프로세스가 Listen 하고 있다root@k8s-w2:~# c1 ss -tpnl '( dport = :15006 or sport = :15006 )'State Recv-Q Send-Q Local Address:Port Peer Address:Port ProcessLISTEN 0 4096 0.0.0.0:15006 0.0.0.0:* users:((&quot;envoy&quot;,pid=3928,fd=37))LISTEN 0 4096 0.0.0.0:15006 0.0.0.0:* users:((&quot;envoy&quot;,pid=3928,fd=36))# 확인 예시c1 ss -tpnt '( dport = :15006 or sport = :15006 or sport = :80 or dport = :80 )'watch -d &quot;nsenter -t $C1PID -n ss -tpnt '( dport = :15006 or sport = :15006 or sport = :80 or dport = :80 )'&quot;# 연결된 소켓 강제 Reset# c0 ss -K dst 172.16.228.66 dport = 44526c1 ss -K dst 172.16.228.66c1 ss -K dst 172.16.46.13 [파드 내부] Istio-proxy 컨테이너 → IPTables 적용‘Istio-proxy 컨테이너’ 는 대리인(Proxy) 역할로, 출발지 IP를 127.0.0.6 으로 변경하여 ‘Nginx 컨테이너’와 연결을 한다123# 로그 내용 중 출발지 정보(127.0.0.6:50763)를 변경하고 전달하는 것을 알 수 있다(istio-k8s:default) root@k8s-m:~# kubectl logs nginx-pod -c istio-proxy -f[2021-12-15T18:05:56.334Z] &quot;GET / HTTP/1.1&quot; 200 - via_upstream - &quot;-&quot; 0 615 0 0 &quot;192.168.10.254&quot; &quot;curl/7.68.0&quot; &quot;0844349b-d290-994b-93b2-da36bf929c62&quot; &quot;www.gasida.dev:30384&quot; &quot;172.16.46.11:80&quot; inbound|80|| 127.0.0.6:50763 172.16.46.11:80 192.168.10.254:0 - default [파드 내부] IPTables 적용 → Nginx 컨테이너 인입파드 내에 IPTables 는 전역(?)으로 적용되므로, ‘Istio-proxy’ 의 인/아웃 시 트래픽 구별이 중요하다. ‘Istio-proxy’ 를 빠져나올때는 출발지 IP가 127.0.0.6 이므로 ISTIO_OUTPUT 에 적용되어 리턴되어 POSTROUTING 를 통해 nginx 로 도착한다 IPTables 확인 : istio-proxy → OUTPUT → ISTIO_OUTOUT(맨 상단 Rule 매칭으로 RETURN) -&gt; POSTROUTING -&gt; NGINX 컨테이너 123456789101112131415# 출발IP가 127.0.0.6 이고, 빠져나오는 인터페이스가(-o lo)에 매칭되어 리턴됨c1 iptables -v --numeric --table nat --list ISTIO_OUTPUTChain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 2 120 RETURN all -- * lo 127.0.0.6 0.0.0.0/0# POSTROUTING 은 특별한 rule 이 없으니 통과!Chain POSTROUTING (policy ACCEPT 144 packets, 12545 bytes) pkts bytes target prot opt in out source destination# 모니터링 예시c1 iptables -t nat --zeroc1 iptables -v --numeric --table nat --list ISTIO_OUTPUTwatch -d &quot;nsenter -t $C1PID -n iptables -v --numeric --table nat --list OUTPUT; echo ; nsenter -t $C1PID -n iptables -v --numeric --table nat --list ISTIO_OUTPUT; echo ; nsenter -t $C1PID -n iptables -v --numeric --table nat --list POSTROUTING&quot;watch -d &quot;nsenter -t $C1PID -n iptables -t nat -L -n -v&quot; 최종적으로 nginx 컨테이너에 클라이언트의 요청 트래픽이 도착한다. nginx 컨테이너의 웹 액세스 로그에서 확인 : XFF 헤더에서 클라이언트의 출발지 IP를 확인 파드(리턴 트래픽) → 클라이언트 nginx (웹 서버)컨테이너에서 리턴 트래픽(응답, 200 OK)를 클라이언트에 전달합니다. IPTables CT(Connection Table)에 정보를 참고해서 역변환 등이 적용되어 전달됩니다. 파드(요청) → 외부 웹서버파드에서 업데이트 나 패치 다운로드 처럼 외부 웹서버 나 인터넷 연결 과정에서의 트래픽의 흐름입니다. 파드 내 IPTables 적용 흐름 : 아래 (9) ~ (15) 까지의 과정을 먼저 설명합니다. [파드 내부] Client PC → Istio IngressGateway 파드 구간‘nginx 컨테이너’ 에서 외부 웹서버 요청 12345678# 아래 처럼 'nginx 컨테이너' 에서 외부 웹서버 요청kubectl exec -it nginx-pod -c nginx-container -- curl -s 192.168.10.254kubectl exec -it nginx-pod -c nginx-container -- curl -s http://wttr.in/seoulkubectl exec -it nginx-pod -c nginx-container -- curl -s wttr.in/seoul?format=3kubectl exec -it nginx-pod -c nginx-container -- curl -s 'wttr.in/{London,Busan}'kubectl exec -it nginx-pod -c nginx-container -- curl -s http://ipinfo.io/citykubectl exec -it nginx-pod -c nginx-container -- curl -k https://www.google.com | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;;echowhile true; do kubectl exec -it nginx-pod -c nginx-container -- curl -s http://ipinfo.io/city; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; echo &quot;--------------&quot; ; sleep 3; done 123(⎈|default:N/A) root@k3s-s:~# c1 conntrack -L --dst-nattcp 6 431348 ESTABLISHED src=172.16.1.4 dst=172.16.0.10 sport=36300 dport=80 src=172.16.0.10 dst=172.16.1.4 sport=15006 dport=36300 [ASSURED] mark=0 use=1conntrack v1.4.6 (conntrack-tools): 1 flow entries have been shown. [파드 내부] IPTables → Istio-proxy 컨테이너 인입파드 내 IPTables Chains/Rules 적용 (NAT 테이블) → ‘Istio-proxy 컨테이너’로 인입됩니다. 1234567891011121314151617181920# iptables 확인c1 iptables -t nat --zeroc1 iptables -v --numeric --table nat --list ISTIO_REDIRECTwatch -d &quot;nsenter -t $C1PID -n iptables -v --numeric --table nat --list OUTPUT; echo ; nsenter -t $C1PID -n iptables -v --numeric --table nat --list ISTIO_OUTPUT; echo ; nsenter -t $C1PID -n iptables -v --numeric --table nat --list ISTIO_REDIRECT&quot;watch -d &quot;nsenter -t $C1PID -n iptables -t nat -L -n -v&quot;# nginx 파드에서 TCP 트래픽 요청으로 인입 시, ISTIO_REDIRECT 에서 redir ports 15001 되어 'Istio-proxy 컨테이너'로 인입됩니다.c1 iptables -t nat -L -n -vChain OUTPUT (policy ACCEPT 5 packets, 455 bytes) pkts bytes target prot opt in out source destination 0 0 ISTIO_OUTPUT tcp -- * * 0.0.0.0/0 0.0.0.0/0Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination ... 0 0 ISTIO_REDIRECT all -- * * 0.0.0.0/0 0.0.0.0/0Chain ISTIO_REDIRECT (1 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15001 [파드 내부] Istio-proxy 컨테이너 → 노드의 호스트 네임스페이스‘Istio-proxy 컨테이너’ 는 대리인(Proxy) 역할로, 출발지 포트를 변경(+2) 후 외부 웹서버에 연결을 한다 12345678910111213141516(⎈|default:N/A) root@k3s-s:~# kubectl logs nginx-pod -c istio-proxy -f2024-10-19T19:56:27.631973Z info FLAG: --concurrency=&quot;0&quot;2024-10-19T19:56:27.632053Z info FLAG: --domain=&quot;default.svc.cluster.local&quot;2024-10-19T19:56:27.632062Z info FLAG: --help=&quot;false&quot;2024-10-19T19:56:27.632067Z info FLAG: --log_as_json=&quot;false&quot;2024-10-19T19:56:27.632070Z info FLAG: --log_caller=&quot;&quot;2024-10-19T19:56:27.632073Z info FLAG: --log_output_level=&quot;default:info&quot;2024-10-19T19:56:27.632076Z info FLAG: --log_rotate=&quot;&quot;2024-10-19T19:56:27.632079Z info FLAG: --log_rotate_max_age=&quot;30&quot;2024-10-19T19:56:27.632082Z info FLAG: --log_rotate_max_backups=&quot;1000&quot;2024-10-19T19:56:27.632085Z info FLAG: --log_rotate_max_size=&quot;104857600&quot;2024-10-19T19:56:27.632088Z info FLAG: --log_stacktrace_level=&quot;default:none&quot;2024-10-19T19:56:27.632108Z info FLAG: --log_target=&quot;[stdout]&quot;2024-10-19T19:56:27.632112Z info FLAG: --meshConfig=&quot;./etc/istio/config/mesh&quot;2024-10-19T19:56:27.632115Z info FLAG: --outlierLogPath=&quot;&quot;2024-10-19T19:56:27.632118Z info FLAG: --profiling=&quot;true&quot; 123456789101112131415161718192021(⎈|default:N/A) root@k3s-s:~# c1 lsof -i TCP:80,15006 -nlsof: no pwd entry for UID 1337lsof: no pwd entry for UID 1337lsof: no pwd entry for UID 1337lsof: no pwd entry for UID 1337COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnginx 31348 root 6u IPv4 316778 0t0 TCP *:http (LISTEN)nginx 31348 root 7u IPv6 316779 0t0 TCP *:http (LISTEN)nginx 31386 systemd-resolve 6u IPv4 316778 0t0 TCP *:http (LISTEN)nginx 31386 systemd-resolve 7u IPv6 316779 0t0 TCP *:http (LISTEN)nginx 31387 systemd-resolve 6u IPv4 316778 0t0 TCP *:http (LISTEN)nginx 31387 systemd-resolve 7u IPv6 316779 0t0 TCP *:http (LISTEN)nginx 31388 systemd-resolve 6u IPv4 316778 0t0 TCP *:http (LISTEN)nginx 31388 systemd-resolve 7u IPv6 316779 0t0 TCP *:http (LISTEN)nginx 31389 systemd-resolve 6u IPv4 316778 0t0 TCP *:http (LISTEN)nginx 31389 systemd-resolve 7u IPv6 316779 0t0 TCP *:http (LISTEN)lsof: no pwd entry for UID 1337envoy 31423 1337 38u IPv4 317525 0t0 TCP *:15006 (LISTEN)lsof: no pwd entry for UID 1337envoy 31423 1337 42u IPv4 317526 0t0 TCP *:15006 (LISTEN)lsof: no pwd entry for UID 1337","link":"/blog/2024/12/15/docs/istio/istio-traffic/"},{"title":"istio-outline","text":"Istio 소개서비스 매시(Service Mesh) 배경 마이크로서비스 아키텍처 환경의 시스템 전체 모니터링의 어려움, 운영 시 시스템 장애나 문제 발생할 때 원인과 병목 구간 찾기 어려움 개념 : 마이크로서비스 간에 매시 형태의 통신이나 그 경로를 제어 - 예) 이스티오(Istio), 링커드(Linkerd) 기본 동작 : 파드 간 통신 경로에 프록시를 놓고 트래픽 모니터링이나 트래픽 컨트롤 → 기존 애플리케이션 코드에 수정 없이 구성 가능! 기존 통신 환경 애플리케이션 수정 없이, 모든 애플리케이션 통신 사이에 Proxy 를 두고 통신 해보자 파드 내에 사이드카 컨테이너로 주입되어서 동작 Proxy 컨테이너가 Application 트래픽을 가로채야됨 → iptables rule 구현 ⇒ 가능한 이유는? Proxy 는 결국 DataPlane 이니, 이를 중앙에서 관리하는 ControlPlane을 두고 중앙에서 관리를 하자 Proxy 는 중앙에서 설정 관리가 잘되는 툴을 선택. 즉, 원격에서 동적인 설정 관리가 유연해야함 → 풍부한 API 지원이 필요 ⇒ Envoy ‘구글 IBM 리프트(Lyft)’가 중심이 되어 개발하고 있는 오픈 소스 소프트웨어이며, C++ 로 구현된 고성능 Proxy 인 엔보이(Envoy) 네트워크의 투명성을 목표, 다양한 필터체인 지원(L3/L4, HTTP L7), 동적 configuration API 제공, api 기반 hot reload 제공 중앙에서 어떤 동작/설정을 관리해야 될까? 라우팅, 보안 통신을 위한 mTLS 관련, 동기화 상태 정보 등 트래픽 모니터링 : 요청의 ‘에러율, 레이턴시, 커넥션 개수, 요청 개수’ 등 메트릭 모니터링, 특정 서비스간 혹은 특정 요청 경로로 필터링 → 원인 파악 용이! 트래픽 컨트롤 : 트래픽 시프팅(Traffic shifting), 서킷 브레이커(Circuit Breaker), 폴트 인젝션(Fault Injection), 속도 제한(Rate Limit) 트래픽 시프팅(Traffic shifting) : 예시) 99% 기존앱 + 1% 신규앱 , 특정 단말/사용자는 신규앱에 전달하여 단계적으로 적용하는 카니리 배포 가능 서킷 브레이커(Circuit Breaker) : 목적지 마이크로서비스에 문제가 있을 시 접속을 차단하고 출발지 마이크로서비스에 요청 에러를 반환 (연쇄 장애, 시스템 전제 장애 예방) 폴트 인젝션(Fault Injection) : 의도적으로 요청을 지연 혹은 실패를 구현 속도 제한(Rate Limit) : 요청 개수를 제한","link":"/blog/2024/10/20/docs/istio/outline/"},{"title":"envoy","text":"EnvoyIstio 구성요소와 envoy : 컨트롤 플레인(istiod) - ADS 를 이용한 Configuration 동기화 - 데이터 플레인(istio-proxy → envoy) Cluster : envoy 가 트래픽을 포워드할 수 있는 논리적인 서비스 (엔드포인트 세트), 실제 요청이 처리되는 IP 또는 엔드포인트의 묶음을 의미. Endpoint : IP 주소, 네트워크 노드로 클러스터로 그룹핑됨, 실제 접근이 가능한 엔드포인트를 의미. 엔드포인트가 모여서 하나의 Cluster 가 된다. Listener : 무엇을 받을지 그리고 어떻게 처리할지 IP/Port 를 바인딩하고, 요청 처리 측면에서 다운스트림을 조정하는 역할. Route : Listener 로 들어온 요청을 어디로 라우팅할 것인지를 정의. 라우팅 대상은 일반적으로 Cluster 라는 것에 대해 이뤄지게 된다. Filter : Listener 로부터 서비스에 트래픽을 전달하기까지 요청 처리 파이프라인 UpStream : envoy 요청을 포워딩해서 연결하는 백엔드 네트워크 노드 - 사이드카일때 application app, 아닐때 원격 백엔드 DownStream : An entity connecting to envoy, In non-sidecar models this is a remote client 설치1234567891011# 설치# echo &quot;deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io focal main&quot; | sudo tee /etc/apt/sources.list.d/envoy.listwget -O- https://apt.envoyproxy.io/signing.key | sudo gpg --dearmor -o /etc/apt/keyrings/envoy-keyring.gpgecho &quot;deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main&quot; | sudo tee /etc/apt/sources.list.d/envoy.listsudo apt-get update &amp;&amp; sudo apt-get install envoy -y# 확인envoy --version# 도움말envoy --help Envoy proxy 실습12345678910111213141516171819202122232425262728293031323334353637383940414243# (터미널1) 데모 config 적용하여 실행curl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/92dcb9714fb6bc288d042029b34c0de4/envoy-demo.yamlenvoy -c envoy-demo.yaml# (터미널2) 정보 확인ss -tnlpState Recv-Q Send-Q Local Address:Port Peer Address:Port ProcessLISTEN 0 4096 0.0.0.0:10000 0.0.0.0:* users:((&quot;envoy&quot;,pid=8007,fd=18),(&quot;envoy&quot;,pid=8007,fd=16))# 접속 테스트curl -s http://127.0.0.1:10000 | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;# 외부 접속 정보 출력echo -e &quot;http://$(curl -s ipinfo.io/ip):10000&quot;http://54.180.243.135:10000--------------------# 자신의 PC 웹브라우저에서 외부 접속 정보 접속 확인!# k3s-s 에서 접속 테스트curl -s http://192.168.10.200:10000 | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot; --------------------# 연결 정보 확인ss -tnp# (터미널1) envoy 실행 취소(CTRL+C) 후 (관리자페이지) 설정 덮어쓰기 - 링크cat &lt;&lt;EOT&gt; envoy-override.yamladmin: address: socket_address: address: 0.0.0.0 port_value: 9902EOTenvoy -c envoy-demo.yaml --config-yaml &quot;$(cat envoy-override.yaml)&quot;# envoy 관리페이지 외부 접속 정보 출력echo -e &quot;http://$(curl -s ipinfo.io/ip):9902&quot;http://54.180.243.135:9902--------------------# 자신의 PC 웹브라우저에서 관리 페이지 외부 접속 정보 접속 확인!","link":"/blog/2024/12/15/docs/istio/envoy/"},{"title":"bridge test","text":"namespace의 veth는 bridge를 통해서 네트워크 통신을 할 수 있도록 구성하는 테스트를 진행한다. namespace, bridge 를 구성하고 netfilter로 데이터 전송에 대한 정책을 확인하고 ping 테스트 및 dump 를 확인한다. 테스트 후에는 fdb, arp등을 확인한다. namespace, bridge 구성12345678910111213141516171819202122232425# add red namespace, virtual ethenet, bridgeip netns add redip link add veth-red type veth peer name brd-redip link set veth-red netns red# add blue namespace, virtual ethenet, bridgeip netns add blueip link add veth-blue type veth peer name brd-blueip link set veth-blue netns blue# bridge를 추가하고, namesapce에 연결된 bridge선과 연결한다.ip link add brd0 type bridgeip link set brd-red master brd0ip link set brd-blue master brd0# 각 namespace의 virtual ethenet 에 ip를 할당.ip netns exec red ip addr add 7.7.7.2/24 dev veth-redip netns exec blue ip addr add 7.7.7.3/24 dev veth-blue# 모든 스위치를 켜준다.ip netns exec red ip link set veth-red upip link set brd-red upip netns exec blue ip link set veth-blue upip link set brd-blue upip link set brd0 up ip address show Network interface 를 확인한다. bridge 선이 어디에 연결되어 있는 지 확인 가능하다. 3: brd-red@if4 는 4: veth-red@if3 와 연결되어 있다. bridge 12345678910111213143: brd-red@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master brd0 state UP group default qlen 1000 link/ether ba:0e:67:a9:5f:55 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::b80e:67ff:fea9:5f55/64 scope link valid_lft forever preferred_lft forever 5: brd-blue@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master brd0 state UP group default qlen 1000 link/ether 06:31:1c:4b:e0:85 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::431:1cff:fe4b:e085/64 scope link valid_lft forever preferred_lft forever 7: brd0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 06:31:1c:4b:e0:85 brd ff:ff:ff:ff:ff:ff inet6 fe80::431:1cff:fe4b:e085/64 scope link valid_lft forever preferred_lft forever red 12345678910root@seongtki:~# nsenter --net=/var/run/netns/redroot@seongtki:~# ip address show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:004: veth-red@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 92:1f:c1:d7:4c:d0 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 1.1.1.1/24 scope global veth-red valid_lft forever preferred_lft forever inet6 fe80::901f:c1ff:fed7:4cd0/64 scope link valid_lft forever preferred_lft forever blue 12345678910root@seongtki:~# nsenter --net=/var/run/netns/blueroot@seongtki:~# ip address show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:006: veth-blue@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:6a:f6:44:98:6b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 1.1.1.2/24 scope global veth-blue valid_lft forever preferred_lft forever inet6 fe80::206a:f6ff:fe44:986b/64 scope link valid_lft forever preferred_lft forever ip route show bridge 1234root@seongtki:~# ip routedefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 10010.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.1510.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100 red 12root@seongtki:~# ip route show1.1.1.0/24 dev veth-red proto kernel scope link src 1.1.1.1 blue 12root@seongtki:~# ip route1.1.1.0/24 dev veth-blue proto kernel scope link src 1.1.1.2 bridge fdb show bridge 1234567891011121314151617181920root@seongtki:~# bridge fdb show33:33:00:00:00:01 dev enp0s3 self permanent01:00:5e:00:00:01 dev enp0s3 self permanent33:33:ff:fb:6f:5e dev enp0s3 self permanent01:80:c2:00:00:00 dev enp0s3 self permanent01:80:c2:00:00:03 dev enp0s3 self permanent01:80:c2:00:00:0e dev enp0s3 self permanentba:0e:67:a9:5f:55 dev brd-red vlan 1 master brd0 permanentba:0e:67:a9:5f:55 dev brd-red master brd0 permanent33:33:00:00:00:01 dev brd-red self permanent01:00:5e:00:00:01 dev brd-red self permanent33:33:ff:a9:5f:55 dev brd-red self permanent06:31:1c:4b:e0:85 dev brd-blue master brd0 permanent06:31:1c:4b:e0:85 dev brd-blue vlan 1 master brd0 permanent33:33:00:00:00:01 dev brd-blue self permanent01:00:5e:00:00:01 dev brd-blue self permanent33:33:ff:4b:e0:85 dev brd-blue self permanent33:33:00:00:00:01 dev brd0 self permanent01:00:5e:00:00:01 dev brd0 self permanent33:33:ff:4b:e0:85 dev brd0 self permanent red 1234root@seongtki:~# bridge fdb show33:33:00:00:00:01 dev veth-red self permanent01:00:5e:00:00:01 dev veth-red self permanent33:33:ff:d7:4c:d0 dev veth-red self permanent blue 1234root@seongtki:~# bridge fdb show33:33:00:00:00:01 dev veth-blue self permanent01:00:5e:00:00:01 dev veth-blue self permanent33:33:ff:44:98:6b dev veth-blue self permanent netfilter 설정확인 host에서 진행한다. 현재 os는 모두 ACCEPT 로 되어 있다. 1234root@seongtki:~# iptables -t filter -L | grep policyChain INPUT (policy ACCEPT)Chain FORWARD (policy ACCEPT)Chain OUTPUT (policy ACCEPT) 하지만 아래처럼 FORWARD가 DROP으로 되어 있는 경우 정책을 변경해 주어야한다. 12345678910root@seongtki:~# iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationDROP all -- 1.1.1.0/24 anywhereChain OUTPUT (policy ACCEPT)target prot opt source destination 정책 변경 INPUT: 로컬로 들어오는 패킷 FORWARD: 외부 패킷이 방화벽을 통과해 다른 네트워크로 흘러가는 것을 제어 OUTPUT: 외부로 나가는 패킷 12345# -t : table# -A : Append chain rule# -s : src address# -j : jump to ACCEPTiptables -t filter -A FORWARD -s 1.1.1.0/24 -j ACCEPT 12# 정책 삭제iptables -D FORWARD 1 # 1은 rownum을 의미 FORWARD: DROP 일 때 테스트 ARP 까지 성공하지만, ICMP는 실패하여 ping 테스트를 할 수 없다. red 1234567root@seongtki:~# ping -c 1 1.1.1.2PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.038 ms--- 1.1.1.2 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.038/0.038/0.038/0.000 ms bridge 123456789root@seongtki:~# tcpdump -l -i brd0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on brd0, link-type EN10MB (Ethernet), capture size 262144 bytes04:22:46.256635 IP one.one.one.one &gt; 1.1.1.2: ICMP echo request, id 2338, seq 1, length 6404:22:46.256661 IP 1.1.1.2 &gt; one.one.one.one: ICMP echo reply, id 2338, seq 1, length 6404:22:51.285442 ARP, Request who-has one.one.one.one tell 1.1.1.2, length 2804:22:51.285451 ARP, Request who-has 1.1.1.2 tell one.one.one.one, length 2804:22:51.285478 ARP, Reply one.one.one.one is-at 92:1f:c1:d7:4c:d0 (oui Unknown), length 2804:22:51.285483 ARP, Reply 1.1.1.2 is-at 22:6a:f6:44:98:6b (oui Unknown), length 28 blue 12345678910root@seongtki:~# tcpdump -l -i veth-bluetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth-blue, link-type EN10MB (Ethernet), capture size 262144 bytes04:22:46.256643 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 2338, seq 1, length 6404:22:46.256660 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 2338, seq 1, length 6404:22:51.285435 ARP, Request who-has 1.1.1.1 tell 1.1.1.2, length 2804:22:51.285471 ARP, Request who-has 1.1.1.2 tell 1.1.1.1, length 2804:22:51.285482 ARP, Reply 1.1.1.2 is-at 22:6a:f6:44:98:6b (oui Unknown), length 2804:22:51.285483 ARP, Reply 1.1.1.1 is-at 92:1f:c1:d7:4c:d0 (oui Unknown), length 28 ping test (성공) red -&gt; bridge -&gt; blue arp, bridge fdb 정보를 이용해서 통신하고 iptables rule 으로 전송여부를 결정한다 통신 후 arp, bridge fdb 에 통신한 virtual ethenet에 mac정보 확인 red 123456root@seongtki:~# ping 1.1.1.2PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.065 ms64 bytes from 1.1.1.2: icmp_seq=2 ttl=64 time=0.078 ms64 bytes from 1.1.1.2: icmp_seq=3 ttl=64 time=0.073 ms64 bytes from 1.1.1.2: icmp_seq=4 ttl=64 time=0.074 ms bridge 12345678910root@seongtki:~# tcpdump -l -i brd0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on brd0, link-type EN10MB (Ethernet), capture size 262144 bytes04:03:41.050595 ARP, Request who-has 1.1.1.2 tell one.one.one.one, length 2804:03:41.050625 ARP, Reply 1.1.1.2 is-at 22:6a:f6:44:98:6b (oui Unknown), length 2804:03:41.050629 IP one.one.one.one &gt; 1.1.1.2: ICMP echo request, id 2299, seq 1, length 6404:03:41.050641 IP 1.1.1.2 &gt; one.one.one.one: ICMP echo reply, id 2299, seq 1, length 64 blue 123456789101112131415root@seongtki:~# tcpdump -l -i veth-bluetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth-blue, link-type EN10MB (Ethernet), capture size 262144 bytes04:03:41.050600 ARP, Request who-has 1.1.1.2 tell 1.1.1.1, length 2804:03:41.050623 ARP, Reply 1.1.1.2 is-at 22:6a:f6:44:98:6b (oui Unknown), length 2804:03:41.050631 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 2299, seq 1, length 6404:03:41.050640 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 2299, seq 1, length 6404:03:42.053818 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 2299, seq 2, length 6404:03:42.053850 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 2299, seq 2, length 6404:03:43.056383 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 2299, seq 3, length 6404:03:43.056411 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 2299, seq 3, length 6404:03:44.060404 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 2299, seq 4, length 6404:03:44.060432 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 2299, seq 4, length 64 arp (ping 테스트 후 ) ping test 후 arp 정보 확인 red 12root@seongtki:~# ip neigh show1.1.1.2 dev veth-red lladdr 22:6a:f6:44:98:6b STALE blue 12root@seongtki:~# ip neigh1.1.1.1 dev veth-blue lladdr 92:1f:c1:d7:4c:d0 STALE bridge fdb show (ping 테스트 후 ) ping 테스트 후에 fdb에 brd-red, brd-blue에 mac주소가 남겨져음을 확인할 수 있다. 123456789...92:1f:c1:d7:4c:d0 dev brd-red master brd0...22:6a:f6:44:98:6b dev brd-blue master brd0... red mac주소 : link/ether 92:1f:c1:d7:4c:d0 brd ff:ff:ff:ff:ff:ff link-netnsid 0 123456789root@seongtki:~# ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:004: veth-red@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 92:1f:c1:d7:4c:d0 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 1.1.1.1/24 scope global veth-red valid_lft forever preferred_lft forever inet6 fe80::901f:c1ff:fed7:4cd0/64 scope link valid_lft forever preferred_lft forever blue Mac주소 : link/ether 22:6a:f6:44:98:6b brd ff:ff:ff:ff:ff:ff link-netnsid 0 123456789root@seongtki:~# ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:006: veth-blue@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:6a:f6:44:98:6b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 1.1.1.2/24 scope global veth-blue valid_lft forever preferred_lft forever inet6 fe80::206a:f6ff:fe44:986b/64 scope link valid_lft forever preferred_lft forever","link":"/blog/2024/08/31/docs/namespace/bridge-test/"},{"title":"bridge to internet","text":"1. bridge 구성12345678910111213141516171819202122232425# add red namespace, virtual ethenet, bridgeip netns add redip link add veth-red type veth peer name brd-redip link set veth-red netns red# add blue namespace, virtual ethenet, bridgeip netns add blueip link add veth-blue type veth peer name brd-blueip link set veth-blue netns blue# bridge를 추가하고, namesapce에 연결된 bridge선과 연결한다.ip link add brd0 type bridgeip link set brd-red master brd0ip link set brd-blue master brd0# 각 namespace의 virtual ethenet 에 ip를 할당.ip netns exec red ip addr add 7.7.7.2/24 dev veth-redip netns exec blue ip addr add 7.7.7.3/24 dev veth-blue# 모든 스위치를 켜준다.ip netns exec red ip link set veth-red upip link set brd-red upip netns exec blue ip link set veth-blue upip link set brd-blue upip link set brd0 up 2. host ip에서 red ns로 ping 통신host 통신실패 12$ ping 7.7.7.2PING 7.7.7.2 (7.7.7.2) 56(84) bytes of data. red 에서 패킷을 확인해보자. 1234# 아무런 반응이 없다.root@seongtki:~# tcpdump -li anytcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes 요청한 ip는 라우터에 없는 정보이다. 그래서 brd0에 접근할 수 있도록 route 정보를 추가해 보자. 1ip addr add 7.7.7.1/24 dev brd0 # 7.7.7.1 는 brd0 ip route table에 추가됨. 12345root@seongtki:~# ip routedefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 1007.7.7.0/24 dev brd0 proto kernel scope link src 7.7.7.110.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.1510.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100 통신성공 1234root@seongtki:~# ping 7.7.7.2PING 7.7.7.2 (7.7.7.2) 56(84) bytes of data.64 bytes from 7.7.7.2: icmp_seq=1 ttl=64 time=0.062 ms64 bytes from 7.7.7.2: icmp_seq=2 ttl=64 time=0.053 ms red arp 정보에 brd0 (7.7.7.1) 추가를 확인. 12345# red$ arpAddress HWtype HWaddress Flags Mask Iface7.7.7.1 ether 5e:09:1c:5e:67:29 C veth-red7.7.7.3 ether 52:13:d2:7c:86:d1 C veth-red 3. red ns의 ip에서 host의 brd0 로 ping 통신red 통신성공. (물리적으로 연결해놓았으니 가능.) 1234root@seongtki:~# ping 7.7.7.1PING 7.7.7.1 (7.7.7.1) 56(84) bytes of data.64 bytes from 7.7.7.1: icmp_seq=1 ttl=64 time=0.033 ms64 bytes from 7.7.7.1: icmp_seq=2 ttl=64 time=0.075 ms 4. red ns의 ip에서 host ip로 ping 통신red 통신 실패 12root@seongtki:~# ping 10.0.2.15connect: Network is unreachable 요청한 ip는 2계층에 없었고, route table에도 매핑정보가 없다. 12root@seongtki:~# ip route7.7.7.0/24 dev veth-red proto kernel scope link src 7.7.7.2 아래와 같이 brd0 로 이동하라고 매핑을 추가한다. 1$ ip route add default via 7.7.7.1 route table 확인 123root@seongtki:~# ip routedefault via 7.7.7.1 dev veth-red7.7.7.0/24 dev veth-red proto kernel scope link src 7.7.7.2 통신성공 1234root@seongtki:~# ping 10.0.2.15PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.029 ms64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.055 ms 5. red ns의 ip에서 구글 ip로 ping 통신red 구글로 통신 실패 12345root@seongtki:~# ping 8.8.8.8PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.--- 8.8.8.8 ping statistics ---3 packets transmitted, 0 received, 100% packet loss, time 2032ms host에서 brd0의 패킷을 보자. 123456root@seongtki:~# tcpdump -li brd0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on brd0, link-type EN10MB (Ethernet), capture size 262144 bytes05:14:50.472549 IP 7.7.7.2 &gt; dns.google: ICMP echo request, id 2255, seq 1, length 6405:14:51.482030 IP 7.7.7.2 &gt; dns.google: ICMP echo request, id 2255, seq 2, length 6405:14:52.504940 IP 7.7.7.2 &gt; dns.google: ICMP echo request, id 2255, seq 3, length 64 여기서는 구글로 요청을 계속 하고 있다. (응답x) POSTROUTING 라우팅 outbound, 포워딩 트래픽에 의해 트리거되는 netfilter hook nat(table) : NAT 대상 패킷의 출발지 혹은 목적지 address 수정방법을 결정 -&gt; POSTROUTING 에서는 SNAT(Source NAT) 패킷이 network로 direct access가 불가능 할 때 사용한다. 내부망 아이피인 7.7.7.0 를 host ip 10.0.2.15 로 둔갑시켜 준다. (NAT) 12$ iptables -t nat -A POSTROUTING -s 7.7.7.0/24 -j MASQUERADE$ iptables -t nat -L 외부(src) -&gt; 외부(dst) 통신일 경우, FORWARD 가 트리거 되는데, 외부에서 7.7.7.0으로 오는 경우에도 접근을 허용하도록 룰을 추가하자. 반대로 7.7.7.0 에서 출발할 때에도 룰을 추가해야 한다. ( $ iptables -t filter -A FORWARD -s 7.7.7.0/24 -j ACCEPT ) 1iptables -t filter -A FORWARD -d 7.7.7.0/24 -j ACCEPT 12345678910root@seongtki:~# iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationACCEPT all -- anywhere 7.7.7.0/24 # addChain OUTPUT (policy ACCEPT)target prot opt source destination IP Forwarding (routing) 커널의 IP Forwarding (routing) 기능 켜기 1: on, 0: off 1echo 1 -n &gt; /proc/sys/net/ipv4/ip_forward 통신 확인 123456root@seongtki:~# ping 8.8.8.8PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.64 bytes from 8.8.8.8: icmp_seq=1 ttl=61 time=46.7 ms64 bytes from 8.8.8.8: icmp_seq=2 ttl=61 time=59.9 ms64 bytes from 8.8.8.8: icmp_seq=3 ttl=61 time=35.7 ms64 bytes from 8.8.8.8: icmp_seq=4 ttl=61 time=50.2 ms","link":"/blog/2024/08/31/docs/namespace/bridge-to-internet/"},{"title":"mount namespace","text":"네임스페이스(namespace) 하나의 시스템에서 수행되지만, 각각 별개의 독립된 공간인 것처럼 격리된 환경을 제공하는 *경량 프로세스 가상화 기술 경량 프로세스는 부모 프로세스와 자원을 공유하는 프로세스를 의미 LXC(LinuX Container)의 네임스페이스는 6 종류와 플래그 마운트 네임스페이스 : CLONE_NEWNS UTS 네임스페이스 : CLONE_NEWUTS IPC 네임스페이스 : CLONE_NEWIPC PID 네임스페이스 : CLONE_NEWPID 사용자 네임스페이스 : CLONE_NEWUSER 네트워크 네임스페이스 : CLONE_NEWNET 상수플래그 정의 (/usr/include/linux/sched.h) 마운트(mount) 네임스페이스란? 프로세스와 그 자식 프로세스가 각기 다른 파일시스템 마운트 지점을 제공 기본적으로 모든 프로세스는 동일한 기본 네임스페이스를 공유하기 때문에, 파일 시스템을 마운트하거나 마운트를 해제하는 등의 변경을 모든 프로세스가 인지할 수 있음 만약 clone() 시스템 콜을 통해 프로세스를 생성할 때 CLONE_NEWNS 플래그가 전달되면, 새로 생성되는 프로세스는 호출한 프로세스가 갖고있는 마운트 트리의 사본을 가져옴 이 사본은 부모 프로세스에 영향을 미치지 않고 새로 생성된 프로세스가 파일시스템을 마운트 or 언마운트 등의 변경을 할 수 있도록 함 이 시점부터 기본 네임스페이스의 모든 파일시스템 마운트 및 언마운트는 새로운 네임스페이스에서 볼 수 있지만, 각각의 프로세스 별 마운트 네임스페이스 내부에서의 변경을 해당 프로세스의 네임스페이스 밖에서는 알 수 없음 마운트 네임스페이스는 “mount point”를 격리 한다.여기서 mount란 루트파일시스템에 서브파일시스템을 부착하는 시스템콜이고, mount point는 파일시스템이 부착(mount) 될 위치(디렉터리)다. mount 하면 원래 디렉터리가 포함하고 있던 파일, 하위 디렉터리 등이 보이지 않고, 새로 부착된 파일시스템의 파일들이 보이게 됩니다. 예를 들어 USB 드라이브를 지정한 mount point로 부착을 시키면, 해당 위치에는 부착된 USB의 파일들이 보이게 됩니다. 마운트 네임스페이스가 mount point를 격리한다는 것은 파일시스템 마운트와 해제 등의 변경사항들이 네임스페이스 밖에서는 보이지 않고, 외부에 전혀 영향을 주지 않음을 의미합니다. 마운트 네임스페이스가 생김으로써 컨테이너를 위해 pivot_root를 안전하게 실행할 수 있게 되었습니다. unshare mount namespacemount 플래그를 unshared 시스템 콜에 전달 부모 프로세스의 mounts를 복제 현재 bash 프로세스를 자체 마운트 네임스페이스로 이동 “-m”옵션 -&gt; 마운트 네임스페이스 12unshare -mroot@seongtki:/tmp# mkdir tmp1 root filesystem이 같기 때문에 양쪽다 폴더가 보임 mount는 namespace 내부에서만 적용되어 보인다. tmp1 : mount point 1root@seongtki:/tmp# mount -o size=1m -t tmpfs tmpfs tmp1 12root@seongtki:/tmp# mount | grep tmp1tmpfs on /tmp/tmp1 type tmpfs (rw,relatime,size=1024k) 1234# readlink 심폴릭링크를 통해 namepsace id를 가져올 수 있다.# /proc/$$ 는 현재 bash 쉘의 프로세스 ID(PID)$ readlink /proc/$$/ns/mntmnt:[4026532211] host 12root@seongtki:/tmp# mount | grep tmp1root@seongtki:/tmp# mount namespace는 per-process mount isolation을 제공 (프로세스에 격리된 파일시스템 마운트 제공) per-process mount isolation new-root 에 nginx를 설치한다. 1docker export $(docker create nginx:latest) | tar -C new-root -xvf - 123456789101112131415161718192021222324251 directory, 0 filesroot@seongtki:/tmp/tmp1# tree -L 1 new-rootnew-root├── bin -&gt; usr/bin├── boot├── dev├── docker-entrypoint.d├── docker-entrypoint.sh├── etc├── hello├── home├── lib -&gt; usr/lib├── media├── mnt├── opt├── proc├── root├── run├── sbin -&gt; usr/sbin├── srv├── sys├── tmp├── usr├── var└── world mount 1root@seongtki:/tmp/tmp1# mount -t tmpfs none new-root 12root@seongtki:/tmp/tmp1# df -h | grep newnone 2.0G 0 2.0G 0% /tmp/tmp1/new-root pivot_root 적용1234/# mkdir old-root/# pivot_root . old-root # 현재디렉토리를 루트파일시스템으로 하고, old-root 폴더에 기존 루트디렉토리를 마운트시킨다./# nginx -g &quot;daemon off;&quot; # nginx 실행 host에서 nginx pid를 이용해서 namespace id를 가져와 해당 파일시스템을 조회해본다. lsns: 현존하는 namespace를 확인하기 위해 사용하는 명령어 자체적인 루트파일시스템이 보장되므로 서버 환경 (시스템 파일, 의존성 라이브러리 충돌, 경로 설정 등) 으로 부터 자유롭다. 서버의 파일시스템으로 부터 완전하게 분리됨으로써 보안상 안전함 컨테이너 내에서 자유롭게 마운트 변경이 가능하고 파일시스템을 확장할 수 있다.","link":"/blog/2024/12/08/docs/namespace/mount-namespace/"},{"title":"docker to internet","text":"docker 내부 컨테이너를 외부망에 ping 테스트 해보자. bridge to internet 과 비교해보자 docker container 생성docker bridge namespace테스트할 때 생성했던 bridge가 여기도 있다. 12345root@seongtki:~# docker network lsNETWORK ID NAME DRIVER SCOPEe9f8463dc960 bridge bridge localba45e28be887 host host localbfe581bee6a5 none null local “Driver”: “bridge” 12&quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,&quot;Gateway&quot;: &quot;172.17.0.1&quot; 1234567891011121314151617181920212223242526272829303132333435363738root@seongtki:~# docker inspect bridge[ { &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;e9f8463dc96066a997b68404d6b87803c070795fe9ef89acbde59685d50e9659&quot;, &quot;Created&quot;: &quot;2023-09-22T23:54:48.214177336Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: { &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ { &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; } ] }, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: { &quot;Network&quot;: &quot;&quot; }, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: {}, &quot;Options&quot;: { &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; }, &quot;Labels&quot;: {} }] docker0 는 이전에 생성했던 brd0 와 비슷한 모양이다. 여긴 state DOWN 인데 왜인지는 잘 모름 (내가 생성했던 bridge 는 up 해주어서 state UP 였다 ) 12345root@seongtki:~# ip addr show3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:75:07:2a:20 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever brctl (ethenet bridge administration) 아직 연결된 인터페이스가 없다. 123root@seongtki:~# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024275072a20 no red container 생성 namespace 라고 봐도 된다. 1docker run -it --name=red busybox blue container 생성1docker run -it --name=blue busybox 컨테이너 조회1234root@seongtki:~# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdad7b7094a48 busybox &quot;sh&quot; 5 seconds ago Up 5 seconds blue83f83cfc4b9b busybox &quot;sh&quot; 53 seconds ago Up 53 seconds red virtual ethenet peerhost 5: veth6008cec@if4 (red container) 7: veth02ce233@if6 (blue container) 123456789101112131415root@seongtki:~# ip a3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:75:07:2a:20 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:75ff:fe07:2a20/64 scope link valid_lft forever preferred_lft forever5: veth6008cec@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 8a:a2:28:fc:88:58 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::88a2:28ff:fefc:8858/64 scope link valid_lft forever preferred_lft forever7: veth02ce233@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether d2:97:cf:02:30:01 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::d097:cfff:fe02:3001/64 scope link valid_lft forever preferred_lft forever interface가 생성되었다 1234root@seongtki:~# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024275072a20 no veth02ce233 veth6008cec red container 4: eth0@if5: 123456789# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever4: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever blue container 6: eth0@if7 123456781: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever6: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 컨테이너 간 통신red container blue container 로 통신 성공 1234567/ # ping 172.17.0.3PING 172.17.0.3 (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.610 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.376 ms64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.446 ms64 bytes from 172.17.0.3: seq=3 ttl=64 time=0.304 ms64 bytes from 172.17.0.3: seq=4 ttl=64 time=0.314 ms host - bridge docker0 를 통해 ARP, ICMP 통신이 잘 됨을 확인. 1234567891011root@seongtki:~# tcpdump -li docker0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on docker0, link-type EN10MB (Ethernet), capture size 262144 bytes00:51:13.636954 ARP, Request who-has 172.17.0.3 tell 172.17.0.2, length 2800:51:13.637063 ARP, Reply 172.17.0.3 is-at 02:42:ac:11:00:03 (oui Unknown), length 2800:51:13.637072 IP 172.17.0.2 &gt; 172.17.0.3: ICMP echo request, id 9, seq 0, length 6400:51:13.637310 IP 172.17.0.3 &gt; 172.17.0.2: ICMP echo reply, id 9, seq 0, length 6400:51:14.638927 IP 172.17.0.2 &gt; 172.17.0.3: ICMP echo request, id 9, seq 1, length 6400:51:14.639126 IP 172.17.0.3 &gt; 172.17.0.2: ICMP echo reply, id 9, seq 1, length 6400:51:15.640112 IP 172.17.0.2 &gt; 172.17.0.3: ICMP echo request, id 9, seq 2, length 6400:51:15.640283 IP 172.17.0.3 &gt; 172.17.0.2: ICMP echo reply, id 9, seq 2, length 64 red &amp; blue arp 조회 123# red/ # ip neigh show172.17.0.3 dev eth0 lladdr 02:42:ac:11:00:03 used 0/0/0 probes 4 STALE 123# blue/ # ip neigh show172.17.0.2 dev eth0 lladdr 02:42:ac:11:00:02 ref 1 used 0/0/0 probes 1 REACHABLE 외부망 통신route: host123456789101112root@seongtki:~# ip routedefault via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.27 metric 100172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.27192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.27 metric 100root@seongtki:~# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.64.1 0.0.0.0 UG 100 0 0 enp0s1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.64.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s1192.168.64.1 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s1 route: red container12345678/ # ip routedefault via 172.17.0.1 dev eth0172.17.0.0/16 dev eth0 scope link src 172.17.0.2/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 iptables: host filter의 FORWARD 정책이 DROP임을 확인. -A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT docker0을 통과하는 FORWARD 를 모두 ACCEPT한다. 도커 컨테이너간의 통신 가능 외부에서 도커컨테이너로 통신 가능. 12345678910111213141516171819root@seongtki:~# iptables -t filter -S-P INPUT ACCEPT-P FORWARD DROP-P OUTPUT ACCEPT-N DOCKER-N DOCKER-ISOLATION-STAGE-1-N DOCKER-ISOLATION-STAGE-2-N DOCKER-USER-A FORWARD -j DOCKER-USER-A FORWARD -j DOCKER-ISOLATION-STAGE-1-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o docker0 -j DOCKER-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -j RETURN-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -j RETURN-A DOCKER-USER -j RETURN nat 의 기본 polick (PREROUTING, INPUT, OUTPUT, POSTROUTING) 가 모두 ACCEPT 임을 확인. -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE src가 172.17.0.0 대역이고 out interface가 docker0 가 아니면 MASQUERADE 설정 (SNAT) 도커 컨테이너(namespace) -&gt; 외부망 통신이 가능. 12345678910root@seongtki:~# iptables -t nat -S-P PREROUTING ACCEPT-P INPUT ACCEPT-P OUTPUT ACCEPT-P POSTROUTING ACCEPT-N DOCKER-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A DOCKER -i docker0 -j RETURN red 통신성공 123456/ # ping 8.8.8.8PING 8.8.8.8 (8.8.8.8): 56 data bytes64 bytes from 8.8.8.8: seq=0 ttl=114 time=43.324 ms64 bytes from 8.8.8.8: seq=1 ttl=114 time=44.720 ms64 bytes from 8.8.8.8: seq=2 ttl=114 time=42.764 ms64 bytes from 8.8.8.8: seq=3 ttl=114 time=36.548 ms host - bridge docker0 를 경유해서 ARP, ICMP 통신을 하고 있음을 확인. 123456789101112131415root@seongtki:~# tcpdump -li docker0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on docker0, link-type EN10MB (Ethernet), capture size 262144 bytes01:27:53.570746 IP 172.17.0.3 &gt; dns.google: ICMP echo request, id 8, seq 0, length 6401:27:53.613814 IP dns.google &gt; 172.17.0.3: ICMP echo reply, id 8, seq 0, length 6401:27:54.575544 IP 172.17.0.3 &gt; dns.google: ICMP echo request, id 8, seq 1, length 6401:27:54.619839 IP dns.google &gt; 172.17.0.3: ICMP echo reply, id 8, seq 1, length 6401:27:55.577202 IP 172.17.0.3 &gt; dns.google: ICMP echo request, id 8, seq 2, length 6401:27:55.619712 IP dns.google &gt; 172.17.0.3: ICMP echo reply, id 8, seq 2, length 6401:27:56.578531 IP 172.17.0.3 &gt; dns.google: ICMP echo request, id 8, seq 3, length 6401:27:56.614834 IP dns.google &gt; 172.17.0.3: ICMP echo reply, id 8, seq 3, length 6401:27:58.816367 ARP, Request who-has 172.17.0.3 tell seongtki, length 2801:27:58.817088 ARP, Reply 172.17.0.3 is-at 02:42:ac:11:00:03 (oui Unknown), length 2801:27:58.817310 ARP, Request who-has seongtki tell 172.17.0.3, length 2801:27:58.817354 ARP, Reply seongtki is-at 02:42:75:07:2a:20 (oui Unknown), length 28 구조도 이상 도커 컨테이너에서의 실습한 통신에 대한 구조도이다. 이전에 bridge &lt;-&gt; internet 통신과 비슷한 모양의 구조임을 알 수 있다.","link":"/blog/2024/08/31/docs/namespace/docker-to-internet/"},{"title":"istio-test","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# istioctl 설치export ISTIOV=1.23.2echo &quot;export ISTIOV=1.23.2&quot; &gt;&gt; /etc/profilecurl -s -L https://istio.io/downloadIstio | ISTIO_VERSION=$ISTIOV TARGET_ARCH=x86_64 sh -tree istio-$ISTIOV -L 2 # sample yaml 포함cp istio-$ISTIOV/bin/istioctl /usr/local/bin/istioctlistioctl version --remote=false# (demo 프로파일) 컨트롤 플레인 배포 - 링크 Customizing# The istioctl command supports the full IstioOperator API via command-line options for individual settings or for passing a yaml file containing an IstioOperator custom resource (CR).istioctl profile lististioctl profile dump defaultistioctl profile dump --config-path components.ingressGatewaysistioctl profile dump --config-path values.gateways.istio-ingressgatewayistioctl profile dump demoistioctl profile dump demo &gt; demo-profile.yamlvi demo-profile.yaml # 복잡성을 줄이게 실습 시나리오 환경 맞춤-------------------- egressGateways: - enabled: false-------------------- istioctl install -f demo-profile.yaml -y✔ Istio core installed ⛵️ ✔ Istiod installed 🧠 ✔ Ingress gateways installed 🛬 ✔ Installation complete # 설치 확인 : istiod, istio-ingressgatewaykubectl get all,svc,ep,sa,cm,secret,pdb -n istio-systemkubectl get crd | grep istio.io | sort# istio-ingressgateway 의 envoy 버전 확인kubectl exec -it deploy/istio-ingressgateway -n istio-system -c istio-proxy -- envoy --versionenvoy version: 6c72b2179f5a58988b920a55b0be8346de3f7b35/1.31.2-dev/Clean/RELEASE/BoringSSL# istio-ingressgateway 서비스 NodePort로 변경kubectl patch svc -n istio-system istio-ingressgateway -p '{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'# istio-ingressgateway 서비스 확인kubectl get svc,ep -n istio-system istio-ingressgateway## istio-ingressgateway 서비스 포트 정보 확인kubectl get svc -n istio-system istio-ingressgateway -o jsonpath={.spec.ports[*]} | jq## istio-ingressgateway 디플로이먼트 파드의 포트 정보 확인 kubectl get deploy/istio-ingressgateway -n istio-system -o jsonpath={.spec.template.spec.containers[0].ports[*]} | jqkubectl get deploy/istio-ingressgateway -n istio-system -o jsonpath={.spec.template.spec.containers[0].readinessProbe} | jq# istiod(컨트롤플레인) 디플로이먼트 정보 확인kubectl exec -it deployment.apps/istiod -n istio-system -- ss -tnlpkubectl exec -it deployment.apps/istiod -n istio-system -- ss -tnpkubectl exec -it deployment.apps/istiod -n istio-system -- ps -efUID PID PPID C STIME TTY TIME CMDistio-p+ 1 0 0 05:27 ? 00:00:07 /usr/local/bin/pilot-discovery discovery --monitoringAddr=:15014 --log_output_l# istio-ingressgateway 디플로이먼트 정보 확인kubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- ss -tnlpkubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- ss -tnpkubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- ps -efistio-p+ 1 0 0 05:27 ? 00:00:01 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.localistio-p+ 15 1 0 05:27 ? 00:00:11 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --draikubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- cat /etc/istio/proxy/envoy-rev.jsonkubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- ss -xnlpkubectl exec -it deployment.apps/istio-ingressgateway -n istio-system -- ss -xnp Istio 통한 외부 노출Nginx 디플로이먼트와 서비스 배포123456# 로그 모니터링kubectl get pod -n istio-system -l app=istiodkubetail -n istio-system -l app=istiod -fkubectl get pod -n istio-system -l app=istio-ingressgatewaykubetail -n istio-system -l app=istio-ingressgateway -f 1234567891011121314151617181920212223242526272829303132333435363738394041cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: ServiceAccountmetadata: name: kans-nginx---apiVersion: apps/v1kind: Deploymentmetadata: name: deploy-websrvspec: replicas: 1 selector: matchLabels: app: deploy-websrv template: metadata: labels: app: deploy-websrv spec: serviceAccountName: kans-nginx terminationGracePeriodSeconds: 0 containers: - name: deploy-websrv image: nginx:alpine ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: svc-clusteripspec: ports: - name: svc-webport port: 80 targetPort: 80 selector: app: deploy-websrv type: ClusterIPEOF 1kubectl get pod,svc,ep,sa -o wide Istio Gateway/VirtualService 설정 - Host 기반 트래픽 라우팅 설정 클라이언트 PC → (Service:NodePort) Istio ingressgateway 파드 → (Gateway, VirtualService, Service 는 Bypass) → Endpoint(파드 : 사이드카 - Application 컨테이너) Gateway : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정 가능 1234567891011121314151617181920212223242526272829303132cat &lt;&lt;EOF | kubectl create -f -apiVersion: networking.istio.io/v1kind: Gatewaymetadata: name: test-gatewayspec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - &quot;*&quot;---apiVersion: networking.istio.io/v1kind: VirtualServicemetadata: name: nginx-servicespec: hosts: - &quot;$MYDOMAIN&quot; gateways: - test-gateway http: - route: - destination: host: svc-clusterip port: number: 80EOF 1234567891011121314151617# Istio Gateway(=gw)/VirtualService(=vs) 설정 정보를 확인kc explain gateways.networking.istio.iokc explain virtualservices.networking.istio.iokubectl api-resources | grep istio# virtual service 는 다른 네임스페이스의 서비스(ex. svc-nn.&lt;ns&gt;)도 참조할 수 있다kubectl get gw,vsNAME AGEgateway.networking.istio.io/test-gateway 21sNAME GATEWAYS HOSTS AGEvirtualservice.networking.istio.io/nginx-service [&quot;test-gateway&quot;] [&quot;www.gasida.dev&quot;] 4m9s# Retrieves last sent and last acknowledged xDS sync from Istiod to each Envoy in the mesh# istioctl proxy-status command was improved to include the time since last change, and more relevant status values.istioctl proxy-status # 단축어 psistioctl ps Istio 를 통한 Nginx 파드 접속 테스트 외부(자신의PC, testpc)에서 접속 테스트 1234# istio ingress gw 를 통한 접속 테스트curl -s $MYDOMAIN:$IGWHTTP | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;curl -v -s $MYDOMAIN:$IGWHTTPcurl -v -s &lt;유동공인이IP&gt;:$IGWHTTP 출력 로그 정보 확인 12kubetail -n istio-system -l app=istio-ingressgateway -fkubetail -l app=deploy-websrv 정보확인 1234567891011#istioctl proxy-statusNAME CLUSTER CDS LDS EDS RDS ECDS ISTIOD VERSIONdeploy-websrv-7d7cf8586c-l22cs.default Kubernetes SYNCED (22m) SYNCED (22m) SYNCED (22m) SYNCED (22m) IGNORED istiod-7f8b586864-mv944 1.23.2istio-ingressgateway-5f9f654d46-c4g7s.istio-system Kubernetes SYNCED (5m19s) SYNCED (5m19s) SYNCED (5m19s) SYNCED (5m19s) IGNORED istiod-7f8b586864-mv944 1.23.2# Envoy config dump : all, cluster, endpoint, listener 등istioctl proxy-config --help istioctl proxy-config all deploy-websrv-7d7cf8586c-l22csistioctl proxy-config all deploy-websrv-7d7cf8586c-l22cs -o json | jqistioctl proxy-config route deploy-websrv-7d7cf8586c-l22cs -o json | jq pilot : istio-proxy 내 uds 로 envoy 와 grpc 통신, istiod 받아온 dynamic config 를 envoy 에 전달 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# istio-proxy 사용자 정보 확인 : uid(1337):gid(1337) 확인 -&gt; iptables rule 에서 사용됨kubectl exec -it deploy/deploy-websrv -c istio-proxy -- tail -n 3 /etc/passwdubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bashtcpdump:x:100:102::/nonexistent:/usr/sbin/nologinistio-proxy:x:1337:1337::/home/istio-proxy:/bin/sh# envoy 설정 정보 확인 : dynamic_resources , static_resources - listeners : 출력되는 IP가 누구인지 확인 해보자kubectl exec -it deploy/deploy-websrv -c istio-proxy -- cat /etc/istio/proxy/envoy-rev.jsonkubectl exec -it deploy/deploy-websrv -c istio-proxy -- ss -nlpkubectl exec -it deploy/deploy-websrv -c istio-proxy -- ss -npkubectl exec -it deploy/deploy-websrv -c istio-proxy -- netstat -npActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 172.16.1.10:15021 172.16.1.1:49548 TIME_WAIT - tcp 0 0 172.16.1.10:15006 172.16.2.4:37814 ESTABLISHED 12/envoy tcp 0 0 172.16.1.10:15021 172.16.1.1:43138 TIME_WAIT - tcp 0 0 127.0.0.1:39158 127.0.0.1:15020 ESTABLISHED 12/envoy tcp 0 0 172.16.1.10:15021 172.16.1.1:42948 TIME_WAIT - tcp 0 0 172.16.1.10:51370 10.10.200.82:15012 ESTABLISHED 1/pilot-agent tcp 0 0 172.16.1.10:15021 172.16.1.1:39522 TIME_WAIT - tcp 0 0 172.16.1.10:51360 10.10.200.82:15012 ESTABLISHED 1/pilot-agent tcp 0 0 127.0.0.1:39172 127.0.0.1:15020 ESTABLISHED 12/envoy tcp6 0 0 127.0.0.1:15020 127.0.0.1:39158 ESTABLISHED 1/pilot-agent tcp6 0 0 127.0.0.1:15020 127.0.0.1:39172 ESTABLISHED 1/pilot-agent Active UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node PID/Program name Pathunix 3 [ ] STREAM CONNECTED 151002 1/pilot-agent var/run/secrets/workload-spiffe-uds/socketunix 3 [ ] STREAM CONNECTED 152729 - unix 3 [ ] STREAM CONNECTED 152723 - unix 3 [ ] STREAM CONNECTED 152727 - unix 3 [ ] STREAM CONNECTED 150129 12/envoy unix 3 [ ] STREAM CONNECTED 152726 - unix 3 [ ] STREAM CONNECTED 152724 - unix 3 [ ] STREAM CONNECTED 152722 - unix 3 [ ] STREAM CONNECTED 150979 12/envoy unix 3 [ ] STREAM CONNECTED 152728 - unix 3 [ ] STREAM CONNECTED 152725 - unix 3 [ ] STREAM CONNECTED 150120 1/pilot-agent etc/istio/proxy/XDS#kubectl exec -it deploy/deploy-websrv -c istio-proxy -- ps -efUID PID PPID C STIME TTY TIME CMDistio-p+ 1 0 0 07:11 ? 00:00:00 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --pistio-p+ 12 1 0 07:11 ? 00:00:02 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --draiistio-p+ 91 0 0 07:21 pts/0 00:00:00 ps -ef# 출력되는 IP가 누구인지 확인 해보자kubectl get pod,svc -A -owidekubectl exec -it deploy/deploy-websrv -c istio-proxy -- netstat -antpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:15004 0.0.0.0:* LISTEN 1/pilot-agent tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15021 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15021 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 12/envoy tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 12/envoy tcp 0 0 172.16.1.10:15006 172.16.2.4:37814 ESTABLISHED 12/envoy tcp 0 0 172.16.1.10:15021 172.16.1.1:42632 TIME_WAIT - tcp 0 0 127.0.0.1:39158 127.0.0.1:15020 ESTABLISHED 12/envoy tcp 0 0 172.16.1.10:15021 172.16.1.1:55752 TIME_WAIT - tcp 0 0 172.16.1.10:51370 10.10.200.82:15012 ESTABLISHED 1/pilot-agent tcp 0 0 172.16.1.10:15021 172.16.1.1:50394 TIME_WAIT - tcp 0 0 172.16.1.10:51360 10.10.200.82:15012 ESTABLISHED 1/pilot-agent tcp 0 0 172.16.1.10:15021 172.16.1.1:49496 TIME_WAIT - tcp 0 0 127.0.0.1:39172 127.0.0.1:15020 ESTABLISHED 12/envoy tcp6 0 0 :::80 :::* LISTEN - tcp6 0 0 :::15020 :::* LISTEN 1/pilot-agent tcp6 0 0 127.0.0.1:15020 127.0.0.1:39158 ESTABLISHED 1/pilot-agent tcp6 0 0 127.0.0.1:15020 127.0.0.1:39172 ESTABLISHED 1/pilot-agent # istiod 정보 같이 확인 : 출력되는 IP가 누구인지 확인 해보자kubectl get pod,svc -A -owidekubectl exec -it deploy/istiod -n istio-system -- ps -efkubectl exec -it deploy/istiod -n istio-system -- netstat -antpkubectl exec -it deploy/istiod -n istio-system -- ss -nlpkubectl exec -it deploy/istiod -n istio-system -- ss -np","link":"/blog/2024/10/20/docs/istio/istio-install/"},{"title":"bridge","text":"bridge? 하나의 LAN 내부에서 단말이 많아질 경우 bridge를 사용하여 개선할 수 있다. 단말기가 많아졌을 경우 발생하는 Collision Domain을 분리해 주는 역할을 한다. 2계층 송수신을 처리한다. Ethenet Frame OSI L2계층 (data link) 을 처리 한다. 패킷 유입 &gt; 목적지 MAC 주소확인 &gt; 주소 테이블과 비교 &gt; 해당 디바이스가 연결된 포트로 패킷 전달. bridge 기능12345678910111213141516171819202122232425Learning.출발지의 맥 어드레스를 배운다.브리지/스위치는 포트에 연결된 PC &quot;A&quot;가 통신을 위해 프레임을 내보내면그때 이 PC의 MAC 주소를 읽어서 자신의 bridge table에 저장한다.** bridge-table: 스위치나 브리지에 연결된 사용자들의 맥 주소를 저장하는 데이터베이스Flooding.들어온 포트를 제외한 나머지를 모든 포트로 데이터를 뿌린다.PC &quot;A&quot;가 통신하고자 하는 목적지 PC의 맥 주소가 브리지 테이블에 없으면모든 포트에 데이터를 전송한다. (브로드캐스트 라고 생각해도 됨. 브로드캐스트도 Flooding이 발생함)ForwardingFlooding과 반대로 목적지 PC의 맥 주소를 알고 그 목적지가 브릿지를 건너야 할때오직 그 해당 포트쪽으로만 데이터를 전송한다. Filtering브릿지를 넘어가지 못하게 막는것.출발지와 목적지가 같은 세그먼트에 있을때 일어남.[세그먼트: 브릿지, 라우터, 허브 또는 스위치에 의해 묶어있는 네트워크의 한 부분]이 Filtering 기능 때문에 허브와 다르게 콜리젼 도메인을 나눌 수 있다.Aging브리지 테이블에 출발지의 MAC 주소가 들어오면 300초 동안 타이머를 설정한다.300초가 지나도록 다시 그 출발지 주소의 프레임이 들어오지 않으면 브리지 테이블에서 삭제한다. Netfilter커널 모듈로 네트워크 패킷을 처리하는 프레임워크이다.네트워크 연산을 핸들러 형태로 처리할 수 있도록 hook 지점을 제공패킷이 어떻게 전송될 지에 대한 결정 방법을 제공한다. Netfilter 항목 항목 내용 키워드 table 용도별 rule 모음 filter, nat, mangle chain 패팃이 지나가는 hook 별로 존재 PREROUTING, FORWARD, INPUT (netfilter의 hook에 매핑) rule table과 chain matrix에 대해서 정의함 protocol type, dest/src address .. action(target) 패킷이 룰에 매칭되면 트리거 됨 ACCEPT, DROP, REJECT red -&gt; bridge -&gt; blue 통신 bridge 관점에서 보자. 패킷은 netfilter에서 파놓은 여러 hook을 통과하는데 hook별로 iptables에 정의한 각 체인룰을 점검한다. hook에 정의된 체인은 테이블 순서대로 등록된 룰을 체크하고 조건을 만족하면 action(target)을 트리거한다. 외부(src) -&gt; 외부(dst) 패킷이므로 FORWARD 체인의 filter 테이블 룰을 봐야 한다. FORWARD: NF_IP_FORWARD (hook) 에 등록된 체인 NF_IP_FORWARD: incoming 패킷이 다른 호스트로 포워딩 되는 경우 트리거되는 netfilter hook filter(table): 패킷을 목적지로 전송여부를 결정 rule 설정 테이블 filter 에서 FORWARD 체인을 추가한다. 출발지가 1.1.1.0/24 이면, ACCEPT 로 설정한다. iptables -t filter -A FORWARD -s 1.1.1.0/24 -j ACCEPT iptalbes netfilter가 파놓은 hook에 룰을 등록하고 관리하는 방법을 제공하는 툴 netfilter에 내가 원하는 정책을 넣어야 하는데 그럴 때 사용하는 프로그램. 룰을 넣고 조회하는 역할을 한다. 정리A ns 에서 B ns 로 통신하기 위해 중간에 bridge 를 통과한다.bridge 는 2계층 데이터 를 처리하는데, A, B 사이의 ARP 테이블을 채워주게 한다.ping의 경우, ICMP를 이용해 송수신하는데 이 때 Netfilter 의 hook이 트리거 되어 ACCEPT 인 경우에만송수신이 가능하다.결국은 arp, bridge fdb 정보를 이용해서 통신하고 iptables rule에 따라 전송여부가 결정된다.","link":"/blog/2024/12/08/docs/namespace/bridge/"},{"title":"network namespace test","text":"namespace, veth 등을 생성해서 1:1 통신을 해보자. 테스트namespace, virture ethernet, ip 설정1234567891011$ ip link$ ip netns$ ip netns add red$ ip netns add blue$ ip link add veth-red type veth peer name veth-blue$ ip link set veth-red netns red$ ip link set veth-blue netns blue$ ip netns exec red ip link set veth-red up$ ip netns exec blue ip link set veth-blue up$ ip netns exec red ip addr add 1.1.1.1/24 dev veth-red$ ip netns exec blue ip addr add 1.1.1.2/24 dev veth-blue Blue namesace tcpdump 실행 red ns 에서 ping 요청을 하면, arp table에 등록 후, 해당 mac 으로 응답을 보내고 이 후 icmp 요청에 대한 응답을 처리한다. 123456789$ nsenter --net=/var/run/netns/blue$ tcpdump -li veth-bluetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth-blue, link-type EN10MB (Ethernet), capture size 262144 bytes07:12:35.267019 ARP, Request who-has 1.1.1.2 tell 1.1.1.1, length 2807:12:35.267034 ARP, Reply 1.1.1.2 is-at 4e:87:56:0c:9e:d0 (oui Unknown), length 2807:12:35.267037 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 1758, seq 1, length 6407:12:35.267045 IP 1.1.1.2 &gt; 1.1.1.1: ICMP echo reply, id 1758, seq 1, length 6407:12:36.268644 IP 1.1.1.1 &gt; 1.1.1.2: ICMP echo request, id 1758, seq 2, length 64 arp 정보 조회 12$ ip neigh show1.1.1.1 dev veth-blue lladdr f6:e9:d4:cf:26:3a STALE 123root@seongtki:~# arpAddress HWtype HWaddress Flags Mask Iface1.1.1.1 ether f6:e9:d4:cf:26:3a C veth-blue 각종 정보 조회 12345678910111213141516171819202122$ ip address show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:005: veth-blue@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 4e:87:56:0c:9e:d0 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 1.1.1.2/24 scope global veth-blue valid_lft forever preferred_lft forever inet6 fe80::4c87:56ff:fe0c:9ed0/64 scope link valid_lft forever preferred_lft forever$ bridge fdb show33:33:00:00:00:01 dev veth-blue self permanent01:00:5e:00:00:01 dev veth-blue self permanent33:33:ff:0c:9e:d0 dev veth-blue self permanent$ ip route show1.1.1.0/24 dev veth-blue proto kernel scope link src 1.1.1.2$ iptables -S-P INPUT ACCEPT-P FORWARD ACCEPT-P OUTPUT ACCEPT red namesace blue ns로 ping을 보낸다. 처음엔 arp 정보가 없는데, arp 응답이 오게 되면 arp table 등록 후 icmp 통시을 하게 된다. 123456$ nsenter --net=/var/run/netns/redPING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.041 ms64 bytes from 1.1.1.2: icmp_seq=2 ttl=64 time=0.045 ms64 bytes from 1.1.1.2: icmp_seq=3 ttl=64 time=0.061 ms arp 정보 조회 12$ ip neigh show1.1.1.2 dev veth-red lladdr 4e:87:56:0c:9e:d0 STAL 123$ arpAddress HWtype HWaddress Flags Mask Iface1.1.1.2 ether 4e:87:56:0c:9e:d0 C veth-red 각종 정보 조회 12345678910111213141516171819202122$ ip address show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:006: veth-red@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether f6:e9:d4:cf:26:3a brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet 1.1.1.1/24 scope global veth-red valid_lft forever preferred_lft forever inet6 fe80::f4e9:d4ff:fecf:263a/64 scope link valid_lft forever preferred_lft forever$ bridge fdb show33:33:00:00:00:01 dev veth-red self permanent01:00:5e:00:00:01 dev veth-red self permanent33:33:ff:cf:26:3a dev veth-red self permanent$ ip route show1.1.1.0/24 dev veth-red proto kernel scope link src 1.1.1.1$ iptables -S-P INPUT ACCEPT-P FORWARD ACCEPT-P OUTPUT ACCEPT 삭제 테스트를 후 생성한 리소들을 삭제해준다. 12$ ip -n &quot;netns name&quot; link del &quot;veth-red&quot;$ ip netns del &quot;name&quot;","link":"/blog/2024/08/31/docs/namespace/namespace-test/"},{"title":"Enumeration","text":"상수목록을 담을 수 있는 데이터 타입. 특정한 변수가 가질 수 있는 값을 제한할 수 있다. (type-safety)를 보장할 수 있다. 다른타입으로 정의할 경우 다른의미로 사용할 때의 예외처리가 필수이다. 싱글톤 패턴을 구현할 때 사용하기 도 한다. 12345public enum OrderStatus { PREPARING(0), SHIPPED(1), DELIVERING(2), DELIVERED(3); ...} jvm 내에서 인스턴스가 딱 하나만 생성이 된다. == 로 비교. (인스턴스가 한개이기 때문.) equals 의 nullPointerException 을 피할 수 있다. 1234Order order = new Order();if (order.orderStatus == OrderStatus.DELIVERED) { System.out.println(&quot;delivered&quot;);} Reference[Java] Enum, 자바의 열거타입을 알아보자 https://techblog.woowahan.com/2527/","link":"/blog/2024/11/10/docs/java/enum/"},{"title":"user namespace","text":"프로세스의 UID/GID Isolation 중첩(nested) 구조 (최대 32레벨) 보안 상 중요 컨테이너의 “root 권한” 문제를 해결 네임스페이스 안과 밖의 UID/GID를 다르게 설정할 수 있음 Linux USER User name? 커널이 취급하는 값이 아닙니다(managed by external tools, /etc/passwd, LDAP, Kerberos, …) 커널이 아는 것은 UID, GID UID, GID space ~ 커널이 관리 프로세스와 호스트 사이의 UID/GID 매핑 ~ Secure 측면에서 중요 권한(permission) 체크 ~ UID, GID를 검사 컨테이너? isolated processes로 호스트의 “커널”을 공유 Same UID, Same User Linux USER namespace isolation -U : user namespace 격리 –map-root-user : user id를 remap 해서 usernamespace 를 생성 1unshare -U --map-root-user /bin/sh namespace 가 격리되어 inode가 다름을 알 수 있다. 1234# iduid=0(root) gid=0(root) groups=0(root),65534(nogroup)# readlink /proc/$$/ns/useruser:[4026532670] 123456789seongtki@seongtki:~$ iduid=1000(seongtki) gid=1000(seongtki) groups=1000(seongtki),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lxd),118(docker)seongtki@seongtki:~$seongtki@seongtki:~$ ps -ef | grep /bin/shseongtki 1898 1415 0 00:35 pts/0 00:00:00 /bin/shseongtki 1902 1662 0 00:35 pts/1 00:00:00 grep --color=auto /bin/shseongtki@seongtki:~$ readlink /proc/$$/ns/useruser:[4026531837] 컨테이너 안에서는 root이지만 실제로 host에서 보면 seongtki로 실행이 되었고, inode도 다르다. USER 네임스페이스 간 UID/GID Remap 되어잇음. Docker Container와 User namespacedocker run (일반 계정)1docker run --name test --rm -d ubuntu sleep 3600 컨테이너가 root로 실행됨 1234seongtki@seongtki:~$ docker exec test ps aux |grep sleeproot 1 0.0 0.0 2200 768 ? Ss 05:14 0:00 sleep 3600seongtki@seongtki:~$ ps aux | grep sleeproot 4629 0.0 0.0 2200 768 ? Ss 05:14 0:00 sleep 3600 Dockerfile에서 실행 시 USER를 정해본다. 12345# DockerfileFROM ubuntu:latestRUN useradd -r -u 1000 testuserUSER testuserENTRYPOINT [&quot;sleep&quot;, &quot;3600&quot;] 12$ docker build -t testuser -f Dockerfile .$ docker run --name test2 --rm -d testuser 도커를 그냥 실행했을 때와 USER를 지정하고 실행했을 때 차이. 1234seongtki@seongtki:~$ docker exec test iduid=0(root) gid=0(root) groups=0(root)seongtki@seongtki:~$ docker exec test2 iduid=1000(testuser) gid=999(testuser) groups=999(testuser) test 컨테이너 - root (uid 0) 으로 실행됨 test2 -&gt; seongtki (uid 1000) 으로 실행됨 test2 는 testuser로 보이고 host에서는 seongtki 로보임 커널에서는 둘 다 uid = 1000 으로 취급된다. (/etc/passwd) 123seongtki@seongtki:~$ ps aux | grep sleeproot 4629 0.0 0.0 2200 768 ? Ss 05:14 0:00 sleep 3600seongtki 4884 0.0 0.0 2200 784 ? Ss 05:16 0:00 sleep 3600 user namespace inode 가 같으므로 같은 호스트임을 알 수 있다. 123456root@seongtki:~# lsns -t user -p 4629 NS TYPE NPROCS PID USER COMMAND4026531837 user 154 1 root /sbin/initroot@seongtki:~# lsns -t user -p 4884 NS TYPE NPROCS PID USER COMMAND4026531837 user 154 1 root /sbin/init 도커 명령어로 user 접근해보기 그런데, –user 0 으로 하면 root 로 실행된다. 12345678910111213docker run --name test3 -d --user 1000 --rm ubuntu sleep 3600seongtki@seongtki:~$ ps aux | grep sleeproot 4629 0.0 0.0 2200 768 ? Ss 05:14 0:00 sleep 3600seongtki 4884 0.0 0.0 2200 784 ? Ss 05:16 0:00 sleep 3600seongtki 5034 0.0 0.0 2200 780 ? Ss 05:21 0:00 sleep 3600seongtki@seongtki:~$ docker exec test3 iduid=1000 gid=0(root) groups=0(root)seongtki@seongtki:~$ sudo lsns -t user -p 5034 NS TYPE NPROCS PID USER COMMAND4026531837 user 155 1 root /sbin/init 도커의 root 사용 패키지인스톨이 쉬움 시스템리소스 사용에 제약이 없음 도커 v1.10+ 에서 user namespace 제공 호스트의 uid/gid를 다른 uid/gid로 매핑하는 방식 보안관점에서 큰 진보 도커 root 보안취약점 실제 root 권한으로 host 파일시스템을 조작할 수 있다. 12seongtki@seongtki:~$ docker run --rm -v /etc:/root/etc -it ubunturoot@1608149fa966:/# echo &quot;127.0.0.1 hello&quot; &gt;&gt; /root/etc/hosts # root 권한이 있어 가능함. host에서 실제 값이 입력됨을 확인. 1234567891011root@seongtki:~# cat /etc/hosts127.0.0.1 localhost127.0.1.1 seongtki# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters127.0.0.1 hello # 컨테이너에서 입력한 값이 실제로 입력됨. docker remap docker에서 user namespace를 적용해보자. uid, gid를 remap할 대역을 정의한다. 12sudo sh -c 'echo dockremap:500000:65536 &gt; /etc/subuid' # {username}:{start}:{count} sudo sh -c 'echo dockremap:500000:65536 &gt; /etc/subgid' # {groupname}:{start}:{count} 1234seongtki@seongtki:~$ sudo cat /etc/subuiddockremap:500000:65536seongtki@seongtki:~$ sudo cat /etc/subgiddockremap:500000:65536 실행 옵션에 remap 설정 1234567891011seongtki@seongtki:~$ sudo vi /etc/systemd/system/docker.service[Service] # add lineExecStart=/usr/bin/dockerd -H fd:// -H tcp://127.0.0.1:2375 --userns-remap=default # add lineseongtki@seongtki:~$ sudo systemctl daemon-reloadseongtki@seongtki:~$ sudo systemctl restart docker# 설정을 다시 없애려면/etc/systemd/system/docker.service # 내부에 내용을 모두 지우고sudo systemctl unmask docker # 실행하고 docker 재실행 하면 된다. subuid, subgid 컨테이너 실행 1docker run --rm -v /etc:/root/etc -it ubuntu 컨테이너에 1234567891011121314151617181920212223# 권한제한 확인root@cb4263e1b582:/# echo &quot;127.0.0.1 hello2&quot; &gt;&gt; /root/etc/hostsbash: /root/etc/hosts: Permission deniedroot@cb4263e1b582:/# iduid=0(root) gid=0(root) groups=0(root) # 컨테이너 내부에서는 root 이다.root@cb4263e1b582:/# ls -al /roottotal 20drwx------ 1 root root 4096 Oct 12 02:57 .drwxr-xr-x 1 root root 4096 Oct 12 02:57 ..-rw-r--r-- 1 root root 3106 Oct 15 2021 .bashrc-rw-r--r-- 1 root root 161 Jul 9 2019 .profiledrwxr-xr-x 110 nobody nogroup 4096 Oct 12 02:54 etc # root 권한이 아니다.root@cb4263e1b582:/# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 4132 3404 pts/0 Ss 02:57 0:00 /bin/bash # pid 1 확인root 10 0.0 0.0 6408 1632 pts/0 R+ 02:57 0:00 ps auxroot@cb4263e1b582:/# lsns -t user -p $$ NS TYPE NPROCS PID USER COMMAND4026532389 user 2 1 root /bin/bash # user namespace inode 확인 host의 user namespace inode 확인 (격리되어 있으므로 다르다) 12345678910root@seongtki:~# ps aux | grep /bin/bash500000 3343 0.0 0.0 4132 3404 pts/0 Ss+ 02:57 0:00 /bin/bashroot 3385 0.0 0.0 5968 660 pts/1 S+ 02:58 0:00 grep --color=auto /bin/bashroot@seongtki:~# lsns -t user -p $$ NS TYPE NPROCS PID USER COMMAND4026531837 user 168 1 root /sbin/init # 컨테이너의 user namespace inode 값과 다르다.root@seongtki:~# readlink /proc/$$/ns/useruser:[4026531837]","link":"/blog/2024/12/08/docs/namespace/user-namespace/"},{"title":"flyweight","text":"객체를 가볍게 만들어 메모리 사용을 줄이는 패턴 자주 변하는 속성(또는 외적인 속성, extrinsit)과 변하지 는 속성(또는 내적인 속성, intrinsit)을 분리하고 재사용하여 메모리 사용을줄일 수 있다. 2. implement 공유하는 인스턴스가 있어서 메모리를 덜 쓴다. 인스턴스를 많이 사용하는 경우에 쓸 수 있는 방식 변경 전 변경되지 않는속성이 중복하여 생성되어 메모리를 잡아먹는다. Font, Font size는 언제사용해도 변하지 않는 속성이디ㅏ. 12345678910public class Client { public static void main(String[] args) { Character c1 = new Character('h', &quot;white&quot;, &quot;Nanum&quot;, 12); Character c2 = new Character('e', &quot;white&quot;, &quot;Nanum&quot;, 12); Character c3 = new Character('l', &quot;white&quot;, &quot;Nanum&quot;, 12); Character c4 = new Character('l', &quot;white&quot;, &quot;Nanum&quot;, 12); Character c5 = new Character('o', &quot;white&quot;, &quot;Nanum&quot;, 12); }} 변경 후 변경되지 않느 속성은 Font class를 생성하여 이동시킨다. 플라이웨이트 팩토리 : FontFactory 를 생성해서 같은 속성(캐싱)을 가져 올 때는 인스턴스를 생성하지 않도록 한다. 플라이웨이트 : Font 123456public static void main(String[] args) { FontFactory fontFactory = new FontFactory(); Character c1 = new Character('h', &quot;white&quot;, fontFactory.getFont(&quot;nanum:12&quot;)); Character c2 = new Character('e', &quot;white&quot;, fontFactory.getFont(&quot;nanum:12&quot;)); Character c3 = new Character('l', &quot;white&quot;, fontFactory.getFont(&quot;nanum:12&quot;)); } 플라이웨이트 (Font) 를 생성 할 때, 사용되는 속성들은 변경이 안되기 때문에 final로 생성한다. 12345678910111213141516171819public final class Font { final String family; final int size; public Font(String family, int size) { this.family = family; this.size = size; } public String getFamily() { return family; } public int getSize() { return size; }} 캐싱 cache를 맴버변수로 생성 getFont 로 cache 한개를 리턴 할때, 포함되어 있으면 그대로 리턴하고 아니라면, 인스턴스를 생성하여 put 한 후 리턴한다. 같은 키 값은 같은 인스턴스를 계속 사용하여 캐싱 효과가 있다. 123456789101112131415public class FontFactory { private Map&lt;String, Font&gt; cache = new HashMap&lt;&gt;(); public Font getFont(String font) { if (cache.containsKey(font)) { return cache.get(font); } else { String[] split = font.split(&quot;:&quot;); Font newFont = new Font(split[0], Integer.parseInt(split[1])); cache.put(font, newFont); return newFont; } }}","link":"/blog/2024/11/10/docs/java/flyweight/"},{"title":"interface","text":"인터페이스에 정적 메소드 기본메소드 (default method)와 정적 메소드를 가질 수 있다. 기본메소드 인터페이스에서 메소드 선언 뿐 아니라, 기본적인 구현체까지 제공할 수 있다. 기존의 인터페이스를 구현하는 클래스에 새로운 기능을 추가할 수 있다. 정적메소드 자바 9부터 인터페이스에서 private static 메소드도 가질 수 있다. 단, private 필드는 아직도 선언할 수 없다. (private 필드가 필요한 경우 인스턴스와 불가 동반클래스가 필요해 보인다.) 인터페이스에서 default, private static method를 정의할 수 있게 되어서인스턴스와 불가 동반 클래스를 만들 필요가 없어졌다. (final class, private 생성자 등) 1234567891011121314151617List&lt;Integer&gt; numbers = new ArrayList();numbers.add(100);numbers.add(20);numbers.add(44);numbers.add(3);System.out.println(numbers);Comparator&lt;Integer&gt; desc = new Comparator&lt;Integer&gt;() { @Override public int compare(Integer o1, Integer o2) { return o2 - o1; }};numbers.sort(desc.reversed());System.out.println(numbers); 1234567@FunctionalInterfacepublic interface Comparator&lt;T&gt; { default Comparator&lt;T&gt; reversed() { return Collections.reverseOrder(this); } ... 인스턴스와 불가 동반 클래스인 Collections 로부터 정적 팩토리 메서드를 수행한다. 12345public class Collections { // Suppresses default constructor, ensuring non-instantiability. private Collections() { }... 123456789101112public interface List&lt;E&gt; extends Collection&lt;E&gt; { default void sort(Comparator&lt;? super E&gt; c) { Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) { i.next(); i.set((E) e); } } ...","link":"/blog/2024/11/10/docs/java/interface/"},{"title":"install","text":"aws 구성설치 VPC 1개(퍼블릭 서브넷 2개), EC2 인스턴스 3대 (Ubuntu 22.04 LTS, t3.xlarge - vCPU 4 , Mem 16) , testpc 1대는 t3.small 12345678910111213141516171819202122232425262728293031curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/kans/kans-8w.yamlaws cloudformation deploy --template-file kans-8w.yaml --stack-name mylab --parameter-overrides KeyName=kp-gasida SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2# CloudFormation 스택 배포 완료 후 작업용 EC2 IP 출력aws cloudformation describe-stacks --stack-name mylab --query 'Stacks[*].Outputs[0].OutputValue' --output text --region ap-northeast-213.125.200.220# [모니터링] CloudFormation 스택 상태 : 생성 완료 확인while true; do date AWS_PAGER=&quot;&quot; aws cloudformation list-stacks \\ --stack-status-filter CREATE_IN_PROGRESS CREATE_COMPLETE CREATE_FAILED DELETE_IN_PROGRESS DELETE_FAILED \\ --query &quot;StackSummaries[*].{StackName:StackName, StackStatus:StackStatus}&quot; \\ --output table sleep 1done----------------------------------| ListStacks |+------------+-------------------+| StackName | StackStatus |+------------+-------------------+| mylab | CREATE_COMPLETE |+------------+-------------------+# 접속ssh -i ~/.ssh/container.pem ubuntu@13.125.200.220","link":"/blog/2024/10/27/docs/cilium/install/"},{"title":"cilium","text":"Cilium은 eBPF (Berkeley Packet Filter)를 기반으로 Pod Network 환경 + 보안 을 제공하는 CNI Plugin 입니다 Kubernetes와 같은 Linux 컨테이너 관리 플랫폼을 사용하여 배포된 응용 프로그램 서비스 간의 네트워크 및 API 연결을 제공하는 오픈 소스 소프트웨어 입니다. BPF/eBPF 소개 - 링크Linux Network Stack : 리눅스 네트워크 스택의 단점은 복잡하고, 변경에 시간이 걸리고, 레이어를 건너뛰기 어렵다. IPTables/Netfilter 방식과 eBPF 방식 비교를 잘 설명한 포스팅 끄적끄적 공간 : 네이버 블로그 Cilium 소개Cilium은 eBPF (Berkeley Packet Filter)를 기반으로 Pod Network 환경 + 보안 을 제공하는 CNI Plugin 입니다 Kubernetes와 같은 Linux 컨테이너 관리 플랫폼을 사용하여 배포된 응용 프로그램 서비스 간의 네트워크 및 API 연결을 제공하는 오픈 소스 소프트웨어 입니다. Cilium 아키텍처 Cilium Agent : 데몬셋으로 실행, K8S API 설정으로 부터 ‘네트워크 설정, 네트워크 정책, 서비스 부하분산, 모니터링’ 등을 수행하며, eBPF 프로그램을 관리한다. Cilium Client (CLI) : Cilium 커멘드툴, eBPF maps 에 직접 접속하여 상태를 확인할 수 있다. Cilium Operator : K8S 클러스터에 대한 한 번씩 처리해야 하는 작업을 관리. Hubble : 네트워크와 보안 모니터링 플랫폼 역할을 하여, ‘Server, Relay, Client, Graphical UI’ 로 구성되어 있다. Data Store : Cilium Agent 간의 상태를 저장하고 전파하는 데이터 저장소, 2가지 종류 중 선택(K8S CRDs, Key-Value Store) Cilium 배포Cilium 설치 정보(w/Helm) 및 확인 - Docs 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 모니터링watch -d kubectl get node,pod -A -owide#helm repo add cilium https://helm.cilium.io/helm repo update#helm install cilium cilium/cilium --version 1.16.3 --namespace kube-system \\--set k8sServiceHost=192.168.10.10 --set k8sServicePort=6443 --set debug.enabled=true \\--set rollOutCiliumPods=true --set routingMode=native --set autoDirectNodeRoutes=true \\--set bpf.masquerade=true --set bpf.hostRouting=true --set endpointRoutes.enabled=true \\--set ipam.mode=kubernetes --set k8s.requireIPv4PodCIDR=true --set kubeProxyReplacement=true \\--set ipv4NativeRoutingCIDR=192.168.0.0/16 --set installNoConntrackIptablesRules=true \\--set hubble.ui.enabled=true --set hubble.relay.enabled=true --set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.metrics.enableOpenMetrics=true \\--set hubble.metrics.enabled=&quot;{dns:query;ignoreAAAA,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip\\,source_namespace\\,source_workload\\,destination_ip\\,destination_namespace\\,destination_workload\\,traffic_direction}&quot; \\--set operator.replicas=1## 주요 파라미터 설명--set debug.enabled=true # cilium 파드에 로그 레벨을 debug 설정--set autoDirectNodeRoutes=true # 동일 대역 내의 노드들 끼리는 상대 노드의 podCIDR 대역의 라우팅이 자동으로 설정--set endpointRoutes.enabled=true # 호스트에 endpoint(파드)별 개별 라우팅 설정--set hubble.relay.enabled=true --set hubble.ui.enabled=true # hubble 활성화--set ipam.mode=kubernetes --set k8s.requireIPv4PodCIDR=true # k8s IPAM 활용--set kubeProxyReplacement=true # kube-proxy 없이 (최대한) 대처할수 있수 있게--set ipv4NativeRoutingCIDR=192.168.0.0/16 # 해당 대역과 통신 시 IP Masq 하지 않음, 보통 사내망 대역을 지정--set operator.replicas=1 # cilium-operator 파드 기본 1개--set enableIPv4Masquerade=true --set bpf.masquerade=true # 파드를 위한 Masquerade , 추가로 Masquerade 을 BPF 로 처리 &gt;&gt; enableIPv4Masquerade=true 인 상태에서 추가로 bpf.masquerade=true 적용이 가능# 설정 및 확인ip -c addrkubectl get node,pod,svc -A -owideiptables -t nat -Siptables -t filter -Siptables -t raw -Siptables -t mangle -Sconntrack -Lkubectl get crdkubectl get ciliumnodes # cilium_host 인터페이스의 IP 확인 : CILIUMINTERNALIPkubectl get ciliumendpoints -Akubectl get cm -n kube-system cilium-config -o json | jqkubetail -n kube-system -l k8s-app=cilium --since 1hkubetail -n kube-system -l k8s-app=cilium-envoy --since 1h# Native XDP 지원 NIC 확인 : https://docs.cilium.io/en/stable/bpf/progtypes/#xdp-driversethtool -i ens5driver: enaversion: 6.8.0-1015-aws...# https://docs.cilium.io/en/stable/operations/performance/tuning/#bypass-iptables-connection-trackingwatch -d kubectl get pod -A # 모니터링helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values --set installNoConntrackIptablesRules=true# 확인: 기존 raw 에 아래 rule 추가 확인iptables -t raw -S | grep notrack-A CILIUM_OUTPUT_raw -d 192.168.0.0/16 -m comment --comment &quot;cilium: NOTRACK for pod traffic&quot; -j CT --notrack-A CILIUM_OUTPUT_raw -s 192.168.0.0/16 -m comment --comment &quot;cilium: NOTRACK for pod traffic&quot; -j CT --notrack...conntrack -Fconntrack -Lconntrack -L |grep -v 2379 Cilium CLI 설치 : inspect the state of a Cilium installation, and enable/disable various features (e.g. clustermesh, Hubble) - Link 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# Cilium CLI 설치CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)CLI_ARCH=amd64if [ &quot;$(uname -m)&quot; = &quot;aarch64&quot; ]; then CLI_ARCH=arm64; ficurl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sumsudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/binrm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}# 확인cilium status --waitcilium config view /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: OK \\__/¯¯\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled # cilium 데몬셋 파드 내에서 cilium 명령어로 상태 확인export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-s -o jsonpath='{.items[0].metadata.name}')alias c0=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium&quot;c0 status --verbose...KubeProxyReplacement: True [ens5 192.168.10.10 fe80::57:abff:fee3:da8d (Direct Routing)]...IPAM: IPv4: 2/254 allocated from 172.16.0.0/24, Allocated addresses: 172.16.0.159 (router) 172.16.0.171 (health)...Routing: Network: Native Host: BPF...Device Mode: vethMasquerading: BPF [ens5] 192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]... Proxy Status: OK, ip 172.16.0.159, 0 redirects active on ports 10000-20000, Envoy: external...KubeProxyReplacement Details: Status: True Socket LB: Enabled Socket LB Tracing: Enabled Socket LB Coverage: Full Devices: ens5 192.168.10.10 fe80::57:abff:fee3:da8d (Direct Routing) Mode: SNAT Backend Selection: Random Session Affinity: Enabled Graceful Termination: Enabled NAT46/64 Support: Disabled XDP Acceleration: Disabled Services: - ClusterIP: Enabled - NodePort: Enabled (Range: 30000-32767) - LoadBalancer: Enabled - externalIPs: Enabled - HostPort: EnabledBPF Maps: dynamic sizing: on (ratio: 0.002500)...# Native Routing 확인 : # 192.168.0.0/16 대역은 IP Masq 없이 라우팅c0 status | grep KubeProxyReplacementKubeProxyReplacement: True [ens5 192.168.10.10 fe80::35:fff:fee7:de77 (Direct Routing)]# enableIPv4Masquerade=true(기본값) , bpf.masquerade=true 확인cilium config view | egrep 'enable-ipv4-masquerade|enable-bpf-masquerade'enable-bpf-masquerade trueenable-ipv4-masquerade truec0 status --verbose | grep MasqueradingMasquerading: BPF [ens5] 192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]# Configure the eBPF-based ip-masq-agent# https://docs.cilium.io/en/stable/network/concepts/masquerading/helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values --set ipMasqAgent.enabled=true#cilium config view | grep -i masqenable-bpf-masquerade trueenable-ip-masq-agent true...export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-s -o jsonpath='{.items[0].metadata.name}')alias c0=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium&quot;c0 status --verbose | grep MasqueradingMasquerading: BPF (ip-masq-agent) [ens5] 192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]kubectl get cm -n kube-system cilium-config -o yaml | grep ip-masq enable-ip-masq-agent: &quot;true&quot; Cilium 기본 정보 확인변수 &amp; 단축키 1234567891011121314151617181920212223# cilium 파드 이름export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-s -o jsonpath='{.items[0].metadata.name}')export CILIUMPOD1=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-w1 -o jsonpath='{.items[0].metadata.name}')export CILIUMPOD2=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-w2 -o jsonpath='{.items[0].metadata.name}')# 단축키(alias) 지정alias c0=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium&quot;alias c1=&quot;kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- cilium&quot;alias c2=&quot;kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- cilium&quot;alias c0bpf=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- bpftool&quot;alias c1bpf=&quot;kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- bpftool&quot;alias c2bpf=&quot;kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- bpftool&quot;# Hubble UI 웹 접속kubectl patch -n kube-system svc hubble-ui -p '{&quot;spec&quot;: {&quot;type&quot;: &quot;NodePort&quot;}}'HubbleUiNodePort=$(kubectl get svc -n kube-system hubble-ui -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Hubble UI URL = http://$(curl -s ipinfo.io/ip):$HubbleUiNodePort&quot;# 자주 사용 명령helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values --setkubetail -n kube-system -l k8s-app=cilium --since 12hkubetail -n kube-system -l k8s-app=cilium-envoy --since 12h 자주 쓰는 Cilium CLI 명령어 1234567891011121314151617181920212223242526272829303132333435363738394041# cilium 파드 확인kubectl get pod -n kube-system -l k8s-app=cilium -owide# cilium 파드 재시작kubectl -n kube-system rollout restart ds/cilium혹은kubectl delete pod -n kube-system -l k8s-app=cilium# cilium 설정 정보 확인cilium config view# cilium 파드의 cilium 상태 확인c0 status --verbose# cilium 엔드포인트 확인kubectl get ciliumendpoints -Ac0 endpoint listc0 bpf endpoint listc0 map get cilium_lxcc0 ip list# Manage the IPCache mappings for IP/CIDR &lt;-&gt; Identityc0 bpf ipcache list# Service/NAT List 확인c0 service listc0 bpf lb listc0 bpf lb list --revnatc0 bpf nat list# List all open BPF mapsc0 map listc0 map list --verbose# List contents of a policy BPF map : Dump all policy mapsc0 bpf policy get --allc0 bpf policy get --all -n# cilium monitorc0 monitor -vc0 monitor -v --type l7 Cilium 기본 정보 확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147# cilium 버전 확인cilium version# cilium 상태 확인cilium status# kube-proxy 파드 확인 &gt;&gt; 없다!kubectl get pod -A# cilium 설정 정보 확인kubectl get cm -n kube-system cilium-config -o yamlcilium config view# ciliumnodes(cn) 정보 확인kubectl get cnkubectl get cn k8s-m -o yaml# 노드별 파드 대역 확인kubectl get ciliumnodes -o yaml | grep podCIDRs -A1# cilium 파드 확인kubectl get pod -n kube-system -l k8s-app=cilium -owide# cilium 엔드포인트 확인kubectl get ciliumendpoints.cilium.io -A--------------------------------------------# cilium cli 도움말c0 help# cilium 파드의 cilium 상태 확인c0 statusc1 statusc2 statusc0 status --verbose# 각 노드에서 파드에 할당된 IP 확인c0 status --verbose | grep Allocated -A5c1 status --verbose | grep Allocated -A5c2 status --verbose | grep Allocated -A5# 엔드포인트 리스트 : ID, 정책, 라벨, IP 주소, 상태 확인c2 endpoint list# 노드 리스트c0 node list# BFP(Direct access to local BPF maps)# BFP 터널 리스트 확인 : Overlay Mode 사용 시 터널 정보 출력c0 bpf tunnel list TUNNEL VALUE 172.16.1.0:0 192.168.200.101:0 172.16.2.0:0 192.168.200.102:0c1 bpf tunnel list TUNNEL VALUE 172.16.2.0:0 192.168.200.102:0 172.16.0.0:0 192.168.200.10:0c2 bpf tunnel list TUNNEL VALUE 172.16.0.0:0 192.168.200.10:0 172.16.1.0:0 192.168.200.101:0# 해당 노드의 로컬 엔드포인트 리스트 : nodemac 은 해당 파드와 veth pair 인 인터페이스의 mac 주소이다!c0 bpf endpoint list# Connection tracking tables - List connection tracking entriesc0 bpf ct list global# Flush all NAT mapping entriesc0 bpf nat flush# List all NAT mapping entriesc0 bpf nat list# service list 확인, Frontend 는 Service IP, Backend 는 Pod IP(endpoint IP)를 보여준다.c0 service list# List load-balancing configurationc0 bpf lb list# List reverse NAT entriesc0 bpf lb list --revnat# List all open BPF mapsc0 map listc0 map list --verbosec0 map get cilium_lxcc0 map get cilium_ipcache# cilium monitorc0 monitor -hc0 monitor -vc0 monitor -v --type l7# Cilium will automatically mount cgroup v2 filesystem required to attach BPF cgroup programs by default at the path /run/cilium/cgroupv2mount | grep ciliumtree /run/cilium/cgroupv2/ -L 1# CNI Plugin 확인tree /etc/cni/net.d/cat /etc/cni/net.d/05-cilium.conf# Manage IP addresses and associated information - IP Listc0 ip list# IDENTITY : 1(host), 2(world), 4(health), 6(remote), 파드마다 개별 ID를 가지는 것으로 보인다!c0 ip list -n# 엔드포인트 설정 확인 및 변경c0 endpoint config &lt;엔트포인트ID&gt;# 엔드포인트 상세 정보 확인c0 endpoint get &lt;엔트포인트ID&gt;# 엔드포인트 로그 확인c0 endpoint log &lt;엔트포인트ID&gt;# Show bpf filesystem mount detailsc0 bpf fs show# bfp 마운트 폴더 확인tree /sys/fs/bpf# List contents of a policy BPF map : Dump all policy mapsc0 bpf policy get --allc0 bpf policy get --all -n# BPF datapath traffic metricsc0 bpf metrics list# Manage the IPCache mappings for IP/CIDR &lt;-&gt; Identityc0 bpf ipcache list# Manage compiled BPF template objectsc0 bpf sha list# Get datapath SHA headerc0 bpf sha get &lt;Datapath SHA&gt;# Retrieve information about an identityc0 identity list# 엔드포인트 기준 IDc0 identity list --endpoints# Access metric statusc0 metrics list 네트워크 기본 정보 확인 : k8s-w1/w2 에 SSH 접속 후 ip -c link/route 정보 확인 12345678910111213141516171819202122232425262728293031323334353637# 네트워크 인터페이스 정보 확인ip -br -c linkip -br -c addr--------------------------------------------# cilium_net 과 cilium_host 는 veth peer 관계이며, cilium_host 는 파드의 GW IP 주소로 지정되며 32bit 이다ip -c addr show cilium_net ; ip -c addr show cilium_host3: cilium_net@cilium_host: &lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP group default qlen 1000 link/ether b2:66:56:35:52:34 brd ff:ff:ff:ff:ff:ff inet6 fe80::b066:56ff:fe35:5234/64 scope link valid_lft forever preferred_lft forever4: cilium_host@cilium_net: &lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP group default qlen 1000 link/ether 76:76:84:b5:8d:5c brd ff:ff:ff:ff:ff:ff inet 172.16.0.128/32 scope global cilium_host valid_lft forever preferred_lft forever inet6 fe80::7476:84ff:feb5:8d5c/64 scope link valid_lft forever preferred_lft forever # proxy arp 는 disable(0) 상태이며, 파드와 연결된 lxc 도 모두 0 이다# 파드의 32bit ip의 gw 가 각각 연결된 veth 인터페이스의 mac 으로 cilium_host 의 IP/MAC 응답을 처리한다, 어떻게 동작이 되는걸까요? &gt;&gt; eBPF program!!!cat /proc/sys/net/ipv4/conf/cilium_net/proxy_arp0cat /proc/sys/net/ipv4/conf/cilium_host/proxy_arp0# lxc_health 인터페이스는 veth 로 cilium(NET NS 0, 호스트와 다름)과 veth pair 이다 - 링크# cilium 인터페이스에 파드 IP가 할당되어 있으며, cilium-health-responder 로 동작한다lsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND4026531840 net 166 1 root unassigned /sbin/init4026532265 net 2 5317 65535 2 /run/netns/cni-f7d5f8f0-c02e-02ca-525c-288b9fd72409 /pause4026532388 net 1 8317 root 0 cilium-health-responder --listen 4240 --pidfile /var/run/cilium/state/health-endpoint.pid4026532390 net 2 5310 65535 1 /run/netns/cni-2dcffbeb-5e8d-d2b8-e920-f7d8bc2449ac /pause Cilium Container Networking Control Flow - Link Hubble UI &amp; CLIObservability Network Observability with Hubble - Link Setting up Hubble Observability - Link Inspecting Network Flows with the CLI - Link Service Map &amp; Hubble UI - Link Configuring Hubble exporter - Link Configure TLS with Hubble - Link Running Prometheus &amp; Grafana - Link Monitoring &amp; Metrics - Link Layer 7 Protocol Visibility - Link Hubble for Network Observability and Security (Part 3): Leveraging Hubble Data for Network Security - Link Hubble for Network Observability and Security (Part 2): Utilizing Hubble for Network Observability - Link Hubble for Network Observability and Security (Part 1): Introduction to Cilium and Hubble - Link Hubble 소개 : 통신 및 서비스와 네트워킹 인프라의 동작에 대한 심층적인 가시성을 완전히 투명한 방식으로 제공하는 관찰성을 제공 - Blog Hubble is a fully distributed networking and security observability platform. → 네트워크/보안 모니터링 It is built on top of Cilium and eBPF to enable deep visibility into the communication and behavior of services as well as the networking infrastructure in a completely transparent manner. without requiring the application to change in any way. → 애플리케이션의 코드 수정 등 추가 설정 없이 동작 containerized workloads as well as more traditional workloads such as virtual machines and standard Linux processes. → VM/서버도 모니터링 가능 By leveraging Linux eBPF, Cilium retains the ability to transparently insert security visibility + enforcement, but does so in a way that is based on service / pod / container identity (in contrast to IP address identification in traditional systems) and can filter on application-layer (e.g. HTTP). → 전통적인 IP 기반은 모니터링/통제가 아니라 서비스/파드/ID 기반으로 모니터링/통제를 제공 기본적으로 Hubble API는 Cilium 에이전트가 실행되는 개별 노드의 범위 내에서 작동합니다. 이는 네트워크 통찰력을 로컬 Cilium 에이전트가 관찰한 트래픽으로 제한합니다. Hubble CLI( hubble)를 사용하여 로컬 Unix Domain Socket을 통해 제공된 Hubble API를 쿼리할 수 있습니다. Hubble CLI 바이너리는 기본적으로 Cilium 에이전트 포드에 설치됩니다. Hubble Relay를 배포하면 전체 클러스터 또는 ClusterMesh 시나리오의 여러 클러스터에 대한 네트워크 가시성이 제공됩니다. 이 모드에서 Hubble 데이터는 Hubble CLI( hubble)를 Hubble Relay 서비스로 지정하거나 Hubble UI를 통해 액세스할 수 있습니다. Hubble UI는 L3/L4 및 L7 계층에서 서비스 종속성 그래프를 자동으로 검색할 수 있는 웹 인터페이스로, 사용자 친화적인 시각화 및 서비스 맵으로서의 데이터 흐름 필터링을 허용합니다. Hubble UI/CLI 접근 및 확인 - Docs 12345678910111213141516171819202122232425262728293031323334353637# 확인cilium status# UI 파드 정보 확인kubectl get pod -n kube-system -l k8s-app=hubble-ui -o wide# Hubble UI 웹 접속kubectl patch -n kube-system svc hubble-ui -p '{&quot;spec&quot;: {&quot;type&quot;: &quot;NodePort&quot;}}'HubbleUiNodePort=$(kubectl get svc -n kube-system hubble-ui -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Hubble UI URL = http://$(curl -s ipinfo.io/ip):$HubbleUiNodePort&quot;## Service NodePort 생성 후 아래 정보 확인!iptables -t nat -Sconntrack -Lconntrack -L |grep -v 2379# Install Hubble ClientHUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)HUBBLE_ARCH=amd64if [ &quot;$(uname -m)&quot; = &quot;aarch64&quot; ]; then HUBBLE_ARCH=arm64; ficurl -L --fail --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}sha256sum --check hubble-linux-${HUBBLE_ARCH}.tar.gz.sha256sumsudo tar xzvfC hubble-linux-${HUBBLE_ARCH}.tar.gz /usr/local/binrm hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}# Hubble API Access : localhost TCP 4245 Relay 를 통해 접근, observe 를 통해서 flow 쿼리 확인 가능!cilium hubble port-forward &amp;# CLI 로 Hubble API 상태 확인hubble status# query the flow API and look for flowshubble observe# hubble observe --pod netpod# hubble observe --namespace galaxy --http-method POST --http-path /v1/request-landing# hubble observe --pod deathstar --protocol http# hubble observe --pod deathstar --verdict DROPPED 자가 테스트 cilium connectivity test → Hubble UI 접속 후 cilium-test-Y 네임스페이스 선택 ⇒ 30분 소요, 여유 시간에 실행 해볼것! 1$ cilium connectivity test 삭제 1kubectl delete ns cilium-test-Y","link":"/blog/2024/10/27/docs/cilium/cilium/"},{"title":"ipvs-install","text":"https://seongtaekkim.github.io/blog/categories/loadbaleancer-metalLB/https://seongtaekkim.github.io/blog/categories/loadbaleancer-ipvs/ strictARP: true : ARP 패킷을 보다 엄격하게 처리하겠다는 설정. stric ARP가 활성화되면, 노드의 인터페이스는 자신에게 할당된 IP주소에 대해서만 ARP 응답을 보낸다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135# 파일 작성cat &lt;&lt;EOT&gt; kind-svc-2w-ipvs.yamlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4featureGates: &quot;InPlacePodVerticalScaling&quot;: true &quot;MultiCIDRServiceAllocator&quot;: truenodes:- role: control-plane labels: mynode: control-plane topology.kubernetes.io/zone: ap-northeast-2a extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - containerPort: 30003 hostPort: 30003 - containerPort: 30004 hostPort: 30004 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: runtime-config: api/all=true controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0 ipvs: strictARP: true- role: worker labels: mynode: worker1 topology.kubernetes.io/zone: ap-northeast-2a- role: worker labels: mynode: worker2 topology.kubernetes.io/zone: ap-northeast-2b- role: worker labels: mynode: worker3 topology.kubernetes.io/zone: ap-northeast-2cnetworking: podSubnet: 10.10.0.0/16 serviceSubnet: 10.200.1.0/24 kubeProxyMode: &quot;ipvs&quot; EOT# k8s 클러스터 설치kind create cluster --config kind-svc-2w-ipvs.yaml --name myk8s --image kindest/node:v1.31.0docker ps# 노드에 기본 툴 설치docker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'for i in worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'; echo; done# kube-proxy configmap 확인kubectl describe cm -n kube-system kube-proxy...mode: ipvsipvs: # 아래 각각 옵션 의미 조사해보자! excludeCIDRs: null minSyncPeriod: 0s scheduler: &quot;&quot; strictARP: true # MetalLB 동작을 위해서 true 설정 변경 필요 syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s...# strictARP: true는 ARP 패킷을 보다 엄격하게 처리하겠다는 설정입니다.## IPVS 모드에서 strict ARP가 활성화되면, 노드의 인터페이스는 자신에게 할당된 IP 주소에 대해서만 ARP 응답을 보내게 됩니다. ## 이는 IPVS로 로드밸런싱할 때 ARP 패킷이 잘못된 인터페이스로 전달되는 문제를 방지합니다.## 이 설정은 특히 클러스터 내에서 여러 노드가 동일한 IP를 갖는 VIP(Virtual IP)를 사용하는 경우 중요합니다.# 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c route; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c addr; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -br -c addr show kube-ipvs0; echo; done# kube-ipvs0 에 할당된 IP(기본 IP + 보조 IP들) 정보 확인 kubectl get svc,ep -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.200.1.1 &lt;none&gt; 443/TCP 3m8skube-system kube-dns ClusterIP 10.200.1.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 3m7s# ipvsadm 툴로 부하분산 되는 정보 확인 : 서비스의 IP와 서비스에 연동되어 있는 파드의 IP 를 확인## Service IP(VIP) 처리를 ipvs 에서 담당 -&gt; 이를 통해 iptables 에 체인/정책이 상당 수준 줄어듬for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ipvsadm -Ln ; echo; done## IPSET 확인docker exec -it myk8s-worker ipset -hdocker exec -it myk8s-worker ipset -L# iptables 정보 확인 : 정책 갯수를 iptables proxy 모드와 비교해보자for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done# 각 노드 bash 접속docker exec -it myk8s-control-plane bashdocker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bashdocker exec -it myk8s-worker3 bash----------------------------------------exit----------------------------------------# mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정 혹은 IP 지정 없이 배포docker run -d --rm --name mypc --network kind --ip 172.18.0.100 nicolaka/netshoot sleep infinity혹은docker run -d --rm --name mypc --network kind nicolaka/netshoot sleep infinitydocker ps","link":"/blog/2024/12/22/docs/loadbalancer/ipvs-install/"},{"title":"pid namespace","text":"Process ID 넘버스페이스를 isolate 한다. 부모,자식 네임스페이스 중첩구조 부모 네임스페이스 (see) -&gt; 자식 네임스페이스 host pid 1 init 프로세스 (커널이 생성해준다) 시그널 처리 좀비, 고아프로세스 처리 kill 1 되면, 시스템 패닉되어 reboot해야함 컨테이너 pid 1 unshare 할 때 fork 하여 자식 pid namespace의 pid 1 로 실행 시그널 처리 좀비, 고아프로세스 처리 죽으면 컨테이너가 종료됨 unshare -p : pid namespace -f : fork –mount-proc : proc 파일시스템 마운트 /proc 메모리 기반의 가상파일 시스템 pseudo filesystem (memory based virtual filesystem) 커널이 관리하는 시스템 정보를 제공 시스템 모니터링과 분석에 활용 커널 데이터 구조의 접근을 쉽게 할 수 있음 ps, mount ,umount 할 때 /proc 파일시스템을 통해서 명령어가 실행됨. 컨테이너 (pid namespace) 안에서 프로세스 정보를 조회하고 제어하기 위해 사용한다. 예제 fork하면서 pid namespace 로 격리함 proc 파일시스템을 마운트 하지 않으면 proc 정보가 보임 12root@seongtki:/tmp# unshare -fp --mount-proc /bin/sh\\# 컨테이너 pid 1은 /bin/sh이다. namespace inode 조회. 12345678910# 컨테이너# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:41 pts/0 00:00:00 /bin/shroot 2 1 0 04:42 pts/0 00:00:00 ps -ef# lsns -t pid -p 1 NS TYPE NPROCS PID USER COMMAND4026532398 pid 2 1 root /bin/sh host pid 1은 /sbin/init 이다. (컨테이너와 다름) 1234root@seongtki:~# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 03:21 ? 00:00:01 /sbin/initroot 2 0 0 03:21 ? 00:00:00 [kthreadd] unshare명령에 대한 프로세스의 자식에 /bin/sh 가 있다. /bin/sh의 inode 를 조회해 보면, 컨테이너의 pid 1의 inode와 같음을 알 수 있다. 123456789root@seongtki:~# ps -ef | grep /bin/shroot 15787 15768 0 04:41 pts/0 00:00:00 unshare -fp --mount-proc /bin/shroot 15788 15787 0 04:41 pts/0 00:00:00 /bin/shroot 15952 15938 0 04:43 pts/2 00:00:00 grep --color=auto /bin/shroot@seongtki:~# lsns -t pid -p 15788 NS TYPE NPROCS PID USER COMMAND4026532398 pid 1 15788 root /bin/sh 위 예제의 pid namespace 구조도는 아래와 같다. signal handlerpid namespace컨테이너의 pid 1을 SIGKILL 하면 컨테이너가 종료되는 것을 알 수 있다. 컨테이너로 시그널을 보낼 때, 핸들링하는 프로그램 데몬이 존재해야 하지만, 없기 때문에 SIGKILL을 제외한 시그널을 무시된다. 12345root@seongtki:~# kill -SIGHUP 15788 # ignore root@seongtki:~# kill -SIGINT 15788 # ignore root@seongtki:~# kill -SIGTERM 15788 # ignore root@seongtki:~# kill -SIGKILL 15788 # ignore root@seongtki:~# kill -SIGKILL 15788 # pid namespace 종료 docker pid busybox 를 실행하고 sleep 3600 cmd 실행하도록 컨테이너를 생성하였다. 1root@seongtki:~# docker run --rm --name busybox busybox sleep 3600 host에서 sleep 3600 가 실행중인걸 알 수 있다. 1234root@seongtki:~# ps aux | grep sleeproot 16005 0.3 0.5 1326556 22572 pts/0 Sl+ 05:20 0:00 docker run --rm --name busybox busybox sleep 3600root 16067 0.0 0.0 3852 400 ? Ss 05:20 0:00 sleep 3600root 16100 0.0 0.0 5968 668 pts/2 S+ 05:20 0:00 grep --color=auto sleep 컨테이너 내부에서 pid1 은 cmd임을 알 수 있다. 이는 시그널을 컨트롤하는 사용자 프로그램이 없다는 뜻이어서, kill을 제외한 다른 시그널에대한 핸들링이 없다. 12345678910root@seongtki:~# docker exec busybox ps -efPID USER TIME COMMAND 1 root 0:00 sleep 3600 7 root 0:00 ps -ef root@seongtki:~# kill -SIGHUP 16067 # ignore root@seongtki:~# kill -SIGINT 16067 # ignoreroot@seongtki:~# kill -SIGTERM 16067 # ignoreroot@seongtki:~# kill -SIGKILL 16067 # ignoreroot@seongtki:~# kill -SIGTERM 16067 # 컨테이너 종료 docker init process –init : docker 에서 제공하는 컨테이너 init process 1root@seongtki:~# docker run --rm --name busybox --init busybox sleep 3600 cmd 인 sleep 3600 부모로 /sbin/docker-init이 존재하는걸 알수 있다. 12345root@seongtki:~# ps aux | grep sleeproot 16182 0.2 0.5 1326812 22972 pts/0 Sl+ 05:22 0:00 docker run --rm --name busybox --init busybox sleep 3600root 16230 0.0 0.0 812 4 ? Ss 05:22 0:00 /sbin/docker-init -- sleep 3600root 16265 0.0 0.0 3852 412 ? S 05:22 0:00 sleep 3600root 16267 0.0 0.0 5968 672 pts/2 S+ 05:22 0:00 grep --color=auto sleep 컨테이너 내부의 ps역시 initprocess 자식으로 cmd가 존재한다. SIGHUP을 날렸을 때, docker-init 프로그램에서 시그널을 핸들링하여 컨테이너가 종료되는 걸 알 수 있다. 1234567root@seongtki:~# docker exec busybox ps -efPID USER TIME COMMAND 1 root 0:00 /sbin/docker-init -- sleep 3600 6 root 0:00 sleep 3600 7 root 0:00 ps -efroot@seongtki:~# kill -SIGHUP 16230","link":"/blog/2024/12/08/docs/namespace/pid-namespace/"},{"title":"ipvs-proxy","text":"구성도 manifests12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOT&gt; 3pod.yamlapiVersion: v1kind: Podmetadata: name: webpod1 labels: app: webpodspec: nodeName: myk8s-worker containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod2 labels: app: webpodspec: nodeName: myk8s-worker2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod3 labels: app: webpodspec: nodeName: myk8s-worker3 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0EOT 1234567891011121314cat &lt;&lt;EOT&gt; netpod.yamlapiVersion: v1kind: Podmetadata: name: net-podspec: nodeName: myk8s-control-plane containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0EOT 1234567891011121314cat &lt;&lt;EOT&gt; svc-clusterip.yamlapiVersion: v1kind: Servicemetadata: name: svc-clusteripspec: ports: - name: svc-webport port: 9000 # 서비스 IP 에 접속 시 사용하는 포트 port 를 의미 targetPort: 80 # 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미 selector: app: webpod # 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨 type: ClusterIP # 서비스 타입EOT 생성 및 확인 : IPVS Proxy Mode123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 생성kubectl apply -f 3pod.yaml,netpod.yaml,svc-clusterip.yaml# 파드와 서비스 사용 네트워크 대역 정보 확인 kubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot;# 확인kubectl get pod -owidekubectl get svc svc-clusteripkubectl describe svc svc-clusteripkubectl get endpoints svc-clusteripkubectl get endpointslices -l kubernetes.io/service-name=svc-clusterip# 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인## ClusterIP 생성 시 kube-ipvs0 인터페이스에 ClusterIP 가 할당되는 것을 확인for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c addr; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -br -c addr show kube-ipvs0; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -d -c addr show kube-ipvs0; echo; done# 변수 지정CIP=$(kubectl get svc svc-clusterip -o jsonpath=&quot;{.spec.clusterIP}&quot;)CPORT=$(kubectl get svc svc-clusterip -o jsonpath=&quot;{.spec.ports[0].port}&quot;)echo $CIP $CPORT# ipvsadm 툴로 부하분산 되는 정보 확인## 10.200.1.216(TCP 9000) 인입 시 3곳의 목적지로 라운드로빈(rr)로 부하분산하여 전달됨을 확인 : 모든 노드에서 동일한 IPVS 분산 설정 정보 확인## 3곳의 목적지는 각각 서비스에 연동된 목적지 파드 3개이며, 전달 시 출발지 IP는 마스커레이딩 변환 처리docker exec -it myk8s-control-plane ipvsadm -Ln -t $CIP:$CPORTfor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ipvsadm -Ln -t $CIP:$CPORT ; echo; done# ipvsadm 툴로 부하분산 되는 현재 연결 정보 확인 : 추가로 --rate 도 있음docker exec -it myk8s-control-plane ipvsadm -Ln -t $CIP:$CPORT --statsfor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ipvsadm -Ln -t $CIP:$CPORT --stats ; echo; donedocker exec -it myk8s-control-plane ipvsadm -Ln -t $CIP:$CPORT --ratefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ipvsadm -Ln -t $CIP:$CPORT --rate ; echo; done# iptables 규칙 확인 : ipset list 를 활용docker exec -it myk8s-control-plane iptables -t nat -S | grep KUBE-CLUSTER-IP# ipset list 정보를 확인 : KUBE-CLUSTER-IP 이름은 아래 6개의 IP:Port 조합을 지칭# 예를 들면 ipset list 를 사용하지 않을 경우 6개의 iptables 규칙이 필요하지만, ipset 사용 시 1개의 규칙으로 가능docker exec -it myk8s-control-plane ipset list KUBE-CLUSTER-IPName: KUBE-CLUSTER-IPType: hash:ip,portRevision: 7Header: family inet hashsize 1024 maxelem 65536 bucketsize 12 initval 0x6343ff52Size in memory: 456References: 3Number of entries: 5Members:10.200.1.1,tcp:44310.200.1.10,tcp:5310.200.1.10,udp:5310.200.1.245,tcp:900010.200.1.10,tcp:9153 IPVS 정보 확인 및 서비스 접속 확인 1234567891011121314151617181920212223242526272829303132for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ipvsadm -Ln -t $CIP:$CPORT ; echo; done# 변수 지정CIP=$(kubectl get svc svc-clusterip -o jsonpath=&quot;{.spec.clusterIP}&quot;)CPORT=$(kubectl get svc svc-clusterip -o jsonpath=&quot;{.spec.ports[0].port}&quot;)echo $CIP $CPORT# 컨트롤플레인 노드에서 ipvsadm 모니터링 실행 : ClusterIP 접속 시 아래 처럼 연결 정보 확인됨watch -d &quot;docker exec -it myk8s-control-plane ipvsadm -Ln -t $CIP:$CPORT --stats; echo; docker exec -it myk8s-control-plane ipvsadm -Ln -t $CIP:$CPORT --rate&quot;--------------------------# 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소SVC1=$(kubectl get svc svc-clusterip -o jsonpath={.spec.clusterIP})echo $SVC1# TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인kubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000kubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000 | grep Hostnamekubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000 | grep Hostname# 서비스(ClusterIP) 부하분산 접속 확인 : 부하분산 비률 확인kubectl exec -it net-pod -- zsh -c &quot;for i in {1..10}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..1000}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;혹은kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; sleep 1; done&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; sleep 0.1; done&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..10000}; do curl -s $SVC1:9000 | grep Hostname; sleep 0.01; done&quot;# 반복 접속kubectl exec -it net-pod -- zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1:9000 | egrep 'Hostname|RemoteAddr|Host:'; date '+%Y-%m-%d %H:%M:%S' ; echo '--------------' ; sleep 1; done&quot; conntrack 확인12345678910111213141516171819202122232425262728293031323334docker exec -it myk8s-control-plane bash----------------------------------------conntrack -hconntrack -Econntrack -Cconntrack -Sconntrack -L --src 10.10.0.6 # net-pod IPconntrack -L --dst $SVC1 # service ClusterIPexit----------------------------------------# (참고) IPVS hash 확인ipvsadm -Lnipvsadm -Ln -t &lt;CluterIP&gt;:&lt;Port&gt;ipvsadm -Ln -t &lt;NodeIP&gt;:&lt;NodePort&gt;# (옵션) IPVS current IPVS connectionsipvsadm -Ln -c# (옵션) IPVS statistics informationipvsadm -Ln --statsipvsadm -Ln --stats -t &lt;NodeIP&gt;:$NPORTwatch -d ipvsadm -Ln --stats -t &lt;NodeIP&gt;:$NPORT# (옵션) IPVS rate informationipvsadm -Ln --rateipvsadm -Ln --rate -t &lt;NodeIP&gt;:$NPORTwatch -d ipvsadm -Ln --rate -t &lt;NodeIP&gt;:$NPORT# iptabels 확인 &gt;&gt; 서비스 정책/룰 추가 확인해보자! , 정책 갯수를 iptables proxy 모드와 비교해보자for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done","link":"/blog/2024/12/22/docs/loadbalancer/ipvs-proxy/"},{"title":"communication","text":"노드 간 파드 통신 확인파드생성 및 확인 123456789101112131415161718192021222324252627282930313233343536373839404142cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: netpod labels: app: netpodspec: nodeName: k8s-s containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod1 labels: app: webpodspec: nodeName: k8s-w1 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod2 labels: app: webpodspec: nodeName: k8s-w2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0EOF 123456789101112# 확인kubectl get pod -o widec0 status --verbose | grep Allocated -A5c1 status --verbose | grep Allocated -A5c2 status --verbose | grep Allocated -A5kubectl get ciliumendpointskubectl get ciliumendpoints -Ac0 endpoint listc0 bpf endpoint listc0 map get cilium_lxcc0 ip list 파드 변수 지정 123456789# 테스트 파드들 IPNETPODIP=$(kubectl get pods netpod -o jsonpath='{.status.podIP}')WEBPOD1IP=$(kubectl get pods webpod1 -o jsonpath='{.status.podIP}')WEBPOD2IP=$(kubectl get pods webpod2 -o jsonpath='{.status.podIP}')# 단축키(alias) 지정alias p0=&quot;kubectl exec -it netpod -- &quot;alias p1=&quot;kubectl exec -it webpod1 -- &quot;alias p2=&quot;kubectl exec -it webpod2 -- &quot; 파드의 ARP 동작 확인 ← Hubble Web UI 모니터링 12345678910111213141516171819202122232425# netpod 네트워크 정보 확인p0 ip -c -4 addrp0 route -np0 ping -c 1 $WEBPOD1IP &amp;&amp; p0 ping -c 1 $WEBPOD2IPp0 curl -s $WEBPOD1IP &amp;&amp; p0 curl -s $WEBPOD2IPp0 curl -s $WEBPOD1IP:8080 ; p0 curl -s $WEBPOD2IP:8080p0 ping -c 1 8.8.8.8 &amp;&amp; p0 curl -s wttr.in/seoulp0 ip -c neigh# hubble cli 확인hubble observe --pod netpodhubble observe --pod webpod1hubble observe --pod webpod2# BPF maps : 목적지 파드와 통신 시 어느곳으로 보내야 될지 확인할 수 있다c0 map get cilium_ipcachec0 map get cilium_ipcache | grep $WEBPOD1IP# netpod 의 LXC 변수 지정LXC=&lt;k8s-s의 가장 나중에 lxc 이름&gt;LXC=lxc335e04832afa# 파드와 veth pair 에 IP가 없다! proxy_arp 도 없다! 하지만 GW MAC 요청 시 lxc(veth)의 MAC 으로 응답이 온다! &gt;&gt; eBPF Magic!# Cilium hijacks ARP table of POD1, forces the next hop to be the peer end (host side) of the veth pair.ip -c addr show dev $LXC Node’s eBPF programs 1234567891011121314151617181920212223# list of eBPF programsc0bpf net showc0bpf net show | grep $LXClxc335e04832afa(12) tcx/ingress cil_from_container prog_id 1529 link_id 26 lxc335e04832afa(12) tcx/egress cil_to_container prog_id 1531 link_id 27 # Use bpftool prog show id to view additional information about a program, including a list of attached eBPF maps:c0bpf prog show id &lt;출력된 prog id 입력&gt;c0bpf prog show id 15291531: sched_cls name cil_to_container tag 3f1e92871a2c4013 gpl loaded_at 2024-10-20T07:47:27+0000 uid 0 xlated 1712B jited 1015B memlock 4096B map_ids 66,239 btf_id 474c0bpf map list...66: percpu_hash name cilium_metrics flags 0x1 key 8B value 16B max_entries 1024 memlock 19384B...239: prog_array name cilium_calls_00 flags 0x0 key 4B value 4B max_entries 50 memlock 720B owner_prog_type sched_cls owner jited... 서비스 통신 확인Socket-Based LoadBalancing 소개 (한글) - 링크그림 왼쪽(네트워크 기반 로드밸런싱) vs 오른쪽(소켓 기반 로드밸런싱) Pod1 안에서 동작하는 앱이 connect() 시스템콜을 이용하여 소켓을 연결할 때 목적지 주소가 서비스 주소(10.10.8.55)이면 소켓의 목적지 주소를 바로 백엔드 주소(10.0.0.31)로 설정한다. 이후 앱에서 해당 소켓을 통해 보내는 모든 패킷의 목적지 주소는 이미 백엔드 주소(10.0.0.31)로 설정되어 있기 때문에 중간에 DNAT 변환 및 역변환 과정이 필요없어진다. 서비스 생성 및 접속 확인 : 파드 내에서 바로 DNAT! Magic서비스 생성 1234567891011121314cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Servicemetadata: name: svcspec: ports: - name: svc-webport port: 80 targetPort: 80 selector: app: webpod type: ClusterIPEOF 서비스 접속 확인 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 서비스 생성 확인kubectl get svc,ep svc# 노드에 iptables 더이상 KUBE-SVC rule 이 생성되지 않는다!iptables-save | grep KUBE-SVCiptables-save | grep CILIUM# 서비스IP를 변수에 지정SVCIP=$(kubectl get svc svc -o jsonpath='{.spec.clusterIP}')# Pod1 에서 Service(ClusterIP) 접속 트래픽 발생kubectl exec netpod -- curl -s $SVCIPkubectl exec netpod -- curl -s $SVCIP | grep Hostname# 지속적으로 접속 트래픽 발생SVCIP=$(kubectl get svc svc -o jsonpath='{.spec.clusterIP}')while true; do kubectl exec netpod -- curl -s $SVCIP | grep Hostname;echo &quot;-----&quot;;sleep 1;done# 파드에서 SVC(ClusterIP) 접속 시 tcpdump 로 확인 &gt;&gt; 파드 내부 캡쳐인데, SVC(10.108.12.195)는 보이지 않고, DNAT 된 web-pod 의 IP가 확인! Magic!kubectl exec netpod -- tcpdump -enni any -qtcpdump: data link type LINUX_SLL2tcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes16:04:05.292279 eth0 Out ifindex 13 e2:dd:e4:3f:03:ba 172.16.0.13.47278 &gt; 172.16.2.11.80: tcp 016:04:05.292795 eth0 In ifindex 13 0a:62:8c:a5:1e:08 172.16.2.11.80 &gt; 172.16.0.13.47278: tcp 016:04:05.292833 eth0 Out ifindex 13 e2:dd:e4:3f:03:ba 172.16.0.13.47278 &gt; 172.16.2.11.80: tcp 016:04:05.292886 eth0 Out ifindex 13 e2:dd:e4:3f:03:ba 172.16.0.13.47278 &gt; 172.16.2.11.80: tcp 74kubectl exec netpod -- sh -c &quot;ngrep -tW byline -d eth0 '' 'tcp port 80'&quot;T 2024/10/20 08:07:36.663329 172.16.0.132:59964 -&gt; 172.16.1.53:80 [AP] #34GET / HTTP/1.1.Host: 10.10.124.15.User-Agent: curl/8.7.1.Accept: */*.# 서비스 정보 확인c0 service listID Frontend Service Type Backend16 10.108.12.195:80 ClusterIP 1 =&gt; 172.16.2.157:80 2 =&gt; 172.16.1.234:80c0 bpf lb listSERVICE ADDRESS BACKEND ADDRESS10.108.12.195:80 0.0.0.0:0 (16) [ClusterIP, non-routable] 172.16.1.234:80 (16) 172.16.2.157:80 (16)# BPF mapsc0 map list --verbosec0 map list --verbose | grep lbc0 map get cilium_lb4_services_v2c0 map get cilium_lb4_backends_v3c0 map get cilium_lb4_reverse_natc0 map get cilium_lb4_reverse_skc0 map get cilium_lxcc0 map get cilium_ipcache Socket-Based LoadBalancing 관련 설정값 확인 및 Cgroup 관련 정보 확인 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# Socket-Based LoadBalancing 관련 설정들 확인c0 status --verbose...KubeProxyReplacement Details: Status: True Socket LB: Enabled Socket LB Tracing: Enabled Socket LB Coverage: Full Devices: ens5 192.168.10.10 fe80::57:abff:fee3:da8d (Direct Routing) Mode: SNAT Backend Selection: Random Session Affinity: Enabled Graceful Termination: Enabled NAT46/64 Support: Disabled XDP Acceleration: Disabled Services: - ClusterIP: Enabled - NodePort: Enabled (Range: 30000-32767) - LoadBalancer: Enabled - externalIPs: Enabled - HostPort: Enabled# cgroup root 경로 확인tree /run/cilium/cgroupv2 -L 1tree /run/cilium/cgroupv2 -L 2cilium config view | grep cgroupcgroup-root /run/cilium/cgroupv2# eBPF cgroup 확인 : Socket based LB 와 관련c0bpf cgroup treeCgroupPathID AttachType AttachFlags Name /sys/fs/cgroup1081 cgroup_device multi 1498 tcx_ingress cil_to_host 1501 tcx_egress cil_from_host # cilium 파드의 Init Containers 에서 cgroup 마운트!Init Containers: mount-cgroup: Container ID: containerd://72e9d2ee9731e3536c893f9daaa7674809638e3d137f9eb0f46fe916c2aa2839 Image: quay.io/cilium/cilium:v1.16.3@sha256:62d2a09bbef840a46099ac4c69421c90f84f28d018d479749049011329aa7f28 Image ID: quay.io/cilium/cilium@sha256:62d2a09bbef840a46099ac4c69421c90f84f28d018d479749049011329aa7f28 Port: &lt;none&gt; Host Port: &lt;none&gt; Command: sh -ec cp /usr/bin/cilium-mount /hostbin/cilium-mount; nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt &quot;${BIN_PATH}/cilium-mount&quot; $CGROUP_ROOT; rm /hostbin/cilium-mount State: Terminated Reason: Completed Exit Code: 0 Started: Sun, 20 Oct 2024 15:45:34 +0900 Finished: Sun, 20 Oct 2024 15:45:34 +0900 Ready: True Restart Count: 0 Environment: CGROUP_ROOT: /run/cilium/cgroupv2 BIN_PATH: /opt/cni/bin Mounts: /hostbin from cni-path (rw) /hostproc from hostproc (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6bcr (ro)# mount-cgroup 로그 확인kubetail -n kube-system -c mount-cgroup --since 12h...[cilium-2rkvc] time=&quot;2024-10-27T00:30:54+09:00&quot; level=info msg=&quot;Mounted cgroupv2 filesystem at /run/cilium/cgroupv2&quot; subsys=cgroups strace 시스템 콜 트레이싱 도구를 통해 파드 내에서 동작 확인* 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Hostname: webpod2IP: 127.0.0.1IP: ::1IP: 172.16.2.11IP: fe80::dc:1cff:fe89:75f5RemoteAddr: 172.16.0.13:45966GET / HTTP/1.1Host: 10.10.39.26User-Agent: curl/8.7.1Accept: */*% time seconds usecs/call calls errors syscall------ ----------- ----------- --------- --------- ---------------- 20.07 0.001076 13 79 mmap 15.50 0.000831 14 56 32 open 8.69 0.000466 17 27 munmap 6.90 0.000370 13 27 close 5.78 0.000310 62 5 getrandom 5.69 0.000305 9 31 rt_sigaction 5.30 0.000284 10 27 read 4.85 0.000260 21 12 fstat 4.50 0.000241 7 31 lseek 3.68 0.000197 197 1 execve 3.21 0.000172 12 14 mprotect 2.87 0.000154 6 23 fcntl 2.41 0.000129 129 1 1 connect 1.60 0.000086 8 10 readv 1.34 0.000072 6 12 rt_sigprocmask 1.10 0.000059 9 6 poll 1.04 0.000056 56 1 sendto 0.97 0.000052 13 4 setsockopt 0.63 0.000034 8 4 getsockname 0.58 0.000031 10 3 brk 0.56 0.000030 30 1 socket 0.56 0.000030 10 3 3 ioctl 0.39 0.000021 21 1 getsockopt 0.34 0.000018 18 1 recvfrom 0.28 0.000015 15 1 pipe 0.28 0.000015 7 2 geteuid 0.22 0.000012 12 1 writev 0.15 0.000008 8 1 getuid 0.15 0.000008 8 1 getgid 0.13 0.000007 7 1 getegid 0.11 0.000006 6 1 arch_prctl 0.09 0.000005 5 1 set_tid_address------ ----------- ----------- --------- --------- ----------------100.00 0.005360 13 389 36 total 삭제 1kubectl delete pod --all &amp;&amp; kubectl delete svc svc Running Prometheus &amp; Grafana123456789101112131415161718# 배포kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/kubernetes/addons/prometheus/monitoring-example.yamlkubectl get all -n cilium-monitoring# 파드와 서비스 확인kubectl get pod,svc,ep -o wide -n cilium-monitoring# NodePort 설정kubectl patch svc grafana -n cilium-monitoring -p '{&quot;spec&quot;: {&quot;type&quot;: &quot;NodePort&quot;}}'kubectl patch svc prometheus -n cilium-monitoring -p '{&quot;spec&quot;: {&quot;type&quot;: &quot;NodePort&quot;}}'# Grafana 웹 접속GPT=$(kubectl get svc -n cilium-monitoring grafana -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Grafana URL = http://$(curl -s ipinfo.io/ip):$GPT&quot;# Prometheus 웹 접속 정보 확인PPT=$(kubectl get svc -n cilium-monitoring prometheus -o jsonpath={.spec.ports[0].nodePort})echo -e &quot;Prometheus URL = http://$(curl -s ipinfo.io/ip):$PPT&quot; Network Policy (L3, L4, L7)Deploy the Demo Application - Docs디플로이먼트(웹 서버, deathstar, replicas 2), 파드(xwing, tiefighter), 서비스(ClusterIP, service/deathstar) 12345678910111213141516171819202122232425262728293031# 배포kubectl create -f https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/minikube/http-sw-app.yamlkubectl get all# 파드 라벨 확인kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSdeathstar-689f66b57d-lth25 1/1 Running 0 10s app.kubernetes.io/name=deathstar,class=deathstar,org=empire,pod-template-hash=689f66b57ddeathstar-689f66b57d-v5xqk 1/1 Running 0 10s app.kubernetes.io/name=deathstar,class=deathstar,org=empire,pod-template-hash=689f66b57dnetpod 1/1 Running 0 36m app=netpodtiefighter 1/1 Running 0 10s app.kubernetes.io/name=tiefighter,class=tiefighter,org=empirewebpod1 1/1 Running 0 36m app=webpodwebpod2 1/1 Running 0 36m app=webpodxwing 1/1 Running 0 10s app.kubernetes.io/name=xwing,class=xwing,org=alliance# cilium endpoint 확인kubectl get ciliumendpointsc1 endpoint listc2 endpoint list# 데스스타 SVC(ClusterIP) 접속하여 웹 파드 연결 확인 &gt;&gt; Hubble UI 에서 실시간 확인해보자!kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landingShip landedkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landingShip landed# 확인hubble observe Identity-Aware and HTTP-Aware Policy Enforcement Apply an L3/L4 Policy - Link &amp; Hubble CLI - 링크 Cilium 에서는 Endpoint IP 대신, 파드의 **Labels(라벨)**을 사용(기준)하여 보안 정책을 적용합니다 IP/Port 필터링을 L3/L4 네트워크 정책이라고 한다 아래 처럼 ‘org=empire’ Labels(라벨) 부착된 파드만 허용해보자 Cilium performs stateful connection tracking 이므로 리턴 트래픽은 자동으로 허용됨 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# L3/L4 정책 생성cat &lt;&lt;EOF | kubectl apply -f - apiVersion: &quot;cilium.io/v2&quot;kind: CiliumNetworkPolicymetadata: name: &quot;rule1&quot;spec: description: &quot;L3-L4 policy to restrict deathstar access to empire ships only&quot; endpointSelector: matchLabels: org: empire class: deathstar ingress: - fromEndpoints: - matchLabels: org: empire toPorts: - ports: - port: &quot;80&quot; protocol: TCPEOF# 정책 확인kubectl get cnpkc describe cnp rule1c0 policy get# 파드 curl 접속 시도 시 파드 sh 접속 후 curl 시도하자!# 데스스타 SVC(ClusterIP) 접속하여 웹 파드 연결 확인 &gt;&gt; Hubble UI 에서 drop 확인!kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landingShip landedkubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landingdrop# hubble cli 모니터링 hubble observe --pod xwinghubble observe --pod tiefighterhubble observe --pod deathstarOct 26 16:22:16.631: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)Oct 26 16:22:16.631: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) Policy denied DROPPED (TCP Flags: SYN)hubble observe --pod deathstar --verdict DROPPEDOct 26 16:22:17.675: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)Oct 26 16:22:17.675: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) Policy denied DROPPED (TCP Flags: SYN)Oct 26 16:22:18.699: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)Oct 26 16:22:18.699: default/xwing:40664 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) Policy denied DROPPED (TCP Flags: SYN)Inspecting the Policy# If we run cilium endpoint list again we will see that the pods with the label org=empire and class=deathstar# now have ingress policy enforcement enabled as per the policy above.# endpoint list 에서 정책 적용 확인c1 endpoint list | grep deathstarc2 endpoint listENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT312 Disabled Disabled 18300 k8s:class=xwing 172.16.2.161 ready k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=alliance1972 Enabled Disabled 21144 k8s:class=deathstar 172.16.2.66 ready k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empirec2 endpoint list Identity-Aware and HTTP-Aware Policy Enforcement Apply and Test HTTP-aware L7 Policy - DocsHTTP L7 필터링을 적용 : 아래 처럼 PUT /v1/exhaust-port 요청을 차단! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 데스스타 SVC(ClusterIP) 접속kubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-portPanic: deathstar exploded...# POST /v1/request-landing API 호출만 허용 정책으로 기존 정책 내용을 업데이트(configured)!cat &lt;&lt;EOF | kubectl apply -f - apiVersion: &quot;cilium.io/v2&quot;kind: CiliumNetworkPolicymetadata: name: &quot;rule1&quot;spec: description: &quot;L7 policy to restrict access to specific HTTP call&quot; endpointSelector: matchLabels: org: empire class: deathstar ingress: - fromEndpoints: - matchLabels: org: empire toPorts: - ports: - port: &quot;80&quot; protocol: TCP rules: http: - method: &quot;POST&quot; path: &quot;/v1/request-landing&quot;EOF# 정책 확인kc describe ciliumnetworkpoliciesc0 policy get# 모니터링c1 monitor -v --type l7c2 monitor -v --type l7&lt;- Request http from 0 ([k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire k8s:class=tiefighter k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default]) to 1972 ([k8s:class=deathstar k8s:org=empire k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.kubernetes.pod.namespace=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.cilium.k8s.policy.cluster=default]), identity 42720-&gt;21144, verdict Denied PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port =&gt; 403 =&gt; 403hubble observe --pod deathstarhubble observe --pod deathstar --verdict DROPPED# 접근 테스트kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landingShip landedkubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-portAccess denied## hubble cli 에 차단 로그 확인hubble observe --pod deathstar --verdict DROPPEDOct 26 16:25:27.755: default/xwing:42756 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)Oct 26 16:25:27.755: default/xwing:42756 (ID:6016) &lt;&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) Policy denied DROPPED (TCP Flags: SYN)hubble observe --pod deathstar --protocol httpOct 26 16:27:18.903: default/tiefighter:51914 (ID:5305) -&gt; default/deathstar-689f66b57d-v5xqk:80 (ID:44044) http-request DROPPED (HTTP/1.1 PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port)# 삭제kubectl delete -f https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/minikube/http-sw-app.yamlkubectl delete cnp rule1 삭제 12kubectl delete -f https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/minikube/http-sw-app.yamlkubectl delete cnp rule1 Bandwidth Manager123456789101112131415161718192021222324252627# 인터페이스 tc qdisc 확인tc qdisc show dev ens5qdisc mq 0: root qdisc fq_codel 0: parent :4 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64 qdisc fq_codel 0: parent :3 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64 qdisc fq_codel 0: parent :2 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64 qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64 # 설정helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values --set bandwidthManager.enabled=true# 적용 확인cilium config view | grep bandwidthenable-bandwidth-manager true# egress bandwidth limitation 동작하는 인터페이스 확인c0 status | grep BandwidthManagerBandwidthManager: EDT with BPF [CUBIC] [ens5]# 인터페이스 tc qdisc 확인 : 설정 전후 옵션값들이 상당히 추가된다tc qdisctc qdisc show dev ens5qdisc mq 8002: root qdisc fq 8005: parent 8002:2 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_drop qdisc fq 8003: parent 8002:4 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_drop qdisc fq 8004: parent 8002:3 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_drop qdisc fq 8006: parent 8002:1 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_drop 설정 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 테스트를 위한 트래픽 발생 서버/클라이언트 파드 생성cat &lt;&lt;EOF | kubectl apply -f ----apiVersion: v1kind: Podmetadata: annotations: # Limits egress bandwidth to 10Mbit/s. kubernetes.io/egress-bandwidth: &quot;10M&quot; labels: # This pod will act as server. app.kubernetes.io/name: netperf-server name: netperf-serverspec: containers: - name: netperf image: cilium/netperf ports: - containerPort: 12865---apiVersion: v1kind: Podmetadata: # This Pod will act as client. name: netperf-clientspec: affinity: # Prevents the client from being scheduled to the # same node as the server. podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - netperf-server topologyKey: kubernetes.io/hostname containers: - name: netperf args: - sleep - infinity image: cilium/netperfEOF# egress BW 제한 정보 확인kubectl describe pod netperf-server | grep Annotations:Annotations: kubernetes.io/egress-bandwidth: 10M# egress BW 제한이 설정된 파드가 있는 cilium pod 에서 제한 정보 확인c1 bpf bandwidth listc2 bpf bandwidth listIDENTITY EGRESS BANDWIDTH (BitsPerSec)904 10Mc1 endpoint listc2 endpoint listENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT904 Disabled Disabled 21565 k8s:app.kubernetes.io/name=netperf-server 172.16.2.153 ready# 트래픽 발생 &gt;&gt; Hubble UI 에서 확인# egress traffic of the netperf-server Pod has been limited to 10Mbit per second. NETPERF_SERVER_IP=$(kubectl get pod netperf-server -o jsonpath='{.status.podIP}')kubectl exec netperf-client -- netperf -t TCP_MAERTS -H &quot;${NETPERF_SERVER_IP}&quot;Recv Send SendSocket Socket Message ElapsedSize Size Size Time Throughputbytes bytes bytes secs. 10^6bits/sec 131072 16384 16384 10.00 9.54 # 10Mbps 제한 확인!# 5M 제한 설정 후 테스트kubectl get pod netperf-server -o json | sed -e 's|10M|5M|g' | kubectl apply -f -c1 bpf bandwidth listc2 bpf bandwidth listkubectl exec netperf-client -- netperf -t TCP_MAERTS -H &quot;${NETPERF_SERVER_IP}&quot;MIGRATED TCP MAERTS TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.84 (172.16.) port 0 AF_INETRecv Send SendSocket Socket Message ElapsedSize Size Size Time Throughputbytes bytes bytes secs. 10^6bits/sec131072 16384 16384 10.04 9.88 # 9.88 Mbps 제한# 10M -&gt; 20M 제한 설정 후 테스트kubectl get pod netperf-server -o json | sed -e 's|10M|20M|g' | kubectl apply -f -kubectl exec netperf-client -- netperf -t TCP_MAERTS -H &quot;${NETPERF_SERVER_IP}&quot;MIGRATED TCP MAERTS TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.84 (172.16.) port 0 AF_INETRecv Send SendSocket Socket Message ElapsedSize Size Size Time Throughputbytes bytes bytes secs. 10^6bits/sec131072 16384 16384 10.01 19.80 # 19.80 Mbps 제한tc qdisc show dev ens5qdisc mq 8002: rootqdisc fq 8005: parent 8002:2 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_dropqdisc fq 8003: parent 8002:4 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_dropqdisc fq 8004: parent 8002:3 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_dropqdisc fq 8006: parent 8002:1 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 18030b initial_quantum 90150b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_drop# 삭제kubectl delete pod netperf-client netperf-server L2 Announcements / L2 Aware LB (Beta) - Link L2 Announcements는 로컬 영역 네트워크에서 서비스를 표시하고 도달 가능하게 만드는 기능입니다. 이 기능은 주로 사무실 또는 캠퍼스 네트워크와 같이 BGP 기반 라우팅이 없는 네트워크 내에서 온프레미스 배포를 위해 고안되었습니다. 이 기능을 사용하면 ExternalIP 및/또는 LoadBalancer IP에 대한 ARP 쿼리에 응답합니다. 이러한 IP는 여러 노드의 가상 IP(네트워크 장치에 설치되지 않음)이므로 각 서비스에 대해 한 번에 한 노드가 ARP 쿼리에 응답하고 MAC 주소로 응답합니다. 이 노드는 서비스 로드 밸런싱 기능으로 로드 밸런싱을 수행하여 북쪽/남쪽 로드 밸런서 역할을 합니다. NodePort 서비스에 비해 이 기능의 장점은 각 서비스가 고유한 IP를 사용할 수 있으므로 여러 서비스가 동일한 포트 번호를 사용할 수 있다는 것입니다. NodePort를 사용할 때 트래픽을 보낼 호스트를 결정하는 것은 클라이언트에게 달려 있으며 노드가 다운되면 IP+Port 콤보를 사용할 수 없게 됩니다. L2 공지를 사용하면 서비스 VIP가 다른 노드로 간단히 마이그레이션되고 계속 작동합니다. 설정 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values \\--set l2announcements.enabled=true --set externalIPs.enabled=true \\--set l2announcements.leaseDuration=3s --set l2announcements.leaseRenewDeadline=1s --set l2announcements.leaseRetryPeriod=200ms #c0 config --all |grep L2EnableL2Announcements : trueEnableL2NeighDiscovery : true# CiliumL2AnnouncementPolicy 생성cat &lt;&lt;EOF | kubectl apply -f - apiVersion: &quot;cilium.io/v2alpha1&quot;kind: CiliumL2AnnouncementPolicymetadata: name: policy1spec: serviceSelector: matchLabels: color: blue nodeSelector: matchExpressions: - key: node-role.kubernetes.io/control-plane operator: DoesNotExist interfaces: - ^ens[0-9]+ externalIPs: true loadBalancerIPs: trueEOF# 확인kubectl get ciliuml2announcementpolicykc describe l2announcement#cat &lt;&lt;EOF | kubectl apply -f - apiVersion: &quot;cilium.io/v2alpha1&quot;kind: CiliumLoadBalancerIPPoolmetadata: name: &quot;cilium-pool&quot;spec: allowFirstLastIPs: &quot;No&quot; blocks: - cidr: &quot;10.10.200.0/29&quot;EOF# cilium ip pool 조회kubectl get CiliumLoadBalancerIPPoolNAME DISABLED CONFLICTING IPS AVAILABLE AGEcilium-pool false False 6 4s 파드, 서비스 생성 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: webpod1 labels: app: webpodspec: nodeName: k8s-w1 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod2 labels: app: webpodspec: nodeName: k8s-w2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Servicemetadata: name: svc1spec: ports: - name: svc1-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer # 서비스 타입이 LoadBalancer---apiVersion: v1kind: Servicemetadata: name: svc2spec: ports: - name: svc2-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer---apiVersion: v1kind: Servicemetadata: name: svc3spec: ports: - name: svc3-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancerEOF 확인 123456789101112131415161718192021222324252627282930#kubectl get svc,epNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.10.0.1 &lt;none&gt; 443/TCP 5h20mservice/svc1 LoadBalancer 10.10.226.228 10.10.200.1 80:32693/TCP 5m30sservice/svc2 LoadBalancer 10.10.166.59 10.10.200.2 80:30107/TCP 5m30sservice/svc3 LoadBalancer 10.10.106.144 10.10.200.3 80:31564/TCP 5m30sNAME ENDPOINTS AGEendpoints/kubernetes 192.168.10.10:6443 5h20mendpoints/svc1 172.16.1.52:80,172.16.2.196:80 5m30sendpoints/svc2 172.16.1.52:80,172.16.2.196:80 5m30sendpoints/svc3 172.16.1.52:80,172.16.2.196:80 5m30s#curl -s 10.10.200.1curl -s 10.10.200.2curl -s 10.10.200.3Hostname: webpod1IP: 127.0.0.1IP: ::1IP: 172.16.1.238IP: fe80::a059:8eff:fe41:9f6aRemoteAddr: 192.168.10.10:51286GET / HTTP/1.1Host: 10.10.200.3User-Agent: curl/7.81.0Accept: */","link":"/blog/2024/10/27/docs/cilium/communication/"},{"title":"network namespace","text":"network namespace 개념에 대해 알아보고 namespace와 veth를 생성해서 1:1 통신 및 가상장치에 대한 테스트를 해보자 이더넷 컴퓨터 네트워킹의 한 방식이다,. Layer 1(물리계층) - 신호/배선 담당 Layer 2(데이터링크) - MAC 프로토콜 담당 MAC address Media Access Control adress Physical Address 16진수 옥텟 6개로 구분되어 있음 ARP (Address Resolution Protocol) ARP 프로토콜은 같은 같은 네트워크 대역에서 통신을 하기 위해 필요한 MAC주소를 IP주소를 이용해서 알아오는 프로토콜이다. 같은 네트워크 대역에서 통신을 한다고 하더라도 데이터를 보내기 위해서는 7계층부터 캡슐화를 통해 데이터를 보내기 때문에 IP주소와 MAC주소가 모두 필요하다. 이 때 IP주소는 알고 MAC주소는 모르더라도 ARP를 통해 통신이 가능하다. 브로드 캐스트 (FF:FF:FF:FF:FF:FF) 라우터는 자신의 MAC을 알려준다. Network namespace 호스트 안에 가상 네트워크를 생성할 수 있다. 네트워크를 isolation 하고 네트워크 stack (OSL 7Layer)을 가상화한다. Isolates network and virtualize network stack ip주소, 라우트 테이블, 소켓, 방화벽 등이 별도 가상화 네트워크에 준비된다. Network interface (랜카드 라고 생각하면 편하다) 가상의 디바이스를 생성하고 마치 랜카드 끼우는 것 처럼 옮겨다니며 장착할 수 있다. Network interface is present in exactly 1 namespace Network interface can ve moved between namespaces Delete Network interface 네트워크 네임스페이스를 삭제하면 포함된 모든 가상장치가 삭제된다. namespace 1:1 통신veth(virtual etcernet) 두 개가 서로 통신을 할 수 있다. 랜카드 두 개를 랜선으로 연결 했다고 생각하면 된다. 통신 준비작업 네트워크 네임스페이스 2개 생성 (red, blue) veth 두개를 pair 로 생성해준다 네임스페이스에 각각의 veth를 링크해준다. 링크 후 veth의 전원을 켜준다. 해당 네임스페이스의 veth에 ip를 할당해준다. dev: “name” # 주소를 정할 장치 명 테스트 내용 리눅스 참고 프로그램nsenter 네임스페이스에 attach하여 지정한 프로그램을 실행한다. COMMAND: 지정하지 않으면 $shell 실행 Option: –net: “net namespace” man nsenter $ nsenter --net=/var/run/netns/red ip addr show (ip addr) 네트워크 장치를 확인 ip neigh show(default show) arp tables management Man ip-neighbour","link":"/blog/2024/08/31/docs/namespace/namespace/"},{"title":"MetalLB-install","text":"실습 환경은 K8S v1.31.0 , CNI(Kindnet, Direct Routing mode) , IPTABLES proxy mode 노드(실제로는 컨테이너) 네트워크 대역 : 172.18.0.0/16 파드 사용 네트워크 대역 : 10.10.0.0/16 ⇒ 각각 10.10.1.0/24, 10.10.2.0/24, 10.10.3.0/24, 10.10.4.0/24 서비스 사용 네트워크 대역 : 10.200.1.0/24 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170cat &lt;&lt;EOT&gt; kind-svc-2w.yamlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4featureGates: &quot;InPlacePodVerticalScaling&quot;: true #실행 중인 파드의 리소스 요청 및 제한을 변경할 수 있게 합니다. &quot;MultiCIDRServiceAllocator&quot;: true #서비스에 대해 여러 CIDR 블록을 사용할 수 있게 합니다.nodes:- role: control-plane labels: mynode: control-plane topology.kubernetes.io/zone: ap-northeast-2a extraPortMappings: #컨테이너 포트를 호스트 포트에 매핑하여 클러스터 외부에서 서비스에 접근할 수 있도록 합니다. - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - containerPort: 30003 hostPort: 30003 - containerPort: 30004 hostPort: 30004 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: #API 서버에 추가 인수를 제공 runtime-config: api/all=true #모든 API 버전을 활성화 controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0- role: worker labels: mynode: worker1 topology.kubernetes.io/zone: ap-northeast-2a- role: worker labels: mynode: worker2 topology.kubernetes.io/zone: ap-northeast-2b- role: worker labels: mynode: worker3 topology.kubernetes.io/zone: ap-northeast-2cnetworking: podSubnet: 10.10.0.0/16 #파드 IP를 위한 CIDR 범위를 정의합니다. 파드는 이 범위에서 IP를 할당받습니다. serviceSubnet: 10.200.1.0/24 #서비스 IP를 위한 CIDR 범위를 정의합니다. 서비스는 이 범위에서 IP를 할당받습니다.EOT# k8s 클러스터 설치kind create cluster --config kind-svc-2w.yaml --name myk8s --image kindest/node:v1.31.0docker ps# 노드에 기본 툴 설치docker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'for i in worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'; echo; done# k8s v1.31.0 버전 확인kubectl get node# 노드 labels 확인kubectl get nodes -o jsonpath=&quot;{.items[*].metadata.labels}&quot; | jq# kind network 중 컨테이너(노드) IP(대역) 확인docker ps -q | xargs docker inspect --format '{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}'# 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용kubectl get cm -n kube-system kubeadm-config -oyaml | grep -i subnetkubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot;# MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/kubectl get servicecidrNAME CIDRS AGEkubernetes 10.200.1.0/24 2m13s# 노드마다 할당된 dedicated subnet (podCIDR) 확인kubectl get nodes -o jsonpath=&quot;{.items[*].spec.podCIDR}&quot;10.10.0.0/24 10.10.4.0/24 10.10.3.0/24 10.10.1.0/24# kube-proxy configmap 확인kubectl describe cm -n kube-system kube-proxy# 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i cat /etc/cni/net.d/10-kindnet.conflist; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c route; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c addr; echo; done# iptables 정보 확인for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done# 각 노드 bash 접속docker exec -it myk8s-control-plane bashdocker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bashdocker exec -it myk8s-worker3 bash----------------------------------------exit----------------------------------------# kind 설치 시 kind 이름의 도커 브리지가 생성된다 : 172.18.0.0/16 대역docker network lsdocker inspect kind# arp scan 해두기docker exec -it myk8s-control-plane arp-scan --interfac=eth0 --localnet# mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용docker run -d --rm --name mypc --network kind --ip 172.18.0.100 nicolaka/netshoot sleep infinity # IP 지정 실행 시IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행docker run -d --rm --name mypc --network kind nicolaka/netshoot sleep infinity # IP 지정 없이 실행 시docker ps# mypc2 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용docker run -d --rm --name mypc2 --network kind --ip 172.18.0.200 nicolaka/netshoot sleep infinity # IP 지정 실행 시IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행docker run -d --rm --name mypc2 --network kind nicolaka/netshoot sleep infinity # IP 지정 없이 실행 시docker ps# kind network 중 컨테이너(노드) IP(대역) 확인docker ps -q | xargs docker inspect --format '{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}'# kube-ops-view 설치helm repo add geek-cookbook https://geek-cookbook.github.io/charts/helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 --set env.TZ=&quot;Asia/Seoul&quot; --namespace kube-system# myk8s-control-plane 배치kubectl -n kube-system edit deploy kube-ops-view---spec: ... template: ... spec: nodeSelector: mynode: control-plane tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Equal&quot; effect: &quot;NoSchedule&quot;---# 설치 확인kubectl -n kube-system get pod -o wide -l app.kubernetes.io/instance=kube-ops-view# kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5&quot;echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2&quot;# kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : Windows 사용자echo -e &quot;KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=1.5&quot;echo -e &quot;KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=2&quot;# kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : AWS_EC2 사용자echo -e &quot;KUBE-OPS-VIEW URL = http://$(curl -s ipinfo.io/ip):30000/#scale=1.5&quot;echo -e &quot;KUBE-OPS-VIEW URL = http://$(curl -s ipinfo.io/ip):30000/#scale=2&quot;","link":"/blog/2024/10/06/docs/loadbalancer/metallb-install/"},{"title":"MetalLB","text":"서비스(Service) LoadBalancer Type 통신 흐름외부 클라이언트가 로드밸런서 접속 시 부하분산 되어 노드 도달 후 iptables 룰로 목적지 파드와 통신됨 외부에서 로드밸런서 (부하분산) 처리 후 → 노드(NodePort) 이후 기본 과정은 NodePort 과정과 동일하다! 노드는 외부에 공개되지 않고 로드밸런서만 외부에 공개되어, 외부 클라이언트는 로드밸랜서에 접속을 할 뿐 내부 노드의 정보를 알 수 없다 로드밸런서가 부하분산하여 파드가 존재하는 노드들에게 전달한다, iptables 룰에서는 자신의 노드에 있는 파드만 연결한다 (externalTrafficPolicy: local) DNAT 2번 동작 : 첫번째(로드밸런서 접속 후 빠져 나갈때), 두번째(노드의 iptables 룰에서 파드IP 전달 시) 외부 클라이언트 IP 보존(유지) : AWS NLB 는 타켓이 인스턴스일 경우 클라이언트 IP를 유지, iptables 룰 경우도 externalTrafficPolicy 로 클라이언트 IP를 보존 쿠버네티스는 Service(LB Type) API 만 정의하고 실제 구현은 add-on 에 맡김 부하분산 최적화 노드에 파드가 없을 경우 ‘로드밸런서’에서 노드에 헬스 체크(상태 검사)가 실패하여 해당 노드로는 외부 요청 트래픽을 전달하지 않는다 서비스(LoadBalancer) 부족한 점 서비스(LoadBalancer) 생성 시 마다 LB(예 AWS NLB)가 생성되어 자원 활용이 비효율적임 ⇒ HTTP 경우 인그레스(Ingress) 를 통해 자원 활용 효율화 가능! 서비스(LoadBalancer)는 HTTP/HTTPS 처리에 일부 부족함(TLS 종료, 도메인 기반 라우팅 등) ⇒ 인그레스(Ingress) 를 통해 기능 동작 가능! (참고) 온프레미스 환경에서 제공 불가능??? ⇒ MetalLB 혹은 OpenELB(구 PorterLB) 를 통해서 온프레미스 환경에서 서비스(LoadBalancer) 기능 동작 가능! MetalLB 요약MetalLB (Layer2 Mode) 리더 파드가 선출되고 해당 리더 파드가 생성된 노드로만 트래픽이 인입되어 해당 노드에서 iptables 분산되어 파드로 접속 MetalLB (BGP Mode) speaker 파드가 BGP로 서비스 정보(EXTERNAL-IP)를 전파 후, 외부에서 라우터를 통해 ECMP 라우팅으로 부하 분산 접속 권장 사용 환경 : 규모가 있고, 클라이언트 IP보존과 장애 시 빠른 절체가 필요하며, 네트워크 팀 협조가 가능할때 MetalLB (BareMetalLoadBalancer) 소개 MetalLB 는 온프레미스 환경(IDC)에서 사용할 수 있는 서비스(로드밸런서 타입)입니다 ← 클라우드 환경의 서비스(로드밸런서 타입)와는 동작이 조금 다릅니다! 서비스(로드 밸런서)의 ‘External IP’ 전파를 위해서 표준 프로토콜인 ARP(IPv4)/NDP(IPv6), BGP 를 사용합니다 데몬셋으로 speaker 파드를 생성하여 ‘External IP’ 전파합니다 일반적인 BGP 데몬과 다르게 BGP 업데이트(외부에서 광고하는 네트워크 대역)을 받지 않습니다. 클라우드 플랫폼 호환 : 대부분의 클라우드 플랫폼(예 AWS, Azure, GCP 등)과 호환되지 않습니다 - 링크 CNI(네트워크 플러그인) 호환 : 일부 CNI와 연동에 이슈가 있습니다 - 링크 예시) Calico IPIP(BGP)와 MetalLB BGP 모드를 상단 라우터와 동시 사용 시 문제 발생 - 링크 Layer 2 모드 (ARP/NDP) - 링크Layer2 모드에서 서비스 접속 모드 소개 : 리더 파드가 선출되고 해당 리더 파드가 생성된 노드로만 트래픽이 인입되어 해당 노드에서 iptables 분산되어 파드로 접속 서비스(로드밸런서) ‘External IP’ 생성 시 speaker 파드 중 1개가 리더가 되고, 리더 speaker 파드가 존재하는 노드로 서비스 접속 트래픽이 인입되게 됩니다 ( 데몬셋으로 배포된 speaker 파드는 호스트 네트워크를 사용합니다 ⇒ “NetworkMode”: “host”) 리더는 ARP(GARP, Gratuitous APR)로 해당 ‘External IP’ 에 대해서 자신이 소유(?)라며 동일 네트워크에 전파를 합니다 만약 리더(노드)가 장애 발생 시 자동으로 나머지 speaker 파드 중 1개가 리더가 됩니다. 멤버 리스터 및 장애 발견은 hashicorp 의 memberlist 를 사용 - Gossip based membership and failure detection Layer 2에서 멤버 발견 및 자동 절체에 Keepalived(VRRP)도 있지만 사용하지 않은 이유 - 링크 BGP 모드 - 링크BGP 모드에서 서비스 접속 모드 소개 : speaker 파드가 BGP로 서비스 정보(EXTERNAL-IP)를 전파 후, 외부에서 라우터를 통해 ECMP 라우팅으로 부하 분산 접속 speaker 파드에 BGP 가 동작하여 서비스 정보(EXTERNAL-IP)를 전파한다 기본은 IP주소(32bit)를 전파하며, 설정으로 축약된 네트워크 정보를 전파할 수 있다 → bgp-advertisements 에 aggregation-length 설정 BGP 커뮤니티, localpref 등 BGP 관련 설정을 할 수 있다 IP 주소 마지막이 0 과 255 를 처리를 못하는 라우터 장비가 있을 경우 avoid-buggy-ips: true 옵션으로 할당되지 않게 할 수 있다 외부 클라이언트에서 SVC(서비스, EXTERNAL-IP)로 접속이 가능하며, 라우터에서 ECMP 라우팅을 통해 부하 분산 접속 할 수 있다 일반적으로 ECMP 는 5-tuple(프로토콜, 출발지IP, 목적지IP, 출발지Port, 목적지Port) 기준으로 동작합니다. 물론 라우터 장비에 따라 다양한 라우팅(분산) 처리가 가능합니다 실습 환경123456789101112131415# 파드와 서비스 사용 가능 네트워크 대역kubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot; &quot;--service-cluster-ip-range=10.200.1.0/24&quot;,&quot;--cluster-cidr=10.10.0.0/16&quot;,# kube-proxy 모드 확인 : iptables proxy 모드kubectl describe configmap -n kube-system kube-proxy | grep modemode: &quot;iptables&quot;# iptables 정보 확인for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done 123456789101112131415161718192021222324252627cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: webpod1 labels: app: webpodspec: nodeName: myk8s-worker containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod2 labels: app: webpodspec: nodeName: myk8s-worker2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0EOF 1234567891011121314kubectl get pod -owide# 파드 IP주소를 변수에 지정WPOD1=$(kubectl get pod webpod1 -o jsonpath=&quot;{.status.podIP}&quot;)WPOD2=$(kubectl get pod webpod2 -o jsonpath=&quot;{.status.podIP}&quot;)echo $WPOD1 $WPOD2# 접속 확인docker exec -it myk8s-control-plane ping -i 1 -W 1 -c 1 $WPOD1docker exec -it myk8s-control-plane ping -i 1 -W 1 -c 1 $WPOD2docker exec -it myk8s-control-plane curl -s --connect-timeout 1 $WPOD1 | grep Hostnamedocker exec -it myk8s-control-plane curl -s --connect-timeout 1 $WPOD2 | grep Hostnamedocker exec -it myk8s-control-plane curl -s --connect-timeout 1 $WPOD1 | egrep 'Hostname|RemoteAddr|Host:'docker exec -it myk8s-control-plane curl -s --connect-timeout 1 $WPOD2 | egrep 'Hostname|RemoteAddr|Host:' MetalLB - Layer 2 모드MetalLB 설치 - 링크 설치 방법 지원 : Kubernetes manifests, using Kustomize, or using Helm 참고 : kube-proxy 의 ipvs 모드 사용 시 ‘strictARP: true’ 설정 필요 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Kubernetes manifests 로 설치kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/refs/heads/main/config/manifests/metallb-native-prometheus.yaml## 혹은 프로메테우스 미설치시 아래 manifests 로 설치kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/refs/heads/main/config/manifests/metallb-native.yaml# metallb crd 확인kubectl get crd | grep metallbbfdprofiles.metallb.io 2024-10-05T16:42:04Zbgpadvertisements.metallb.io 2024-10-05T16:42:04Zbgppeers.metallb.io 2024-10-05T16:42:04Zcommunities.metallb.io 2024-10-05T16:42:04Zipaddresspools.metallb.io 2024-10-05T16:42:04Zl2advertisements.metallb.io 2024-10-05T16:42:04Zservicel2statuses.metallb.io 2024-10-05T16:42:04Z# 생성된 리소스 확인 : metallb-system 네임스페이스 생성, 파드(컨트롤러, 스피커) 생성, RBAC(서비스/파드/컨피그맵 조회 등등 권한들), SA 등kubectl get-all -n metallb-system # kubectl krew 플러그인 get-all 설치 후 사용 가능kubectl get all,configmap,secret,ep -n metallb-system# 파드 내에 kube-rbac-proxy 컨테이너는 프로메테우스 익스포터 역할 제공kubectl get pods -n metallb-system -l app=metallb -o jsonpath=&quot;{range .items[*]}{.metadata.name}{':\\n'}{range .spec.containers[*]}{' '}{.name}{' -&gt; '}{.image}{'\\n'}{end}{end}&quot;## metallb 컨트롤러는 디플로이먼트로 배포됨kubectl get ds,deploy -n metallb-system## 데몬셋으로 배포되는 metallb 스피커 파드의 IP는 네트워크가 host 모드이므로 노드의 IP를 그대로 사용kubectl get pod -n metallb-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScontroller-8b7b6bf6b-92hh8 1/1 Running 0 28s 10.10.2.3 myk8s-worker2 &lt;none&gt; &lt;none&gt;speaker-b9pxr 0/1 Running 0 26s 172.18.0.2 myk8s-worker2 &lt;none&gt; &lt;none&gt;speaker-bw7lz 0/1 Running 0 26s 172.18.0.5 myk8s-control-plane &lt;none&gt; &lt;none&gt;speaker-lwq9p 0/1 Running 0 26s 172.18.0.3 myk8s-worker3 &lt;none&gt; &lt;none&gt;speaker-rtgkq 0/1 Running 0 26s 172.18.0.4 myk8s-worker &lt;none&gt; &lt;none&gt;## metallb 데몬셋으로 배포되는 스피커 파드는 hostNetwork 를 사용함kubectl get ds -n metallb-system -o yaml## 스피커 파드의 Ports 정보 확인 : Host Ports 같이 확인kubectl describe pod -n metallb-system -l component=speaker| grep Ports: Ports: 7472/TCP, 7946/TCP, 7946/UDP Host Ports: 7472/TCP, 7946/TCP, 7946/UDP# (참고) 상세 정보 확인kubectl get sa,cm,secret -n metallb-systemkubectl describe role -n metallb-systemkubectl describe deploy controller -n metallb-systemkubectl describe ds speaker -n metallb-system 컨피그맵 생성 : 모드 및 서비스 대역 지정 123456789101112131415161718192021222324252627282930313233343536373839404142434445# kind 설치 시 kind 이름의 도커 브리지가 생성된다 : 172.18.0.0/16 대역docker network lsdocker inspect kind# kind network 중 컨테이너(노드) IP(대역) 확인 : 172.18.0.2~ 부터 할당되며, control-plane 이 꼭 172.18.0.2가 안될 수 도 있음docker ps -q | xargs docker inspect --format '{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}'# IPAddressPool 생성 : LoadBalancer External IP로 사용할 IP 대역## MetalLB는 서비스를 위한 외부 IP 주소를 관리하고, 서비스가 생성될 때 해당 IP 주소를 동적으로 할당할 수 있습니다.kubectl explain ipaddresspools.metallb.iocat &lt;&lt;EOF | kubectl apply -f -apiVersion: metallb.io/v1beta1kind: IPAddressPoolmetadata: name: my-ippool namespace: metallb-systemspec: addresses: - 172.18.255.200-172.18.255.250EOFkubectl get ipaddresspools -n metallb-systemNAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSESmy-ippool true false [&quot;172.18.255.200-172.18.255.250&quot;]# L2Advertisement 생성 : 설정한 IPpool을 기반으로 Layer2 모드로 LoadBalancer IP 사용 허용## Kubernetes 클러스터 내의 서비스가 외부 네트워크에 IP 주소를 광고하는 방식을 정의kubectl explain l2advertisements.metallb.iocat &lt;&lt;EOF | kubectl apply -f -apiVersion: metallb.io/v1beta1kind: L2Advertisementmetadata: name: my-l2-advertise namespace: metallb-systemspec: ipAddressPools: - my-ippoolEOFkubectl get l2advertisements -n metallb-systemNAME IPADDRESSPOOLS IPADDRESSPOOL SELECTORS INTERFACESmy-l2-advertise [&quot;my-ippool&quot;] log 확인 12345678910# (옵션) metallb-speaker 파드 로그 확인kubectl logs -n metallb-system -l app=metallb -fkubectl logs -n metallb-system -l component=speaker --since 1hkubectl logs -n metallb-system -l component=speaker -f# (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능kubectl stern -n metallb-system -l app=metallbkubectl stern -n metallb-system -l component=speaker --since 1hkubectl stern -n metallb-system -l component=speaker # 기본 설정이 followkubectl stern -n metallb-system speaker # 매칭 사용 가능 서비스 생성 및 확인서비스(LoadBalancer 타입) 생성12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: svc1spec: ports: - name: svc1-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer # 서비스 타입이 LoadBalancer---apiVersion: v1kind: Servicemetadata: name: svc2spec: ports: - name: svc2-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer---apiVersion: v1kind: Servicemetadata: name: svc3spec: ports: - name: svc3-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancerEOF 서비스 확인 및 리더 Speaker 파드 확인 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# arp scan 해두기docker exec -it myk8s-control-plane arp-scan --interfac=eth0 --localnet# LoadBalancer 타입의 서비스 생성 확인 : EXTERNAL-IP가 서비스 마다 할당되며, 실습 환경에 따라 다를 수 있음## LoadBalancer 타입의 서비스는 NodePort 와 ClusterIP 를 포함함 - 'allocateLoadBalancerNodePorts : true' 기본값## ExternalIP 로 접속 시 사용하는 포트는 PORT(S) 의 앞에 있는 값을 사용 (아래의 경우는 TCP 80 임)## 만약 노드의 IP에 NodePort 로 접속 시 사용하는 포트는 PORT(S) 의 뒤에 있는 값을 사용 (아래는 30485 임)kubectl get service,ep# LoadBalancer 타입은 기본적으로 NodePort를 포함 사용. NodePort는 ClusterIP를 포함 사용.## 클라우드사업자 LB Type이나 온프레미스환경 HW LB Type 경우 LB 사용 시 NodePort 미사용 설정 가능kubectl describe svc svc1## 아래 처럼 LB VIP 별로 이던 speaker 배포된 노드가 리더 역할을 하는지 확인 가능kubectl describe svc | grep Events: -A5...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 40m metallb-controller Assigned IP [&quot;172.18.255.201&quot;] Normal nodeAssigned 40m metallb-speaker announcing from node &quot;myk8s-worker&quot; with protocol &quot;layer2&quot;...kubectl get svc svc1 -o json | jq# metallb CRD인 servicel2status 로 상태 정보 확인kubectl explain servicel2statuskubectl get servicel2status -n metallb-systemkubectl describe servicel2status -n metallb-systemkubectl get servicel2status -n metallb-system -o json --watch # watch 모드# 현재 SVC EXTERNAL-IP를 변수에 지정SVC1EXIP=$(kubectl get svc svc1 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')SVC2EXIP=$(kubectl get svc svc2 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')SVC3EXIP=$(kubectl get svc svc3 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')echo $SVC1EXIP $SVC2EXIP $SVC3EXIP# mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾는법 : arping 툴 사용## Unicast reply from 172.18.255.200: 해당 IP 주소에서 응답을 받았음을 의미합니다. ## Sent 1 probes (1 broadcast(s)): 하나의 ARP 요청을 보냈고, 브로드캐스트 방식으로 요청을 전송했음을 나타냅니다.## Received 1 response(s): 하나의 응답을 수신했음을 나타냅니다.docker exec -it mypc arping -I eth0 -f -c 1 $SVC1EXIPdocker exec -it mypc arping -I eth0 -f -c 1 $SVC2EXIPdocker exec -it mypc arping -I eth0 -f -c 1 $SVC3EXIPfor i in $SVC1EXIP $SVC2EXIP $SVC3EXIP; do docker exec -it mypc arping -I eth0 -f -c 1 $i; donedocker exec -it mypc ip -c neighdocker exec -it mypc ping -c 1 -w 1 -W 1 $SVC1EXIPdocker exec -it mypc ping -c 1 -w 1 -W 1 $SVC2EXIPdocker exec -it mypc ping -c 1 -w 1 -W 1 $SVC3EXIPfor i in $SVC1EXIP $SVC2EXIP $SVC3EXIP; do docker exec -it mypc ping -c 1 -w 1 -W 1 $i; donefor i in 172.18.0.2 172.18.0.3 172.18.0.4 172.18.0.5; do docker exec -it mypc ping -c 1 -w 1 -W 1 $i; done# mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!docker exec -it mypc ip -c neigh | sort172.18.0.2 dev eth0 lladdr 02:42:ac:12:00:02 REACHABLE172.18.0.3 dev eth0 lladdr 02:42:ac:12:00:03 REACHABLE172.18.0.4 dev eth0 lladdr 02:42:ac:12:00:04 REACHABLE172.18.0.5 dev eth0 lladdr 02:42:ac:12:00:05 REACHABLE172.18.255.200 dev eth0 lladdr 02:42:ac:12:00:04 REACHABLE172.18.255.201 dev eth0 lladdr 02:42:ac:12:00:04 REACHABLE172.18.255.202 dev eth0 lladdr 02:42:ac:12:00:03 REACHABLEkubectl get node -owide # mac 주소에 매칭되는 IP(노드) 찾기# (옵션) 노드에서 ARP 패킷 캡쳐 확인docker exec -it myk8s-control-plane tcpdump -i eth0 -nn arpdocker exec -it myk8s-worker tcpdump -i eth0 -nn arpdocker exec -it myk8s-worker2 tcpdump -i eth0 -nn arpdocker exec -it myk8s-worker3 tcpdump -i eth0 -nn arp# (옵션) metallb-speaker 파드 로그 확인kubectl logs -n metallb-system -l app=metallb -fkubectl logs -n metallb-system -l component=speaker --since 1hkubectl logs -n metallb-system -l component=speaker -f# (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능kubectl stern -n metallb-system -l app=metallbkubectl stern -n metallb-system -l component=speaker --since 1hkubectl stern -n metallb-system -l component=speaker # 기본 설정이 followkubectl stern -n metallb-system speaker # 매칭 사용 가능 서비스 접속 테스트12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# 현재 SVC EXTERNAL-IP를 변수에 지정SVC1EXIP=$(kubectl get svc svc1 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')SVC2EXIP=$(kubectl get svc svc2 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')SVC3EXIP=$(kubectl get svc svc3 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')echo $SVC1EXIP $SVC2EXIP $SVC3EXIP# 부하분산 접속됨docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $SVC1EXIP | grep Hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $SVC2EXIP | grep Hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $SVC3EXIP | grep Hostname; done | sort | uniq -c | sort -nr&quot;# 지속적으로 반복 접속docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1EXIP | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC2EXIP | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC3EXIP | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;# 컨트롤플레인에서 확인 : 너무 복잡해서 리턴 트래픽에 대해서는 상세히 분석 정리하지 않습니다.docker exec -it myk8s-worker bash----------------------------------------# iptables 확인iptables -t filter -Siptables -t nat -Siptables -t nat -S | wc -liptables -t mangle -S# iptables 상세 확인 - 매칭 패킷 카운트, 인터페이스 정보 등 포함iptables -nvL -t filteriptables -nvL -t natiptables -nvL -t mangle# rule 갯수 확인iptables -nvL -t filter | wc -liptables -nvL -t nat | wc -l# 규칙 패킷 바이트 카운트 초기화iptables -t filter --zero; iptables -t nat --zero; iptables -t mangle --zero# 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!iptables -t nat -nvLiptables -t nat -SPREROUTING# SVC1(External-IP) 접속 시 iptables 에서 DNAT 되어 파드로 전달된다 (SNAT 도 동작)iptables -t nat -S PREROUTING-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICESiptables -t nat -S KUBE-SERVICESSVC1EXIP=&lt;직접입력&gt;SVC1EXIP=172.18.255.202iptables -t nat -S KUBE-SERVICES |grep $SVC1EXIP-A KUBE-SERVICES -d 172.18.255.202/32 -p tcp -m comment --comment &quot;default/svc1:svc1-webport loadbalancer IP&quot; -m tcp --dport 80 -j KUBE-EXT-DLGPAL4ZCYSJ7UPRiptables -t nat -S KUBE-EXT-DLGPAL4ZCYSJ7UPR-A KUBE-EXT-DLGPAL4ZCYSJ7UPR -m comment --comment &quot;masquerade traffic for default/svc1:svc1-webport external destinations&quot; -j KUBE-MARK-MASQ-A KUBE-EXT-DLGPAL4ZCYSJ7UPR -j KUBE-SVC-DLGPAL4ZCYSJ7UPRiptables -t nat -S KUBE-MARK-MASQ-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000iptables -t nat -S KUBE-SVC-DLGPAL4ZCYSJ7UPR-A KUBE-SVC-DLGPAL4ZCYSJ7UPR ! -s 10.10.0.0/16 -d 10.200.1.82/32 -p tcp -m comment --comment &quot;default/svc1:svc1-webport cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ-A KUBE-SVC-DLGPAL4ZCYSJ7UPR -m comment --comment &quot;default/svc1:svc1-webport -&gt; 10.10.1.6:80&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-USUIKZUHVJLOFB4D-A KUBE-SVC-DLGPAL4ZCYSJ7UPR -m comment --comment &quot;default/svc1:svc1-webport -&gt; 10.10.3.6:80&quot; -j KUBE-SEP-SXKPWGGXEDMXXU4H#iptables -t nat -S KUBE-SEP-USUIKZUHVJLOFB4D-A KUBE-SEP-USUIKZUHVJLOFB4D -s 10.10.1.6/32 -m comment --comment &quot;default/svc1:svc1-webport&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-USUIKZUHVJLOFB4D -p tcp -m comment --comment &quot;default/svc1:svc1-webport&quot; -m tcp -j DNAT --to-destination 10.10.1.6:80iptables -t nat -S KUBE-SEP-SXKPWGGXEDMXXU4H-A KUBE-SEP-SXKPWGGXEDMXXU4H -s 10.10.3.6/32 -m comment --comment &quot;default/svc1:svc1-webport&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-SXKPWGGXEDMXXU4H -p tcp -m comment --comment &quot;default/svc1:svc1-webport&quot; -m tcp -j DNAT --to-destination 10.10.3.6:80POSTROUTINGiptables -t nat -S POSTROUTINGiptables -t nat -S KUBE-POSTROUTING-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE --random-fully# SVC2(External-IP) 접속 시 iptables 에서 DNAT 되어 파드로 전달된다 (SNAT 도 동작) : 위와 상동!# SVC3(External-IP) 접속 시 iptables 에서 DNAT 되어 파드로 전달된다 (SNAT 도 동작) : 위와 상동! Failover 테스트 리더 Speaker 파드가 존재하는 노드를 재부팅! curl 접속 테스트 시 10~20초 정도의 장애 시간이 발생하였다 ⇒ 이후 자동 원복 되며, 원복 시 5초 정도 장애 시간 발생! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 사전 준비## 지속적으로 반복 접속SVC1EXIP=$(kubectl get svc svc1 -o jsonpath='{.status.loadBalancer.ingress[0].ip}')docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1EXIP | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;## 상태 모니터링watch -d kubectl get pod,svc,ep## 실시간 로그 확인kubectl logs -n metallb-system -l app=metallb -f혹은kubectl stern -n metallb-system -l app=metallb# 장애 재연## 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)를 중지docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 9docker stop myk8s-worker --signal 9혹은docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 15docker stop myk8s-worker --signal 15docker ps -adocker ps -a | grep worker$## 지속적으로 반복 접속 상태 모니터링### curl 연속 접속 시도 &gt;&gt; 대략 10초 이내에 정상 접근 되었지만, 20초까지는 불안정하게 접속이 되었다### 실제로는 다른 노드의 speaker 파드가 리더가 되고, 이후 다시 노드(컨테이너)가 정상화되면, 다시 리더 speaker 가 됨docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1EXIP | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;Hostname: webpod2RemoteAddr: 10.10.2.1:550322024-10-05 17:21:17Hostname: webpod2RemoteAddr: 10.10.2.1:609892024-10-05 17:21:182024-10-05 17:21:202024-10-05 17:21:22Hostname: webpod2RemoteAddr: 10.10.2.1:136212024-10-05 17:21:23Hostname: webpod2RemoteAddr: 10.10.2.1:563202024-10-05 17:21:24# 변경된 리더 Speaker 파드 확인# mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기for i in $SVC1EXIP $SVC2EXIP $SVC3EXIP; do docker exec -it mypc ping -c 1 -w 1 -W 1 $i; done# mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!docker exec -it mypc ip -c neigh | sortkubectl get node -owide # mac 주소에 매칭되는 IP(노드) 찾기# 장애 원복(노드 정상화)## 노드(실제 컨테이너) 정상화 docker start &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt;docker start myk8s-worker# 변경된 리더 Speaker 파드 확인# mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기for i in $SVC1EXIP $SVC2EXIP $SVC3EXIP; do docker exec -it mypc ping -c 1 -w 1 -W 1 $i; done# mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!docker exec -it mypc ip -c neigh | sortkubectl get node -owide # mac 주소에 매칭되는 IP(노드) 찾기","link":"/blog/2024/12/22/docs/loadbalancer/metallb/"},{"title":"Container Layer Stack","text":"컨테이너 설치 시 이미지가 어떤 방식, 구조로 저장되는지 알아보자 도커 컨테이너 , 이미지 삭제 12\\# docker rm $(docker ps -a -q)\\# docker rmi -f $(docker images -q) docker pull nginx:latest 123456789101112root@seongtki:/# docker pull nginx:latestlatest: Pulling from library/nginxe886f0f47ef5: Pull complete # 배포id (distribution id)9a9138853e32: Pull complete598a42ec6587: Pull complete82e490cc2043: Pull complete948128637a91: Pull completee4cad15ac3f6: Pull complete096332b242c2: Pull completeDigest: sha256:32da30332506740a2f7c34d5dc70467b7f14ec67d912703568daff790ab3f755Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest image id 확인 123root@seongtki:/# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 2a4fbb36e966 9 days ago 192MB layer id 확인 (image id 조회) 12345678910111213root@seongtki:/# docker image inspect 2a4fbb36e966 | jq '.[].RootFS'{ &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73&quot;, # layer id &quot;sha256:674ca5344f959090515e9eac79055d8630851b21bbd584ac32765315284c05c5&quot;, &quot;sha256:7f8881cae5296e2ef0103706a326235494b5f1dcd9ea960656165cf7643ddd47&quot;, &quot;sha256:f21128d548a5122f430a58f123deb9c8b80f70b9fc98df280a9619b7e38b6a32&quot;, &quot;sha256:dca0e6f80e8df5d85d45426101fefba3d41abe4fc90be994074066d4b135bef5&quot;, &quot;sha256:d9664e6da0f787fb8c04e80f1e731223f8ef93f6e097e934442e1792da614dff&quot;, &quot;sha256:ed44ff7a11e0afd3e5b58c730514faeac7d6a12faff3f52aa33d33e0369e7aad&quot; ]} 도커는 다운로드한 이미지를 로컬에 저장하고 관리하기 위해 **”레이어 DB”**를 사용한다. 12345678910111213root@seongtki:/var/lib/docker/image/overlay2# tree -L 2.├── distribution│ ├── diffid-by-digest│ └── v2metadata-by-diffid├── imagedb│ ├── content│ └── metadata├── layerdb # 레이어 스택 정보│ ├── mounts│ ├── sha256│ └── tmp└── repositories.json 레이어디비 항목 내용 db id 로컬에서 레이어식별을 위한 레이어디비 id이미지를 pull 받을 때 레이어 별로 생성됨. (받을 때마다 달라짐) cache id 레이어가 저장된 로컬경로 id (경로를 식별)- /var/lib/docker/overlay2/{cache-id} diff layer id (레이어를 식별) parent 부모레이어를 가리키는 포인터 (db id) 어떤 레이어 위로 올라갈지 지정 db id1234567891011121314151617181920212223242526272829root@seongtki:/var/lib/docker/image/overlay2/layerdb# tree -L 2 sha256 -I &quot;tar-*|size&quot;sha256├── 311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73 # db id│ ├── cache-id│ └── diff # layer id├── 807d78e4cdc589c2cdefb7bb6049a31bcc7bf0f90f335a8a8de1abeb0e6a3993│ ├── cache-id│ ├── diff│ └── parent├── 89b7e61557f3502dc01aeae499b451574376c3e4c46e49294c72e22cd2d9634c│ ├── cache-id│ ├── diff│ └── parent├── 9f55692205d904037fbc69a1d59354665af3c0c2a767b5e799e719170085bc11│ ├── cache-id│ ├── diff│ └── parent├── a0e6c2974935e56e8d57d6fea041399b290da1566d82ea1a7d9c3ec7ef60a366│ ├── cache-id│ ├── diff│ └── parent├── a216b497301a9fc6283077c0906140e22b8f3a67ba92ab6253f036f48e1ff4b1│ ├── cache-id│ ├── diff│ └── parent└── a3169fdf28b9329e3f1882c095bf11e12a465419c759169682ca706de81bef84 ├── cache-id ├── diff └── parent tree -L 2 sha256 -I “tar-*|size” 에서 -I 는 제외항목을 작성 cache-id : db-id cache-id 리스트 포맷: “cache-id” ./sha256/“db id” 12345678root@seongtki:/var/lib/docker/image/overlay2/layerdb# find . -name cache-id -exec cat {} \\; -print52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d./sha256/311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73/cache-idbd58ec4c3b456591049ce5ad929fac42a96ad23fc28836a29917039d07295ed4./sha256/a0e6c2974935e56e8d57d6fea041399b290da1566d82ea1a7d9c3ec7ef60a366/cache-id5ebc10176fffee11727b0baa5f552146a74a4fa53935268e2c10726f08f61aa7./sha256/89b7e61557f3502dc01aeae499b451574376c3e4c46e49294c72e22cd2d9634c/cache-id20fae55020e28561082510b431ec0f85a8bb3a1cebb34f36916b91acc3833c84./sha256/807d78e4cdc589c2cdefb7bb6049a31bcc7bf0f90f335a8a8de1abeb0e6a3993/cache-idf3453bed47cc62a3d58d00327b6a76fbf15612220a7b1b4825936296fd0b845f./sha256/9f55692205d904037fbc69a1d59354665af3c0c2a767b5e799e719170085bc11/cache-id64c2e72761ef2dbf88db1e27bf590f901f5d0e7b3f778635ee976a2f834478ba./sha256/a216b497301a9fc6283077c0906140e22b8f3a67ba92ab6253f036f48e1ff4b1/cache-id067ec145f89b61cd0b6fd41745163c76e04f86db5b4a59a5358cc82a3099526a./sha256/a3169fdf28b9329e3f1882c095bf11e12a465419c759169682ca706de81bef84/cache-id 레이어의 로컬 저장 경로(/var/lib/docker/overlay2/{cache-id}) 12345678910root@seongtki:/var/lib/docker/overlay2# ls -ltotal 32drwx--x--- 4 root root 4096 Sep 30 12:31 067ec145f89b61cd0b6fd41745163c76e04f86db5b4a59a5358cc82a3099526adrwx--x--- 4 root root 4096 Sep 30 12:31 20fae55020e28561082510b431ec0f85a8bb3a1cebb34f36916b91acc3833c84drwx--x--- 3 root root 4096 Sep 30 12:31 52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5ddrwx--x--- 4 root root 4096 Sep 30 12:31 5ebc10176fffee11727b0baa5f552146a74a4fa53935268e2c10726f08f61aa7drwx--x--- 4 root root 4096 Sep 30 12:31 64c2e72761ef2dbf88db1e27bf590f901f5d0e7b3f778635ee976a2f834478badrwx--x--- 4 root root 4096 Sep 30 12:31 bd58ec4c3b456591049ce5ad929fac42a96ad23fc28836a29917039d07295ed4drwx--x--- 4 root root 4096 Sep 30 12:31 f3453bed47cc62a3d58d00327b6a76fbf15612220a7b1b4825936296fd0b845fdrwx------ 2 root root 4096 Sep 30 12:31 l layer id : db id 앞에가 layer id, 뒤에가 db id 이다. 12345678root@seongtki:/var/lib/docker/image/overlay2/layerdb# find . -name diff -exec cat {} \\; -printsha256:311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73./sha256/311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73/diffsha256:674ca5344f959090515e9eac79055d8630851b21bbd584ac32765315284c05c5./sha256/a0e6c2974935e56e8d57d6fea041399b290da1566d82ea1a7d9c3ec7ef60a366/diffsha256:d9664e6da0f787fb8c04e80f1e731223f8ef93f6e097e934442e1792da614dff./sha256/89b7e61557f3502dc01aeae499b451574376c3e4c46e49294c72e22cd2d9634c/diffsha256:ed44ff7a11e0afd3e5b58c730514faeac7d6a12faff3f52aa33d33e0369e7aad./sha256/807d78e4cdc589c2cdefb7bb6049a31bcc7bf0f90f335a8a8de1abeb0e6a3993/diffsha256:7f8881cae5296e2ef0103706a326235494b5f1dcd9ea960656165cf7643ddd47./sha256/9f55692205d904037fbc69a1d59354665af3c0c2a767b5e799e719170085bc11/diffsha256:dca0e6f80e8df5d85d45426101fefba3d41abe4fc90be994074066d4b135bef5./sha256/a216b497301a9fc6283077c0906140e22b8f3a67ba92ab6253f036f48e1ff4b1/diffsha256:f21128d548a5122f430a58f123deb9c8b80f70b9fc98df280a9619b7e38b6a32./sha256/a3169fdf28b9329e3f1882c095bf11e12a465419c759169682ca706de81bef84/diff parent id : db id parent 파일은 부모레이어의 db id를 갖고 있음 (가장 밑바닥은 parent 없음) sha256:”db-id”./sha256/“parent-id” 311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73 가 최상위 부모 db -id 이다. 1234567root@seongtki:/var/lib/docker/image/overlay2/layerdb# find . -name parent -exec cat {} \\; -printsha256:311627f8702d2d97e63b916cad711aa9f962a1f20da5572d68b8f223ff051e73./sha256/a0e6c2974935e56e8d57d6fea041399b290da1566d82ea1a7d9c3ec7ef60a366/parentsha256:a216b497301a9fc6283077c0906140e22b8f3a67ba92ab6253f036f48e1ff4b1./sha256/89b7e61557f3502dc01aeae499b451574376c3e4c46e49294c72e22cd2d9634c/parentsha256:89b7e61557f3502dc01aeae499b451574376c3e4c46e49294c72e22cd2d9634c./sha256/807d78e4cdc589c2cdefb7bb6049a31bcc7bf0f90f335a8a8de1abeb0e6a3993/parentsha256:a0e6c2974935e56e8d57d6fea041399b290da1566d82ea1a7d9c3ec7ef60a366./sha256/9f55692205d904037fbc69a1d59354665af3c0c2a767b5e799e719170085bc11/parentsha256:a3169fdf28b9329e3f1882c095bf11e12a465419c759169682ca706de81bef84./sha256/a216b497301a9fc6283077c0906140e22b8f3a67ba92ab6253f036f48e1ff4b1/parentsha256:9f55692205d904037fbc69a1d59354665af3c0c2a767b5e799e719170085bc11./sha256/a3169fdf28b9329e3f1882c095bf11e12a465419c759169682ca706de81bef84/parent 레이어 스택 layer id : 레이어 식별 db id : 레이어관리 cache-id : 레이어 저장 실제저장 위치 cache id 폴더이름 12345678910root@seongtki:/var/lib/docker/overlay2# tree -L 1.├── 067ec145f89b61cd0b6fd41745163c76e04f86db5b4a59a5358cc82a3099526a├── 20fae55020e28561082510b431ec0f85a8bb3a1cebb34f36916b91acc3833c84├── 52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d├── 5ebc10176fffee11727b0baa5f552146a74a4fa53935268e2c10726f08f61aa7├── 64c2e72761ef2dbf88db1e27bf590f901f5d0e7b3f778635ee976a2f834478ba├── bd58ec4c3b456591049ce5ad929fac42a96ad23fc28836a29917039d07295ed4├── f3453bed47cc62a3d58d00327b6a76fbf15612220a7b1b4825936296fd0b845f└── l l 디렉터리에는 레이어 저장소(cache-id) 심볼릭 링크가 존재","link":"/blog/2024/12/29/docs/overlayFS/container-layer-stack/"},{"title":"Overlay Filesystem","text":"여러 filesystem을 하나로 mount하는 기능. 두 파일시스템에 동일한 파일이 있는경우 나중에 마운트 되는 파일시스템의 파일을 오버레이한다. 하위 파일시스템에 대한 쓰기 작업 시 그 시점에 Copy-on-write 실시 () 복사본을 생성하여 수행 (원본유지)) 이령, 상속 파일시스템으로 불린다. OverlayFS2 Merged Dir: 통합 뷰 (mount point) Upper Dir: Writable. 컨테이너에서의 변경된 내용이 쓰이는 레이어 Lower Dir: Read only, 기존 이미지 영역 Work Dir: “atomic action”을 보장하기 위해 merge에 반영되기 전에 파일을 준비하는 데 사용된다. overlay test1234567mkdir overlay-testcd overlay-testmkdir image1 image2 container work mergetouch image1/a image1/b image2/c# merge: mount pointmount -t overlay overlay -o lowerdir=image2:image1,upperdir=container,workdir=work merge 12root@seongtki:/# mount | grep overlay-test/overlay on /overlay-test/merge type overlay (rw,relatime,lowerdir=image2:image1,upperdir=container,workdir=work) 파일구조 12345678910111213root@seongtki:/overlay-test# tree -I .├── container├── image1│ ├── a│ └── b├── image2│ └── c├── merge│ ├── a│ ├── b│ └── c└── work └── work merge/a 삭제 후 디렉토리 1234567891011121314root@seongtki:/overlay-test# rm merge/aroot@seongtki:/overlay-test# tree -I .├── container│ └── a # 삭제파일이 upperdir 에 적용되어 보인다 (RW)├── image1│ ├── a # lowerdir는 변경이 단됨 (RO)│ └── b├── image2│ └── c├── merge│ ├── b│ └── c└── work └── work lowerdir 에 파일 수정 시 merge에도 적용되어 보인다. 123456root@seongtki:/overlay-test# echo 'hello bb' &gt; image1/broot@seongtki:/overlay-test# cat image1/bhello bbbroot@seongtki:/overlay-test# cat merge/bhello bbb merge파일을 수정하면, lowerdir은 더이상 변경되지 않는다. Copy-On-Write 가 발생하기 때문이다. (실제 파일이 사용될 때 카피를 하고 수정하기 때문.) 12345root@seongtki:/overlay-test# echo 'hello m' &gt; merge/broot@seongtki:/overlay-test# cat merge/bhello mroot@seongtki:/overlay-test# cat image1/bhello bbb 이후 lowerdir파일을 변경해도 merge파일을 변경되지 않는다. 123456root@seongtki:/overlay-test# echo 'hello bbbb' &gt; image1/broot@seongtki:/overlay-test# cat image1/bhello bbbbroot@seongtki:/overlay-test# cat merge/bhello m 123456789101112131415root@seongtki:/overlay-test# tree -I ..├── container│ ├── a│ └── b├── image1│ ├── a│ └── b├── image2│ └── c├── merge│ ├── b│ └── c└── work └── work","link":"/blog/2024/12/29/docs/overlayFS/overlayFS/"},{"title":"container layer","text":"컨테이너 레이어구조 (overlayFS)를 알아보고, 장점에 대해 알아보자 docker info (overlay2) Storage Driver: overlay2 Docker Root Dir: /var/lib/docker 123456789root@seongtki:/# docker info... Storage Driver: overlay2... Cgroup Driver: cgroupfs... Docker Root Dir: /var/lib/docker...WARNING: No swap limit support 123root@seongtki:/# docker info | grep StorageWARNING: No swap limit support Storage Driver: overlay2 도커 이미지 레이어 구조 오버레이 파일시스템과 같음 Image layers == Lower Dir R/O (Read Only, 읽기 전용)Container layer == Upper Dir R/W (Read-Write) - 컨테이너 실행 시 image layer 위에 올라감 레이어 구조는 중복도 최소화하고 동일한 레이어를 사용할 경우 공유할 수 있어 저장공간을 절약한다. 대신 컨테이너에서 Image layer의 파일을 변경해야 할 경우에는 CoW (Copy-On-Write)로 동작한다. 이 때, 변경할 파일을 Container layer(Thin R/W layer)로 복사해 와서 처리해야 하므로, 일반적인 write와 비교해서 속도도 느리고 오버헤드가 발생한다. (따라서 Image layer의 파일 변경은 되도록 피해야 한다) 도커 이미지삭제12\\# docker rm $(docker ps -a -q)\\# docker rmi -f $(docker images -q) 레이어 구조 도커 이미지의 레이어 구조는 “GraphDriver”에 나타나 있다. LowerDir 옵션은 콜론(:) 을 기준으로 여러개 레이어를 설정할 수 있다. 맨 뒤가 베이스레이어 이다. 베이스레이어 cache-id 로 들어가면 이미지 디렉토리가 보임 도커는 내부적으로 Debian 12 slim 12root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d/diff/etc# cat issueDebian GNU/Linux 12 \\n \\l debian:12-slim 는 nginx 이미지레이어에 이미 존재하므로 새로 받지 않는다. 레이어는 받지 않지만, 이미지 정보에는 추가됨 (docker images) 123456root@seongtki:~# docker pull debian:12-slim12-slim: Pulling from library/debiane886f0f47ef5: Already exists # 해당 distribution id 는 이미 있다고 받지 않음Digest: sha256:24c92a69df28b21676d721fe18c0bf64138bfc69b486746ad935b49cc31b0b91Status: Downloaded newer image for debian:12-slimdocker.io/library/debian:12-slim 두 개 이미지의 레이어를 조회했을 때, 데비안은 nginx 의 레이어 중 하나와 공유한다. 이미지레이어 공유 확인 lowerDir의 실제 저장소에서 파일을 저장해본다 ** lowerDir는 RO인데, 이렇게 host권한을 이용하는건 어쩔수 없다. 컨테이너 실행중에 실제로 이런식으로 사용하면 안됨 1root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d/diff# echo &quot;hello worled&quot; &gt; world debian:12-slim 컨테이너실행 (두개실행한다) lowerDir에서 변경한 데이터 확인 가능 123root@seongtki:~# docker run -it debian:12-slimroot@c267289b8c2b:/# cat worldhello worled 컨테이너 두개 구조를 살펴보면, LowerDir 은 Debian의 cache-id 에 있던 id와 같음을 알 수 있다. 컨테이너의 overlay 구조를 mount에서 파악 가능 위 명령어결과에서 lowerDir 는 심볼릭링크 되어있는데, 따라가보면 실제 이미지저장장소가 나옴 컨테이너 데이터 변경컨테이너의 쓰기는 UpperDir에서 이루어진다. 컨테이너1에서 hello파일을 변경하면, 컨테이너1의 UpperDir에서 COW 가 이루어져 쓰기가 발생하고, 변경내용을 읽을 수 있다. 하지만 컨테이너2에서 world 파일을 변경한 내용은 읽을 수 없다. 1234567root@c267289b8c2b:/# cat worldhello worledroot@c267289b8c2b:/# echo &quot;container mod&quot; &gt; helloroot@c267289b8c2b:/# cat hellocontainer modroot@c267289b8c2b:/# cat worldhello worled 컨테이너2도 마찬가지. 1234567root@d082030bfc8f:/# cat hellohelloroot@d082030bfc8f:/# echo &quot;container2 mod&quot; &gt; worldroot@d082030bfc8f:/# cat hellohelloroot@d082030bfc8f:/# cat worldcontainer2 mod lowerDir 에서도 컨테이너가 변경한 내용을 볼 수 없다. 단지, lowerDir에서 다시 hello 파일 내용을 변경했을때, UpperDir 가존재하는 컨테이너1에서는 변경내용을 못읽지만 UpperDir가 없는 container2는 읽을 수 있다. 1234root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d/diff# cat hellohelloroot@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d/diff# cat worldhello worled overlayFS 와 같은 개념이라고 보면 된다. history 로 이미지 distribution id 확인 가능. 123456789101112131415161718root@seongtki:/var/lib/docker/overlay2# docker history nginxIMAGE CREATED CREATED BY SIZE COMMENT2a4fbb36e966 9 days ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon… 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) STOPSIGNAL SIGQUIT 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) EXPOSE 80 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) ENTRYPOINT [&quot;/docker-entr… 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) COPY file:9e3b2b63db9f8fc7… 4.62kB&lt;missing&gt; 9 days ago /bin/sh -c #(nop) COPY file:57846632accc8975… 3.02kB&lt;missing&gt; 9 days ago /bin/sh -c #(nop) COPY file:3b1b9915b7dd898a… 298B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) COPY file:caec368f5a54f70a… 2.12kB&lt;missing&gt; 9 days ago /bin/sh -c #(nop) COPY file:01e75c6dd0ce317d… 1.62kB&lt;missing&gt; 9 days ago /bin/sh -c set -x &amp;&amp; groupadd --system -… 94.9MB&lt;missing&gt; 9 days ago /bin/sh -c #(nop) ENV PKG_RELEASE=1~bookworm 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) ENV NJS_VERSION=0.8.0 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.25.2 0B&lt;missing&gt; 9 days ago /bin/sh -c #(nop) LABEL maintainer=NGINX Do… 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) CMD [&quot;bash&quot;] 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ADD file:ec1a6e0aedd76c8fd… 97.1M 12345678910root@seongtki:~# docker run --nginx=nginx --rm -it 2a4fbb36e966unknown flag: --nginxSee 'docker run --help'.root@seongtki:~# docker run --nginx=nginx --rm -it 2a4fbb36e966 /bin/bashunknown flag: --nginxSee 'docker run --help'.root@seongtki:~# docker run --name=nginx --rm -it 2a4fbb36e966 /bin/bashroot@0d2ed11b96f0:/# rm /bin/Display all 281 possibilities? (y or n)^Croot@0d2ed11b96f0:/# rm /bin/cat 1234root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d# docker container diff nginxC /usrC /usr/binD /usr/bin/cat 12345678910root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d# docker container commit nginx nginx:rm_catsha256:6f8e2630eee109051d1878733b1d064b5e5158af1eae73f5a5a44a8b6c032d76root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d# docker imsagesdocker: 'imsages' is not a docker command.See 'docker --help'root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx rm_cat 6f8e2630eee1 9 seconds ago 192MBnginx latest 2a4fbb36e966 10 days ago 192MBdebian 12-slim 217435774160 10 days ago 97.1MB 12345678910111213141516171819root@seongtki:/var/lib/docker/overlay2/52943c6506870bd23a1bec337a8fcfc018fd0b6581be161ddc4d3bb0363b8b5d# docker history nginx:rm_catIMAGE CREATED CREATED BY SIZE COMMENT6f8e2630eee1 29 seconds ago /bin/bash 0B2a4fbb36e966 10 days ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon… 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) STOPSIGNAL SIGQUIT 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) EXPOSE 80 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ENTRYPOINT [&quot;/docker-entr… 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) COPY file:9e3b2b63db9f8fc7… 4.62kB&lt;missing&gt; 10 days ago /bin/sh -c #(nop) COPY file:57846632accc8975… 3.02kB&lt;missing&gt; 10 days ago /bin/sh -c #(nop) COPY file:3b1b9915b7dd898a… 298B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) COPY file:caec368f5a54f70a… 2.12kB&lt;missing&gt; 10 days ago /bin/sh -c #(nop) COPY file:01e75c6dd0ce317d… 1.62kB&lt;missing&gt; 10 days ago /bin/sh -c set -x &amp;&amp; groupadd --system -… 94.9MB&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ENV PKG_RELEASE=1~bookworm 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ENV NJS_VERSION=0.8.0 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.25.2 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) LABEL maintainer=NGINX Do… 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) CMD [&quot;bash&quot;] 0B&lt;missing&gt; 10 days ago /bin/sh -c #(nop) ADD file:ec1a6e0aedd76c8fd… 97.1M 기존 이미지에 레이어 추가 컨테이너 실행후, 파일변경 123456root@seongtki:~# docker run --name=nginx --rm -it 2a4fbb36e966 /bin/bashroot@db6d1638925b:/# rm /bin/tatabs tac tail tar tasksetroot@db6d1638925b:/# rm /bin/tatabs tac tail tar tasksetroot@db6d1638925b:/# rm /bin/tar 호스트에서 파일 변경 확인하고, 새로운 이미지로 커밋한다. 12docker container diff nginxdocker container commit nginx nginx:rm_tar 새로운 레이어를 확인한다. 1docker image inspect nginx:rm_tar | jq '.[].RootFS' 마지막에 layerid 하나가 추가되었다. layerid 에대한 db-id 확인후, cacheid 를 확인해서 실제 레이어 저장소로 이동한다. 1find /var/lib/docker/image/overlay2/layerdb -name &quot;diff&quot; -exec cat {} \\; -print writeout 정보가 있음을 확인.","link":"/blog/2024/12/29/docs/overlayFS/container-layer/"},{"title":"ClusterIP","text":"통신 흐름클라이언트(TestPod)가 ‘CLUSTER-IP’ 접속 시 해당 노드의 iptables 룰(랜덤 분산)에 의해서 DNAT 처리가 되어 목적지(backend) 파드와 통신 클러스터 내부에서만 ‘CLUSTER-IP’ 로 접근 가능 ⇒ 서비스에 DNS(도메인) 접속도 가능! 서비스(ClusterIP 타입) 생성하게 되면, apiserver → (kubelet) → kube-proxy → iptables 에 rule(룰)이 생성됨 모든 노드(마스터 포함)에 iptables rule 이 설정되므로, 파드에서 접속 시 해당 노드에 존재하는 iptables rule 에 의해서 분산 접속이 됨 실습 구성 목적지(backend) 파드(Pod) 생성 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOT&gt; 3pod.yamlapiVersion: v1kind: Podmetadata: name: webpod1 labels: app: webpodspec: nodeName: myk8s-worker containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod2 labels: app: webpodspec: nodeName: myk8s-worker2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0---apiVersion: v1kind: Podmetadata: name: webpod3 labels: app: webpodspec: nodeName: myk8s-worker3 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0EOT 클라이언트(TestPod) 생성 1234567891011121314cat &lt;&lt;EOT&gt; netpod.yamlapiVersion: v1kind: Podmetadata: name: net-podspec: nodeName: myk8s-control-plane containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0EOT 서비스(ClusterIP) 생성 1234567891011121314cat &lt;&lt;EOT&gt; svc-clusterip.yamlapiVersion: v1kind: Servicemetadata: name: svc-clusteripspec: ports: - name: svc-webport port: 9000 # 서비스 IP 에 접속 시 사용하는 포트 port 를 의미 targetPort: 80 # 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미 selector: app: webpod # 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨 type: ClusterIP # 서비스 타입EOT 생성 확인 12345678910➜ service git:(master) ✗ kubectl apply -f 3pod.yaml,netpod.yaml,svc-clusterip.yamlpod/webpod1 createdpod/webpod2 createdpod/webpod3 createdpod/net-pod createdservice/svc-clusterip createdkubectl get endpoints svc-clusteripNAME ENDPOINTS AGEsvc-clusterip 10.10.1.4:80,10.10.2.4:80,10.10.3.3:80 51m 서비스(ClusterIP) 접속 확인클라이언트(TestPod) Shell 에서 접속 테스트 &amp; 서비스(ClusterIP) 부하분산 접속 확인 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# webpod 파드의 IP 를 출력kubectl get pod -l app=webpod -o jsonpath=&quot;{.items[*].status.podIP}&quot;# webpod 파드의 IP를 변수에 지정WEBPOD1=$(kubectl get pod webpod1 -o jsonpath={.status.podIP})WEBPOD2=$(kubectl get pod webpod2 -o jsonpath={.status.podIP})WEBPOD3=$(kubectl get pod webpod3 -o jsonpath={.status.podIP})echo $WEBPOD1 $WEBPOD2 $WEBPOD3# net-pod 파드에서 webpod 파드의 IP로 직접 curl 로 반복 접속for pod in $WEBPOD1 $WEBPOD2 $WEBPOD3; do kubectl exec -it net-pod -- curl -s $pod; donefor pod in $WEBPOD1 $WEBPOD2 $WEBPOD3; do kubectl exec -it net-pod -- curl -s $pod | grep Hostname; donefor pod in $WEBPOD1 $WEBPOD2 $WEBPOD3; do kubectl exec -it net-pod -- curl -s $pod | grep Host; donefor pod in $WEBPOD1 $WEBPOD2 $WEBPOD3; do kubectl exec -it net-pod -- curl -s $pod | egrep 'Host|RemoteAddr'; done# 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소SVC1=$(kubectl get svc svc-clusterip -o jsonpath={.spec.clusterIP})echo $SVC1# 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨 docker exec -it myk8s-control-plane iptables -t nat -S | grep $SVC1for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i iptables -t nat -S | grep $SVC1; echo; done-A KUBE-SERVICES -d 10.200.1.52/32 -p tcp -m comment --comment &quot;default/svc-clusterip:svc-webport cluster IP&quot; -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF## (참고) ss 툴로 tcp listen 정보에는 없음 , 별도 /32 host 라우팅 추가 없음 -&gt; 즉, iptables rule 에 의해서 처리됨을 확인docker exec -it myk8s-control-plane ss -tnlpdocker exec -it myk8s-control-plane ip -c route# TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인kubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1kubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000kubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000 | grep Hostnamekubectl exec -it net-pod -- curl -s --connect-timeout 1 $SVC1:9000 | grep Hostname# 서비스(ClusterIP) 부하분산 접속 확인kubectl exec -it net-pod -- zsh -c &quot;for i in {1..10}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..1000}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;혹은kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; sleep 1; done&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; sleep 0.1; done&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..10000}; do curl -s $SVC1:9000 | grep Hostname; sleep 0.01; done&quot;# conntrack 확인docker exec -it myk8s-control-plane bash----------------------------------------conntrack -hconntrack -Econntrack -Cconntrack -Sconntrack -L --src 10.10.0.6 # net-pod IPconntrack -L --dst $SVC1 # service ClusterIPexit----------------------------------------# (참고) Link layer 에서 동작하는 ebtablesebtables -L 반복해서 실행을 해보면, SVC1 IP로 curl 접속 시 3개의 파드로 대략 33% 정도로 부하분산 접속됨을 확인 1234➜ service git:(master) ✗ kubectl exec -it net-pod -- zsh -c &quot;for i in {1..1000}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot; 340 Hostname: webpod1 330 Hostname: webpod3 330 Hostname: webpod 각 워커노드에서 패킷 덤프 확인1234567891011121314151617181920212223242526272829303132333435# 방안1 : 1대 혹은 3대 bash 진입 후 tcpdump 해둘 것docker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bashdocker exec -it myk8s-worker3 bash----------------------------------# nic 정보 확인ip -c linkip -c routeip -c addr# tcpdump/ngrep : eth0 &gt;&gt; tcp 9000 포트 트래픽은 왜 없을까? iptables rule 동작 그림을 한번 더 확인하고 이해해보자## ngrep 네트워크 패킷 분석기 활용해보기 : 특정 url 호출에 대해서만 필터 등 깔끔하게 볼 수 있음 - 링크tcpdump -i eth0 tcp port 80 -nnqtcpdump -i eth0 tcp port 80 -w /root/svc1-1.pcaptcpdump -i eth0 tcp port 9000 -nnqngrep -tW byline -d eth0 '' 'tcp port 80'# tcpdump/ngrep : vethXVETH1=&lt;각자 자신의 veth 이름&gt;tcpdump -i $VETH1 tcp port 80 -nntcpdump -i $VETH1 tcp port 80 -w /root/svc1-2.pcaptcpdump -i $VETH1 tcp port 9000 -nnngrep -tW byline -d $VETH1 '' 'tcp port 80'exit----------------------------------# 방안2 : 노드(?) 컨테이너 bash 직접 접속하지 않고 호스트에서 tcpdump 하기docker exec -it myk8s-worker tcpdump -i eth0 tcp port 80 -nnqVETH1=&lt;각자 자신의 veth 이름&gt; # docker exec -it myk8s-worker ip -c routedocker exec -it myk8s-worker tcpdump -i $VETH1 tcp port 80 -nnq# 호스트PC에 pcap 파일 복사 &gt;&gt; wireshark 에서 분석docker cp myk8s-worker:/root/svc1-1.pcap .docker cp myk8s-worker:/root/svc1-2.pcap . IPTABLES 정책 확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# 컨트롤플레인에서 확인 : 너무 복잡해서 리턴 트래픽에 대해서는 상세히 분석 정리하지 않습니다.docker exec -it myk8s-control-plane bash----------------------------------------# iptables 확인iptables -t filter -Siptables -t nat -Siptables -t nat -S | wc -liptables -t mangle -S# iptables 상세 확인 - 매칭 패킷 카운트, 인터페이스 정보 등 포함iptables -nvL -t filteriptables -nvL -t natiptables -nvL -t mangle# rule 갯수 확인iptables -nvL -t filter | wc -liptables -nvL -t nat | wc -l# 규칙 패킷 바이트 카운트 초기화iptables -t filter --zero; iptables -t nat --zero; iptables -t mangle --zero# 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!iptables -t nat -nvLiptables -v --numeric --table nat --list PREROUTING | column -tChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 778 46758 KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */iptables -v --numeric --table nat --list KUBE-SERVICES | column# 바로 아래 룰(rule)에 의해서 서비스(ClusterIP)를 인지하고 처리를 합니다Chain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 92 5520 KUBE-SVC-KBDEBIL6IU6WL7RF tcp -- * * 0.0.0.0/0 10.105.114.73 /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF | columnwatch -d 'iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF'SVC1=$(kubectl get svc svc-clusterip -o jsonpath={.spec.clusterIP})kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; sleep 1; done&quot;# SVC-### 에서 랜덤 확률(대략 33%)로 SEP(Service EndPoint)인 각각 파드 IP로 DNAT 됩니다!## 첫번째 룰에 일치 확률은 33% 이고, 매칭되지 않을 경우 아래 2개 남을때는 룰 일치 확률은 50%가 됩니다. 이것도 매칭되지 않으면 마지막 룰로 100% 일치됩니다Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references) pkts bytes target prot opt in out source destination 38 2280 KUBE-SEP-6TM74ZFOWZXXYQW6 all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */ statistic mode random probability 0.33333333349 29 1740 KUBE-SEP-354QUAZJTL5AR6RR all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */ statistic mode random probability 0.50000000000 25 1500 KUBE-SEP-PY4VJNJPBUZ3ATEL all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */iptables -v --numeric --table nat --list KUBE-SEP-&lt;각자 값 입력&gt;Chain KUBE-SEP-6TM74ZFOWZXXYQW6 (1 references) pkts bytes target prot opt in out source destination 38 2280 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */ tcp to:172.16.158.3:80iptables -v --numeric --table nat --list KUBE-SEP-354QUAZJTL5AR6RR | column -tChain KUBE-SEP-6TM74ZFOWZXXYQW6 (1 references) pkts bytes target prot opt in out source destination 29 1500 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */ tcp to:172.16.184.3:80iptables -v --numeric --table nat --list KUBE-SEP-PY4VJNJPBUZ3ATEL | column -tChain KUBE-SEP-6TM74ZFOWZXXYQW6 (1 references) pkts bytes target prot opt in out source destination 25 1740 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/svc-clusterip:svc-webport */ tcp to:172.16.34.3:80iptables -t nat --zeroiptables -v --numeric --table nat --list POSTROUTING | column; echo ; iptables -v --numeric --table nat --list KUBE-POSTROUTING | columnwatch -d 'iptables -v --numeric --table nat --list POSTROUTING; echo ; iptables -v --numeric --table nat --list KUBE-POSTROUTING'# POSTROUTE(nat) : 0x4000(2진수로 0100 0000 0000 0000, 10진수 16384) 마킹 되어 있지 않으니 RETURN 되고 그냥 빠져나가서 SNAT 되지 않는다!Chain KUBE-POSTROUTING (1 references) pkts bytes target prot opt in out source destination 572 35232 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 mark match ! 0x4000/0x4000 0 0 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK xor 0x4000 0 0 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ random-fullyiptables -t nat -S | grep KUBE-POSTROUTING-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE --random-fully...exit----------------------------------------# 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨을 한번 더 확이docker exec -it myk8s-control-plane iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF...for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF; echo; done... 파드 1개 장애 발생 시 동작 확인모니터링 12345678# 터미널1 &gt;&gt; ENDPOINTS 변화를 잘 확인해보자!watch -d 'kubectl get pod -owide;echo; kubectl get svc,ep svc-clusterip;echo; kubectl get endpointslices -l kubernetes.io/service-name=svc-clusterip'# 터미널2SVC1=$(kubectl get svc svc-clusterip -o jsonpath={.spec.clusterIP})kubectl exec -it net-pod -- zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1:9000 | egrep 'Hostname|IP: 10'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;혹은kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot; 파드 하나 삭제 후 확인 12345678910111213141516# (방안1) 파드3번 삭제 &gt;&gt; 서비스의 엔드포인트가 어떻게 변경되는지 확인 하자!, 지속적인 curl 접속 결과 확인!, for 문 실행 시 결과 확인!, 절체 시간(순단) 확인!kubectl delete pod webpod3# (방안1) 결과 확인 후 다시 파드 3번 생성 &gt;&gt; 서비스 디스커버리!kubectl apply -f 3pod.yaml---------------------------------# (방안2) 파드3번에 레이블 삭제kubectl get pod --show-labels## 레이블(라벨)의 키값 바로 뒤에 하이픈(-) 입력 시 해당 레이블 삭제됨! &gt;&gt; 레이블과 셀렉터는 쿠버네티스 환경에서 매우 많이 활용된다!kubectl label pod webpod3 app-kubectl get pod --show-labels# (방안2) 결과 확인 후 파드3번에 다시 레이블 생성kubectl label pod webpod3 app=webpod k8s 클러스터 외부(mypc)에서는 서비스(ClusterIP)로 접속이 불가능 1234docker psdocker exec -it mypc ping -c 1 172.18.0.2docker exec -it mypc curl &lt;SVC1_IP&gt;:9000docker exec -it mypc curl &lt;Pod IP&gt;:80 sessionAffinity: ClientIPsessionAffinity: ClientIP : 클라이언트가 접속한 목적지(파드)에 고정적인 접속을 지원 - k8s_Docs 만약 클라이언트의 요청을 매번 동일한 목적지 파드로 전달하기 위해서 세션어피니티가 필요하다. 클라이언트가 서비스를 통해 최초 전달된 파드에 대한 연결상태를 기록해두고, 이후 동일한 클라이언트가 서비스에 접속 시 연결상태 정보를 확인하여 최초 전달된 파드, 즉 동일한 혹은 마치 고정적인 파드로 전달할 수 있다,. 123456789101112131415161718192021222324# 기본 정보 확인kubectl get svc svc-clusterip -o yamlkubectl get svc svc-clusterip -o yaml | grep sessionAffinity# 반복 접속kubectl exec -it net-pod -- zsh -c &quot;while true; do curl -s --connect-timeout 1 $SVC1:9000 | egrep 'Hostname|IP: 10|Remote'; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;# sessionAffinity: ClientIP 설정 변경kubectl patch svc svc-clusterip -p '{&quot;spec&quot;:{&quot;sessionAffinity&quot;:&quot;ClientIP&quot;}}'혹은kubectl get svc svc-clusterip -o yaml | sed -e &quot;s/sessionAffinity: None/sessionAffinity: ClientIP/&quot; | kubectl apply -f -#kubectl get svc svc-clusterip -o yaml... sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800...# 클라이언트(TestPod) Shell 실행kubectl exec -it net-pod -- zsh -c &quot;for i in {1..100}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot;kubectl exec -it net-pod -- zsh -c &quot;for i in {1..1000}; do curl -s $SVC1:9000 | grep Hostname; done | sort | uniq -c | sort -nr&quot; 서비스(ClusterIP) 부족한 점 클러스터 외부에서는 서비스(ClusterIP)로 접속이 불가능 ⇒ NodePort 타입으로 외부에서 접속 가능! IPtables 는 파드에 대한 헬스체크 기능이 없어서 문제 있는 파드에 연결 가능 ⇒ 서비스 사용, 파드에 Readiness Probe 설정으로 파드 문제 시 서비스의 엔드포인트에서 제거되게 하자! ← 이 정도면 충분한가? 혹시 부족한 점이 없을까? 서비스에 연동된 파드 갯수 퍼센트(%)로 랜덤 분산 방식, 세션어피니티 이외에 다른 분산 방식 불가능 ⇒ IPVS 경우 다양한 분산 방식(알고리즘) 가능 목적지 파드 다수가 있는 환경에서, 출발지 파드와 목적지 파드가 동일한 노드에 배치되어 있어도, 랜덤 분산으로 다른 노드에 목적지 파드로 연결 가능","link":"/blog/2024/09/29/docs/service/clusterip/"},{"title":"ingress concept","text":"Ingress 요약Nginx 인그레스 컨트롤러 : 외부에서 인그레스로 접속 시 Nginx 인그레스 컨트롤러 파드로 인입되고, 이후 애플리케이션 파드의 IP로 직접 통신 클러스터 내부를 외부에 노출 - 발전 단계 파드 생성 : K8S 클러스터 내부에서만 접속 서비스(Cluster Type) 연결 : K8S 클러스터 내부에서만 접속 동일한 애플리케이션의 다수의 파드의 접속을 용이하게 하기 위한 서비스에 접속 서비스(NodePort Type) 연결 : 외부 클라이언트가 서비스를 통해서 클러스터 내부의 파드로 접속 서비스(NodePort Type)의 일부 단점을 보완한 서비스(LoadBalancer Type) 도 있습니다! 인그레스 컨트롤러 파드를 배치 : 서비스 앞단에 HTTP 고급 라우팅 등 기능 동작을 위한 배치 인그레스(정책)이 적용된 인그레스 컨트롤러 파드(예. nginx pod)를 앞단에 배치하여 고급 라우팅 등 기능을 제공 인그레스 컨트롤러 파드 이중화 구성 : Active(Leader) - Standby(Follower) 로 Active 파드 장애에 대비 인그레스 컨트롤러 파드를 외부에 노출 : 인그레스 컨트롤러 파드를 외부에서 접속하기 위해서 노출(expose) 인그레스 컨트롤러 노출 시 서비스(NodePort Type) 보다는 좀 더 많은 기능을 제공하는 서비스(LoadBalancer Type)를 권장합니다 (80/443 포트 오픈 시) 인그레스와 파드간 내부 연결의 효율화 방안 : 인그레스 컨트롤러 파드(Layer7 동작)에서 서비스 파드의 IP로 직접 연결 인그레스 컨트롤러 파드는 K8S API서버로부터 서비스의 엔드포인트 정보(파드 IP)를 획득 후 바로 파드의 IP로 연결 - 링크 지원되는 인그레스 컨트롤러 : Nginx, Traefix 등 현재 대부분의 인그레스 컨트롤러가 지원함 인그레스(Ingress) 소개인그레스 소개 : 클러스터 내부의 서비스(ClusterIP, NodePort, Loadbalancer)를 외부로 노출(HTTP/HTTPS) - Web Proxy 역할 인그레스 기능 : HTTP(서비스) 부하분산 , 카나리 업그레이드 인그레스 컨트롤러 : 인그레스의 실제 동작 구현은 인그레스 컨트롤러(Nginx, Kong 등)가 담당 인그레스 + 인그레스 컨트롤러(Nginx) 기능 : HTTP(서비스) 부하분산 , 카나리 업그레이드 , HTTPS 처리(TLS 종료) Nginx 인그레스 컨트롤러 설치123456789101112131415161718192021222324252627282930313233343536373839404142# Ingress-Nginx 컨트롤러 생성cat &lt;&lt;EOT&gt; ingress-nginx-values.yamlcontroller: service: type: NodePort nodePorts: http: 30080 https: 30443 nodeSelector: kubernetes.io/hostname: &quot;k3s-s&quot; metrics: enabled: true serviceMonitor: enabled: trueEOThelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm repo updatekubectl create ns ingresshelm install ingress-nginx ingress-nginx/ingress-nginx -f ingress-nginx-values.yaml --namespace ingress --version 4.11.2# 확인kubectl get all -n ingresskc describe svc -n ingress ingress-nginx-controller# externalTrafficPolicy 설정kubectl patch svc -n ingress ingress-nginx-controller -p '{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;: &quot;Local&quot;}}'# 기본 nginx conf 파일 확인kc describe cm -n ingress ingress-nginx-controllerkubectl exec deploy/ingress-nginx-controller -n ingress -it -- cat /etc/nginx/nginx.conf# 관련된 정보 확인 : 포드(Nginx 서버), 서비스, 디플로이먼트, 리플리카셋, 컨피그맵, 롤, 클러스터롤, 서비스 어카운트 등kubectl get all,sa,cm,secret,roles -n ingresskc describe clusterroles ingress-nginxkubectl get pod,svc,ep -n ingress -o wide -l app.kubernetes.io/component=controller# 버전 정보 확인POD_NAMESPACE=ingressPOD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o name)kubectl exec $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version 인그레스(Ingress) 실습 및 통신 흐름 확인 컨트롤플레인 노드에 인그레스 컨트롤러(Nginx) 파드를 생성, NodePort 로 외부에 노출 인그레스 정책 설정 : Host/Path routing, 실습의 편리를 위해서 도메인 없이 IP로 접속 설정 가능 디플로이먼트와 서비스를 생성svc1-pod.yaml 123456789101112131415161718192021222324252627282930apiVersion: apps/v1kind: Deploymentmetadata: name: deploy1-websrvspec: replicas: 1 selector: matchLabels: app: websrv template: metadata: labels: app: websrv spec: containers: - name: pod-web image: nginx---apiVersion: v1kind: Servicemetadata: name: svc1-webspec: ports: - name: web-port port: 9001 targetPort: 80 selector: app: websrv type: ClusterIP svc2-pod.yaml 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: deploy2-guestsrvspec: replicas: 2 selector: matchLabels: app: guestsrv template: metadata: labels: app: guestsrv spec: containers: - name: pod-guest image: gcr.io/google-samples/kubernetes-bootcamp:v1 ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: svc2-guestspec: ports: - name: guest-port port: 9002 targetPort: 8080 selector: app: guestsrv type: NodePort svc3-pod.yaml 12345678910111213141516171819202122232425262728293031apiVersion: apps/v1kind: Deploymentmetadata: name: deploy3-adminsrvspec: replicas: 3 selector: matchLabels: app: adminsrv template: metadata: labels: app: adminsrv spec: containers: - name: pod-admin image: k8s.gcr.io/echoserver:1.5 ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: svc3-adminspec: ports: - name: admin-port port: 9003 targetPort: 8080 selector: app: adminsrv 생성 123456# 생성kubectl taint nodes k3s-s role=controlplane:NoSchedulekubectl apply -f svc1-pod.yaml,svc2-pod.yaml,svc3-pod.yaml# 확인 : svc1, svc3 은 ClusterIP 로 클러스터 외부에서는 접속할 수 없다 &gt;&gt; Ingress 는 연결 가능!kubectl get pod,svc,ep 인그레스(정책) 생성 - 링크 ingress1 12345678910111213141516171819202122232425262728293031323334cat &lt;&lt;EOT&gt; ingress1.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-1 annotations: #nginx.ingress.kubernetes.io/upstream-hash-by: &quot;true&quot;spec: ingressClassName: nginx rules: - http: paths: - path: / pathType: Prefix backend: service: name: svc1-web port: number: 80 - path: /guest pathType: Prefix backend: service: name: svc2-guest port: number: 8080 - path: /admin pathType: Prefix backend: service: name: svc3-admin port: number: 8080EOT 12345678910111213141516171819202122# 모니터링watch -d 'kubectl get ingress,svc,ep,pod -owide'# 생성kubectl apply -f ingress1.yaml# 확인kubectl get ingresskc describe ingress ingress-1...Rules: Host Path Backends ---- ---- -------- * / svc1-web:80 () /guest svc2-guest:8080 () /admin svc3-admin:8080 ()...# 설정이 반영된 nginx conf 파일 확인kubectl exec deploy/ingress-nginx-controller -n ingress -it -- cat /etc/nginx/nginx.conf 인그레스를 통한 내부 접속 Nginx 인그레스 컨트롤러를 통한 접속(HTTP 인입) 경로 : 인그레스 컨트롤러 파드에서 서비스 파드의 IP로 직접 연결 인그레스(Nginx 인그레스 컨트롤러)를 통한 접속(HTTP 인입) 확인 HTTP 부하분산 &amp; PATH 기반 라우팅, 애플리케이션 파드에 연결된 서비스는 Bypass 1234567891011121314151617181920212223242526272829303132333435363738394041#kubectl get ingressNAME CLASS HOSTS ADDRESS PORTS AGEingress-1 nginx * 10.10.200.24 80 3m44s kubectl describe ingress ingress-1 | sed -n &quot;5, \\$p&quot;Rules: Host Path Backends ---- ---- -------- * / svc1-web:80 () /guest svc2-guest:8080 () /admin svc3-admin:8080 ()# 접속 로그 확인 : kubetail 설치되어 있음 - 출력되는 nginx 의 로그의 IP 확인kubetail -n ingress -l app.kubernetes.io/component=controller-------------------------------# 자신의 집 PC에서 인그레스를 통한 접속 : 각각 echo -e &quot;Ingress1 sv1-web URL = http://$(curl -s ipinfo.io/ip):30080&quot;echo -e &quot;Ingress1 sv2-guest URL = http://$(curl -s ipinfo.io/ip):30080/guest&quot;echo -e &quot;Ingress1 sv3-admin URL = http://$(curl -s ipinfo.io/ip):30080/admin&quot;# svc1-web 접속MYIP=&lt;EC2 공인 IP&gt;MYIP=13.124.93.150curl -s $MYIP:30080# svvc2-guest 접속curl -s $MYIP:30080/guestcurl -s $MYIP:30080/guestfor i in {1..100}; do curl -s $MYIP:30080/guest ; done | sort | uniq -c | sort -nr# svc3-admin 접속 &gt; 기본적으로 Nginx 는 라운드로빈 부하분산 알고리즘을 사용 &gt;&gt; Client_address 와 XFF 주소는 어떤 주소인가요?curl -s $MYIP:30080/admincurl -s $MYIP:30080/admin | egrep '(client_address|x-forwarded-for)'for i in {1..100}; do curl -s $MYIP:30080/admin | grep Hostname ; done | sort | uniq -c | sort -nr# (옵션) 디플로이먼트의 파드 갯수를 증가/감소 설정 후 접속 테스트 해보자 노드에서 아래 패킷 캡처 확인 : flannel vxlan, 파드 간 통신 시 IP 정보 확인 12345678910111213#ngrep -tW byline -d ens5 '' udp port 8472 or tcp port 80#tcpdump -i ens5 udp port 8472 -nn# vethY는 각자 k3s-s 의 가장 마지막 veth 를 지정tcpdump -i vethY tcp port 8080 -nntcpdump -i vethY tcp port 8080 -w /tmp/ingress-nginx.pcap# 자신의 PC에서 k3s-s EC2 공인 IP로 pcap 다운로드scp ubuntu@&lt;k3s-s EC2 공인 IP&gt;:/tmp/ingress-nginx.pcap ~/Downloadsscp ubuntu@52.78.155.19:/tmp/ingress-nginx.pcap ~/Downloads Nginx 분산 알고리즘 변경 nginx 는 기본 RR 라운드 로빈 이지만, IP-Hash 나 Session Cookie 설정으로 대상 유지 가능 - 링크 12annotations: nginx.ingress.kubernetes.io/upstream-hash-by: &quot;true&quot; 확인 12345678910111213141516171819# mypcfor i in {1..100}; do curl -s $MYIP:30080/admin | grep Hostname ; done | sort | uniq -c | sort -nrwhile true; do curl -s --connect-timeout 1 $MYIP:30080/admin | grep Hostname ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; echo &quot;--------------&quot; ; sleep 1; done# 아래 ingress 설정 중 IP-Hash 설정 &gt; # 주석 제거sed -i 's/#nginx.ingress/nginx.ingress/g' ingress1.yamlkubectl apply -f ingress1.yaml# 접속 확인for i in {1..100}; do curl -s $MYIP:30080/admin | grep Hostname ; done | sort | uniq -c | sort -nrwhile true; do curl -s --connect-timeout 1 $MYIP:30080/admin | grep Hostname ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; echo &quot;--------------&quot; ; sleep 1; done# 다시 원복(라운드 로빈) &gt; # 주석 추가sed -i 's/nginx.ingress/#nginx.ingress/g' ingress1.yamlkubectl apply -f ingress1.yaml# 접속 확인for i in {1..100}; do curl -s $MYIP:30080/admin | grep Hostname ; done | sort | uniq -c | sort -nrwhile true; do curl -s --connect-timeout 1 $MYIP:30080/admin | grep Hostname ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; echo &quot;--------------&quot; ; sleep 1; done 삭제 1kubectl delete deployments,svc,ingress --all AWS Ingress (ALB) 모드 인스턴스 모드 : AWS ALB(Ingress)로 인입 후 각 워커노드의 NodePort 로 전달 후 IPtables 룰(SEP)에 따라 파드로 분배 IP 모드 : nginx ingress controller 동작과 유사하게 AWS LoadBalancer Controller 파드가 kube api 를 통해서 파드의 IP를 제공받아서 AWS ALB 에 타켓(파드 IP)를 설정 Host 기반 라우팅 1234567891011121314151617181920212223242526272829cat &lt;&lt;EOT&gt; ingress2.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-2spec: ingressClassName: nginx rules: - host: kans.com http: paths: - path: / pathType: Prefix backend: service: name: svc3-admin port: number: 8080 - host: &quot;*.kans.com&quot; http: paths: - path: /echo pathType: Prefix backend: service: name: svc3-admin port: number: 8080EOT 생성 1234567891011121314151617181920212223242526# 터미널1watch -d 'kubectl get ingresses,svc,ep,pod -owide'# 도메인 변경MYDOMAIN1=staek.comsed -i &quot;s/kans.com/$MYDOMAIN1/g&quot; ingress2.yaml# 생성kubectl apply -f ingress2.yaml,svc3-pod.yaml# 확인kubectl get ingresskubectl describe ingress ingress-2kubectl describe ingress ingress-2 | sed -n &quot;5, \\$p&quot;Ingress Class: nginxDefault backend: &lt;default&gt;Rules: Host Path Backends ---- ---- -------- staek.com / svc3-admin:8080 () *.staek.com /echo svc3-admin:8080 ()Annotations: &lt;none&gt;Events: &lt;none&gt; 인그레스(Nginx 인그레스 컨트롤러)를 통한 접속(HTTP 인입) 확인123456789101112131415161718192021222324252627282930313233343536373839# 로그 모니터링kubetail -n ingress -l app.kubernetes.io/component=controller# (옵션) ingress nginx 파드 vethY 에서 패킷 캡처 후 확인 해 볼 것------------# 자신의 PC 에서 접속 테스트# svc3-admin 접속 &gt; 결과 확인 : 왜 접속이 되지 않는가? HTTP 헤더에 Host 필드를 잘 확인해보자!curl $MYIP:30080 -vcurl $MYIP:30080/echo -v# mypc에서 접속을 위한 설정## /etc/hosts 수정 : 도메인 이름으로 접속하기 위해서 변수 지정## 윈도우 C:\\Windows\\System32\\drivers\\etc\\hosts## 맥 sudo vim /etc/hostsMYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt;MYDOMAIN2=&lt;test.각자 자신의 닉네임의 도메인&gt;MYDOMAIN1=staek.comMYDOMAIN2=test.staek.comecho $MYIP $MYDOMAIN1 $MYDOMAIN2echo &quot;$MYIP $MYDOMAIN1&quot; | sudo tee -a /etc/hostsecho &quot;$MYIP $MYDOMAIN2&quot; | sudo tee -a /etc/hostscat /etc/hosts | grep $MYDOMAIN1# svc3-admin 접속 &gt; 결과 확인curl $MYDOMAIN1:30080 -vcurl $MYDOMAIN1:30080/admincurl $MYDOMAIN1:30080/echocurl $MYDOMAIN1:30080/echo/1curl $MYDOMAIN2:30080 -vcurl $MYDOMAIN2:30080/admincurl $MYDOMAIN2:30080/echocurl $MYDOMAIN2:30080/echo/1curl $MYDOMAIN2:30080/echo/1/2## (옵션) /etc/hosts 파일 변경 없이 접속 방안curl -H &quot;host: $MYDOMAIN1&quot; $MYIP:30080 카나리 업그레이드canary-svc1-pod.yaml 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: dp-v1spec: replicas: 3 selector: matchLabels: app: svc-v1 template: metadata: labels: app: svc-v1 spec: containers: - name: pod-v1 image: k8s.gcr.io/echoserver:1.5 ports: - containerPort: 8080 terminationGracePeriodSeconds: 0---apiVersion: v1kind: Servicemetadata: name: svc-v1spec: ports: - name: web-port port: 9001 targetPort: 8080 selector: app: svc-v1 canary-svc2-pod.yaml 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: dp-v2spec: replicas: 3 selector: matchLabels: app: svc-v2 template: metadata: labels: app: svc-v2 spec: containers: - name: pod-v2 image: k8s.gcr.io/echoserver:1.6 ports: - containerPort: 8080 terminationGracePeriodSeconds: 0---apiVersion: v1kind: Servicemetadata: name: svc-v2spec: ports: - name: web-port port: 9001 targetPort: 8080 selector: app: svc-v2 1234567891011121314151617181920212223242526272829303132# 터미널1watch -d 'kubectl get ingress,svc,ep,pod -owide'# 생성curl -s -O https://raw.githubusercontent.com/gasida/NDKS/main/7/canary-svc1-pod.yamlcurl -s -O https://raw.githubusercontent.com/gasida/NDKS/main/7/canary-svc2-pod.yamlkubectl apply -f canary-svc1-pod.yaml,canary-svc2-pod.yaml# 확인kubectl get svc,ep,pod# 파드 버전 확인: 1.13.0 vs 1.13.1for pod in $(kubectl get pod -o wide -l app=svc-v1 |awk 'NR&gt;1 {print $6}'); do curl -s $pod:8080 | egrep '(Hostname|nginx)'; doneHostname: dp-v1-8684d45558-9kb59 server_version=nginx: 1.13.0 - lua: 10008Hostname: dp-v1-8684d45558-drwzf server_version=nginx: 1.13.0 - lua: 10008Hostname: dp-v1-8684d45558-hzcth server_version=nginx: 1.13.0 - lua: 10008 for pod in $(kubectl get pod -o wide -l app=svc-v2 |awk 'NR&gt;1 {print $6}'); do curl -s $pod:8080 | egrep '(Hostname|nginx)'; doneHostname: dp-v2-7757c4bdc-b8bkt server_version=nginx: 1.13.1 - lua: 10008Hostname: dp-v2-7757c4bdc-ggx8l server_version=nginx: 1.13.1 - lua: 10008Hostname: dp-v2-7757c4bdc-zfxfc server_version=nginx: 1.13.1 - lua: 10008 canary-ingress1.yaml 12345678910111213141516171819cat &lt;&lt;EOT&gt; canary-ingress1.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-canary-v1spec: ingressClassName: nginx rules: - host: kans.com http: paths: - path: / pathType: Prefix backend: service: name: svc-v1 port: number: 8080EOT canary-ingress2.yaml 12345678910111213141516171819202122cat &lt;&lt;EOT&gt; canary-ingress2.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-canary-v2 annotations: nginx.ingress.kubernetes.io/canary: &quot;true&quot; nginx.ingress.kubernetes.io/canary-weight: &quot;10&quot;spec: ingressClassName: nginx rules: - host: kans.com http: paths: - path: / pathType: Prefix backend: service: name: svc-v2 port: number: 8080EOT 12345678910111213141516171819202122232425262728293031323334353637# 터미널1watch -d 'kubectl get ingress,svc,ep'# 도메인 변경MYDOMAIN1=staek.comsed -i &quot;s/kans.com/$MYDOMAIN1/g&quot; canary-ingress1.yamlsed -i &quot;s/kans.com/$MYDOMAIN1/g&quot; canary-ingress2.yaml# 생성kubectl apply -f canary-ingress1.yaml,canary-ingress2.yaml# 로그 모니터링kubetail -n ingress -l app.kubernetes.io/component=controller# 접속 테스트curl -s $MYDOMAIN1:30080curl -s $MYDOMAIN1:30080 | grep nginx# 접속 시 v1 v2 버전별 비율이 어떻게 되나요? 왜 이렇게 되나요?for i in {1..100}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nrfor i in {1..1000}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nrwhile true; do curl -s --connect-timeout 1 $MYDOMAIN1:30080 | grep Hostname ; echo &quot;--------------&quot; ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; sleep 1; done# 비율 조정 &gt;&gt; 개발 배포 버전 전략에 유용하다!kubectl annotate --overwrite ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight=50# 접속 테스트for i in {1..100}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nrfor i in {1..1000}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nr# (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?kubectl annotate --overwrite ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight=100for i in {1..100}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nr# (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?kubectl annotate --overwrite ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight=0for i in {1..100}; do curl -s $MYDOMAIN1:30080 | grep nginx ; done | sort | uniq -c | sort -nr 아래와 같이 기본 비율과 canary-weight=50 비율 차이를 보자. 1kubectl annotate --overwrite ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight=50 삭제 1kubectl delete deployments,svc,ingress --all HTTPS 처리 (TLS 종료) - 링크 svc-pod.yaml 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: pod-https labels: app: httpsspec: containers: - name: container image: k8s.gcr.io/echoserver:1.6 terminationGracePeriodSeconds: 0---apiVersion: v1kind: Servicemetadata: name: svc-httpsspec: selector: app: https ports: - port: 8080 ssl-termination-ingress.yaml 1234567891011121314151617181920212223cat &lt;&lt;EOT&gt; ssl-termination-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: httpsspec: ingressClassName: nginx tls: - hosts: - kans.com secretName: secret-https rules: - host: kans.com http: paths: - path: / pathType: Prefix backend: service: name: svc-https port: number: 8080EOT 123456789101112131415161718192021222324252627282930313233# 서비스와 파드 생성curl -s -O https://raw.githubusercontent.com/gasida/NDKS/main/7/svc-pod.yamlkubectl apply -f svc-pod.yaml# 도메인 변경MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt; 예시) gasida.comMYDOMAIN1=kans.comecho $MYDOMAIN1sed -i &quot;s/kans.com/$MYDOMAIN1/g&quot; ssl-termination-ingress.yaml# 인그레스 생성kubectl apply -f ssl-termination-ingress.yaml# 인증서 생성# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=dkos.com/O=dkos.com&quot;mkdir key &amp;&amp; cd keyMYDOMAIN1=kans.comopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=$MYDOMAIN1/O=$MYDOMAIN1&quot;tree# Secret 생성kubectl create secret tls secret-https --key tls.key --cert tls.crt# Secret 확인 kubectl get secrets secret-httpskubectl get secrets secret-https -o yaml-------------------# 자신의 PC 에서 접속 확인 : PC 웹브라우저# 접속 확인 : -k 는 https 접속 시 : 접속 포트 정보 확인curl -Lk https://$MYDOMAIN1:30443## (옵션) /etc/hosts 파일 변경 없이 접속 방안curl -Lk -H &quot;host: $MYDOMAIN1&quot; https://$MYDOMAIN1:30443 Nginx SSL Termination 패킷 확인 : 중간 172.16.29.11 이 nginx controller 12345# 패킷 캡처 명령어 참고export IngHttp=$(kubectl get service -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}')export IngHttps=$(kubectl get service -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[1].nodePort}')tcpdump -i &lt;nginx 파드 veth&gt; -nnq tcp port 80 or tcp port 443 or tcp port 8080 or tcp port $IngHttp or tcp port $IngHttpstcpdump -i &lt;nginx 파드 veth&gt; -nn tcp port 80 or tcp port 443 or tcp port 8080 or tcp port $IngHttp or tcp port $IngHttps -w /tmp/ingress.pcap 삭제 12kubectl delete pod,svc,ingress --allhelm uninstall -n ingress ingress-nginx 123456789101112# CloudFormation 스택 삭제aws cloudformation delete-stack --stack-name mylab# [모니터링] CloudFormation 스택 상태 : 삭제 확인while true; do date AWS_PAGER=&quot;&quot; aws cloudformation list-stacks \\ --stack-status-filter CREATE_IN_PROGRESS CREATE_COMPLETE CREATE_FAILED DELETE_IN_PROGRESS DELETE_FAILED \\ --query &quot;StackSummaries[*].{StackName:StackName, StackStatus:StackStatus}&quot; \\ --output table sleep 1done","link":"/blog/2024/12/01/docs/ingress/ingress/"},{"title":"overlay network concept","text":"가상화 또는 클라우드 기반의 네트워크는 유연하게 확장이 되어야 한다. 관리하는 Subnet들이 많아지고 연결되어 있는 host의 수가 증가하면 포워딩, 스위칭, 라우팅 과 같은 네트워크 구성이 복잡해진다. 따라서 물리적인 네트워크망은 고려하지 않고 가상으로 host와 host의 연결만을 고려해서 구현한 기술이 바로 Network Overlay이다. 오버레이 네트워크 종류 Virtual Extensible Local Area Network (VxLAN) Generic Routing Encapsulation (GRE) IP Security (IPsec) Multiprotocol Label Switching (MPLS) Ethernet Virtual Private Network(EVPN) Software-Defined WAN (SD-WAN) Software-Defined Access (SD-Access) Application Centric Infrastructure (ACI) Cisco Virtual Topology System (VTS) practice hostA 내부 veth와 hostB 내부 veth간 ICMP 통신을 실습 hostA : 192.168.64.27, hostB: 192.168.64.28 hostA 설정 namsepace, bridge, VxLAN을 설정한다. 123456789101112131415161718192021222324252627282930313233# red: VxLAN을 설정한 namespaceip netns add red# blue: 실제 통신을 위한 가상 단말기ip netns add blue# red와 blue의 veth peer를 구성# mtu 1450으로 설정ip link add dev red-veth mtu 1450 netns red type veth peer name blue-veth mtu 1450 netns blue# 실제통신 단말기의 ip와 mac을 설정ip netns exec blue ip link set dev blue-veth address 02:42:c0:a8:00:02ip netns exec blue ip addr add dev blue-veth 7.7.7.2/24# red에 bridge device설정 및 ip 세팅ip netns exec red ip link add dev br0 type bridgeip netns exec red ip addr add dev br0 7.7.7.1/24# endpoint 식별을 위해 id 설정 (VNI)# proxy옵션: arp 쿼리에 응답# learning: bridge fdb entry 자동갱신 (수정설정할 필요 없음)# dstport 4789: UDP port 터널링ip link add dev vxlan1 netns red type vxlan id 42 proxy learning dstport 4789# red 내부에 VxLAN, peer veth 장비를 bridge와 연결한다.ip netns exec red ip link set red-veth master br0ip netns exec red ip link set vxlan1 master br0# 모든 가상장치를 실행한다.ip netns exec red ip link set br0 upip netns exec red ip link set vxlan1 upip netns exec red ip link set red-veth upip netns exec blue ip link set blue-veth up hostB 설정1234567891011121314151617181920ip netns add redip netns add blueip link add dev red-veth mtu 1450 netns red type veth peer name blue-veth mtu 1450 netns blueip netns exec blue ip link set dev blue-veth address 02:42:c0:a8:00:03ip netns exec blue ip addr add dev blue-veth 7.7.7.3/24ip netns exec red ip link add dev br0 type bridgeip netns exec red ip addr add dev br0 7.7.7.1/24ip link add dev vxlan1 netns red type vxlan id 42 proxy learning dstport 4789ip netns exec red ip link set red-veth master br0ip netns exec red ip link set vxlan1 master br0ip netns exec red ip link set br0 upip netns exec red ip link set vxlan1 upip netns exec red ip link set red-veth upip netns exec blue ip link set blue-veth up hostA &gt; blue12345\\# nsenter --net=/var/run/netns/blueping 7.7.7.3 # 실패\\# ip neigh # arp에 등록이 안되어있음.7.7.7.3 dev blue-veth FAILED 123\\# nsenter --net=/var/run/netns/redtcpdump -i br0 # 로그 확인05:28:15.024947 ARP, Request who-has 7.7.7.3 tell 7.7.7.2, length 28 # arp 요청만 한다. mac 추가 VxLAN이 통신을 해야하는데, 해당 ip는 host 내부에 없다. hostA의 VxLAN에 통신할 arp 정보를 수동으로 등록해준다. 123456\\# nsenter --net=/var/run/netns/red\\# ip neigh add 7.7.7.3 lladdr 02:42:c0:a8:00:03 dev vxlan1 # in red namespace\\# ip neigh show7.7.7.3 dev vxlan1 lladdr 02:42:c0:a8:00:03 PERMANENT arp table state 다시 테스트하면 ARP 통신이 된다. 1234\\# ping 7.7.7.3PING 7.7.7.3 (7.7.7.3) 56(84) bytes of data.--- 7.7.7.3 ping statistics ---5 packets transmitted, 0 received, 100% packet loss, time 4087ms 12\\# ip neigh # arp테이블에 업데이트됨을 확인7.7.7.3 dev blue-veth lladdr 02:42:c0:a8:00:03 REACHABLE hostA &gt; red ARP 통신 이후 ICMP 통신이 실패함을 알 수 있다. ICMP (Internet Control Message Protocol) - L3 (Network Layer) 프로토콜 123456789root@seongtki:~# tcpdump -i br0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes05:32:09.944756 ARP, Request who-has 7.7.7.3 tell 7.7.7.2, length 2805:32:09.944784 ARP, Reply 7.7.7.3 is-at 02:42:c0:a8:00:03 (oui Unknown), length 2805:32:09.944861 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1821, seq 1, length 6405:32:10.964330 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1821, seq 2, length 6405:32:11.987056 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1821, seq 3, length 6405:32:13.011901 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1821, seq 4, length 64 hostA &gt; blue br0 존재하는 blue 에서 netfilter 룰을 조회. FORWARD 가 ACCEPT 임을 확인. 1234root@seongtki:~# iptables -t filter -L | grep policyChain INPUT (policy ACCEPT)Chain FORWARD (policy ACCEPT)Chain OUTPUT (policy ACCEPT) route 조회 12root@seongtki:~# ip route7.7.7.0/24 dev br0 proto kernel scope link src 7.7.7.1 bridge fdb 조회 vxlan1의 정보를 확인할 수 있다. 12root@seongtki:~# bridge fdb | grep 02:42:c0:a8:00:0302:42:c0:a8:00:03 dev vxlan1 master br0 정확한 목적지 ip, VxLAN ip 등을 명시해 fdb entry에 추가한다. 12345root@seongtki:~# bridge fdb add 02:42:c0:a8:00:03 dev vxlan1 self dst 192.168.64.28 vni 42 port 4789root@seongtki:~# bridge fdb | grep 02:42:c0:a8:00:0302:42:c0:a8:00:03 dev vxlan1 master br002:42:c0:a8:00:03 dev vxlan1 dst 192.168.64.28 link-netnsid 1 self permanent hostB &gt; red hostB에도 필요한 정보가 전부 존재하는지 조회를 해보자. arp는 수동으로 추가해주어야 한다. fdb는 이미 추가되어 있다. -&gt; vxlan1 추가 시 learning 키워드를 추가해주면, fdb는 필요한 정보를 자동으로 업데이트 한다.(hostA에서 이미 추가되어있어 요청했을 때 자동으로 추가됨) 1234567\\# nsenter --net=/var/run/netns/redroot@seongtki:~# ip neighroot@seongtki:~# ip route7.7.7.0/24 dev br0 proto kernel scope link src 7.7.7.1root@seongtki:~# bridge fdb | grep 02:42:c0:a8:00:0202:42:c0:a8:00:02 dev vxlan1 master br002:42:c0:a8:00:02 dev vxlan1 dst 192.168.64.27 link-netnsid 1 self arp 정보 추가 1234\\# ip neigh add 7.7.7.2 lladdr 02:42:c0:a8:00:02 dev vxlan1root@seongtki:~# ip neigh show7.7.7.2 dev vxlan1 lladdr 02:42:c0:a8:00:02 PERMANENT 최종 테스트hostA &gt; blue 정상적으로 응답 123456root@seongtki:~# ping 7.7.7.3PING 7.7.7.3 (7.7.7.3) 56(84) bytes of data.64 bytes from 7.7.7.3: icmp_seq=153 ttl=64 time=1008 ms64 bytes from 7.7.7.3: icmp_seq=154 ttl=64 time=4.79 ms64 bytes from 7.7.7.3: icmp_seq=155 ttl=64 time=3.85 ms64 bytes from 7.7.7.3: icmp_seq=156 ttl=64 time=3.37 ms hostB &gt; red 정상적인 통신로그 조회 123456789root@seongtki:~# tcpdump -i br0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes03:12:11.488557 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1958, seq 174, length 6403:12:11.488648 IP 7.7.7.3 &gt; 7.7.7.2: ICMP echo reply, id 1958, seq 174, length 6403:12:12.490214 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1958, seq 175, length 6403:12:12.490423 IP 7.7.7.3 &gt; 7.7.7.2: ICMP echo reply, id 1958, seq 175, length 6403:12:13.495586 IP 7.7.7.2 &gt; 7.7.7.3: ICMP echo request, id 1958, seq 176, length 6403:12:13.495828 IP 7.7.7.3 &gt; 7.7.7.2: ICMP echo reply, id 1958, seq 176, length 64 지금까지 구성한 overlay network 구성도. referencehttps://ebt-forti.tistory.com/m/145?category=849281 https://netpple.github.io/docs/make-container-without-docker/","link":"/blog/2024/11/24/docs/overlaynetwork/overlay-network/"},{"title":"vpccni-install","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464AWSTemplateFormatVersion: '2010-09-09'Metadata: AWS::CloudFormation::Interface: ParameterGroups: - Label: default: &quot;&lt;&lt;&lt;&lt;&lt; Deploy EC2 &gt;&gt;&gt;&gt;&gt;&quot; Parameters: - KeyName - MyIamUserAccessKeyID - MyIamUserSecretAccessKey - SgIngressSshCidr - MyInstanceType - LatestAmiId - Label: default: &quot;&lt;&lt;&lt;&lt;&lt; EKS Config &gt;&gt;&gt;&gt;&gt;&quot; Parameters: - ClusterBaseName - KubernetesVersion - WorkerNodeInstanceType - WorkerNodeCount - WorkerNodeVolumesize - Label: default: &quot;&lt;&lt;&lt;&lt;&lt; Region AZ &gt;&gt;&gt;&gt;&gt;&quot; Parameters: - TargetRegion - AvailabilityZone1 - AvailabilityZone2 - AvailabilityZone3 - Label: default: &quot;&lt;&lt;&lt;&lt;&lt; VPC Subnet &gt;&gt;&gt;&gt;&gt;&quot; Parameters: - VpcBlock - PublicSubnet1Block - PublicSubnet2Block - PublicSubnet3Block - PrivateSubnet1Block - PrivateSubnet2Block - PrivateSubnet3BlockParameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instances. Linked to AWS Parameter Type: AWS::EC2::KeyPair::KeyName ConstraintDescription: must be the name of an existing EC2 KeyPair. MyIamUserAccessKeyID: Description: IAM User - AWS Access Key ID (won't be echoed) Type: String NoEcho: true MyIamUserSecretAccessKey: Description: IAM User - AWS Secret Access Key (won't be echoed) Type: String NoEcho: true SgIngressSshCidr: Description: The IP address range that can be used to communicate to the EC2 instances Type: String MinLength: '9' MaxLength: '18' Default: 0.0.0.0/0 AllowedPattern: (\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2}) ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x. MyInstanceType: Description: Enter t2.micro, t2.small, t2.medium, t3.micro, t3.small, t3.medium. Default is t2.micro. Type: String Default: t3.medium AllowedValues: - t2.micro - t2.small - t2.medium - t3.micro - t3.small - t3.medium LatestAmiId: Description: (DO NOT CHANGE) Type: 'AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;' Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2' AllowedValues: - /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 ClusterBaseName: Type: String Default: myeks AllowedPattern: &quot;[a-zA-Z][-a-zA-Z0-9]*&quot; Description: must be a valid Allowed Pattern '[a-zA-Z][-a-zA-Z0-9]*' ConstraintDescription: ClusterBaseName - must be a valid Allowed Pattern KubernetesVersion: Description: Enter Kubernetes Version, 1.28 ~ 1.31 Type: String Default: &quot;1.30&quot; WorkerNodeInstanceType: Description: Enter EC2 Instance Type. Default is t3.medium. Type: String Default: t3.medium WorkerNodeCount: Description: Worker Node Counts Type: String Default: 3 WorkerNodeVolumesize: Description: Worker Node Volumes size Type: String Default: 30 TargetRegion: Type: String Default: ap-northeast-2 AvailabilityZone1: Type: String Default: ap-northeast-2a AvailabilityZone2: Type: String Default: ap-northeast-2b AvailabilityZone3: Type: String Default: ap-northeast-2c VpcBlock: Type: String Default: 192.168.0.0/16 PublicSubnet1Block: Type: String Default: 192.168.1.0/24 PublicSubnet2Block: Type: String Default: 192.168.2.0/24 PublicSubnet3Block: Type: String Default: 192.168.3.0/24 PrivateSubnet1Block: Type: String Default: 192.168.11.0/24 PrivateSubnet2Block: Type: String Default: 192.168.12.0/24 PrivateSubnet3Block: Type: String Default: 192.168.13.0/24Resources:# VPC EksVPC: Type: AWS::EC2::VPC Properties: CidrBlock: !Ref VpcBlock EnableDnsSupport: true EnableDnsHostnames: true Tags: - Key: Name Value: !Sub ${ClusterBaseName}-VPC# PublicSubnets PublicSubnet1: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone1 CidrBlock: !Ref PublicSubnet1Block VpcId: !Ref EksVPC MapPublicIpOnLaunch: true Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PublicSubnet1 - Key: kubernetes.io/role/elb Value: 1 PublicSubnet2: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone2 CidrBlock: !Ref PublicSubnet2Block VpcId: !Ref EksVPC MapPublicIpOnLaunch: true Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PublicSubnet2 - Key: kubernetes.io/role/elb Value: 1 PublicSubnet3: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone3 CidrBlock: !Ref PublicSubnet3Block VpcId: !Ref EksVPC MapPublicIpOnLaunch: true Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PublicSubnet3 - Key: kubernetes.io/role/elb Value: 1 InternetGateway: Type: AWS::EC2::InternetGateway VPCGatewayAttachment: Type: AWS::EC2::VPCGatewayAttachment Properties: InternetGatewayId: !Ref InternetGateway VpcId: !Ref EksVPC PublicSubnetRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PublicSubnetRouteTable PublicSubnetRoute: Type: AWS::EC2::Route Properties: RouteTableId: !Ref PublicSubnetRouteTable DestinationCidrBlock: 0.0.0.0/0 GatewayId: !Ref InternetGateway PublicSubnet1RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnet1 RouteTableId: !Ref PublicSubnetRouteTable PublicSubnet2RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnet2 RouteTableId: !Ref PublicSubnetRouteTable PublicSubnet3RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnet3 RouteTableId: !Ref PublicSubnetRouteTable# PrivateSubnets PrivateSubnet1: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone1 CidrBlock: !Ref PrivateSubnet1Block VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PrivateSubnet1 - Key: kubernetes.io/role/internal-elb Value: 1 PrivateSubnet2: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone2 CidrBlock: !Ref PrivateSubnet2Block VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PrivateSubnet2 - Key: kubernetes.io/role/internal-elb Value: 1 PrivateSubnet3: Type: AWS::EC2::Subnet Properties: AvailabilityZone: !Ref AvailabilityZone3 CidrBlock: !Ref PrivateSubnet3Block VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PrivateSubnet3 - Key: kubernetes.io/role/internal-elb Value: 1 PrivateSubnetRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-PrivateSubnetRouteTable PrivateSubnet1RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PrivateSubnet1 RouteTableId: !Ref PrivateSubnetRouteTable PrivateSubnet2RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PrivateSubnet2 RouteTableId: !Ref PrivateSubnetRouteTable PrivateSubnet3RouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PrivateSubnet3 RouteTableId: !Ref PrivateSubnetRouteTable# EKSCTL-Host EKSEC2SG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: eksctl-host Security Group VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-HOST-SG SecurityGroupIngress: - IpProtocol: '-1' #FromPort: '22' #ToPort: '22' CidrIp: !Ref SgIngressSshCidr EKSEC2: Type: AWS::EC2::Instance Properties: InstanceType: !Ref MyInstanceType ImageId: !Ref LatestAmiId KeyName: !Ref KeyName Tags: - Key: Name Value: !Sub ${ClusterBaseName}-bastion NetworkInterfaces: - DeviceIndex: 0 SubnetId: !Ref PublicSubnet1 GroupSet: - !Ref EKSEC2SG AssociatePublicIpAddress: true PrivateIpAddress: 192.168.1.100 BlockDeviceMappings: - DeviceName: /dev/xvda Ebs: VolumeType: gp3 VolumeSize: 30 DeleteOnTermination: true UserData: Fn::Base64: !Sub | #!/bin/bash hostnamectl --static set-hostname &quot;${ClusterBaseName}-bastion&quot; # Config Root account echo 'root:qwe123' | chpasswd sed -i &quot;s/^#PermitRootLogin yes/PermitRootLogin yes/g&quot; /etc/ssh/sshd_config sed -i &quot;s/^PasswordAuthentication no/PasswordAuthentication yes/g&quot; /etc/ssh/sshd_config rm -rf /root/.ssh/authorized_keys systemctl restart sshd # Config convenience echo 'alias vi=vim' &gt;&gt; /etc/profile echo &quot;sudo su -&quot; &gt;&gt; /home/ec2-user/.bashrc sed -i &quot;s/UTC/Asia\\/Seoul/g&quot; /etc/sysconfig/clock ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages yum -y install tree jq git htop # Install kubectl &amp; helm cd /root curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.30.4/2024-09-11/bin/linux/amd64/kubectl install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash # Install eksctl curl -sL &quot;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz&quot; | tar xz -C /tmp mv /tmp/eksctl /usr/local/bin # Install aws cli v2 curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; unzip awscliv2.zip &gt;/dev/null 2&gt;&amp;1 ./aws/install complete -C '/usr/local/bin/aws_completer' aws echo 'export AWS_PAGER=&quot;&quot;' &gt;&gt;/etc/profile export AWS_DEFAULT_REGION=${AWS::Region} echo &quot;export AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION&quot; &gt;&gt; /etc/profile # Create SSH Keypair ssh-keygen -t rsa -N &quot;&quot; -f /root/.ssh/id_rsa # IAM User Credentials export AWS_ACCESS_KEY_ID=${MyIamUserAccessKeyID} export AWS_SECRET_ACCESS_KEY=${MyIamUserSecretAccessKey} export AWS_DEFAULT_REGION=${AWS::Region} export ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text) echo &quot;export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID&quot; &gt;&gt; /etc/profile echo &quot;export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY&quot; &gt;&gt; /etc/profile echo &quot;export AWS_REGION=$AWS_DEFAULT_REGION&quot; &gt;&gt; /etc/profile echo &quot;export AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION&quot; &gt;&gt; /etc/profile echo &quot;export ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)&quot; &gt;&gt; /etc/profile # CLUSTER_NAME export CLUSTER_NAME=${ClusterBaseName} echo &quot;export CLUSTER_NAME=$CLUSTER_NAME&quot; &gt;&gt; /etc/profile # K8S Version export KUBERNETES_VERSION=${KubernetesVersion} echo &quot;export KUBERNETES_VERSION=$KUBERNETES_VERSION&quot; &gt;&gt; /etc/profile # VPC &amp; Subnet export VPCID=$(aws ec2 describe-vpcs --filters &quot;Name=tag:Name,Values=$CLUSTER_NAME-VPC&quot; | jq -r .Vpcs[].VpcId) echo &quot;export VPCID=$VPCID&quot; &gt;&gt; /etc/profile export PubSubnet1=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PublicSubnet1&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) export PubSubnet2=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PublicSubnet2&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) export PubSubnet3=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PublicSubnet3&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) echo &quot;export PubSubnet1=$PubSubnet1&quot; &gt;&gt; /etc/profile echo &quot;export PubSubnet2=$PubSubnet2&quot; &gt;&gt; /etc/profile echo &quot;export PubSubnet3=$PubSubnet3&quot; &gt;&gt; /etc/profile export PrivateSubnet1=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PrivateSubnet1&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) export PrivateSubnet2=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PrivateSubnet2&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) export PrivateSubnet3=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=&quot;$CLUSTER_NAME-PrivateSubnet3&quot; --query &quot;Subnets[0].[SubnetId]&quot; --output text) echo &quot;export PrivateSubnet1=$PrivateSubnet1&quot; &gt;&gt; /etc/profile echo &quot;export PrivateSubnet2=$PrivateSubnet2&quot; &gt;&gt; /etc/profile echo &quot;export PrivateSubnet3=$PrivateSubnet3&quot; &gt;&gt; /etc/profile # Create EKS Cluster &amp; Nodegroup eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=ng1 --node-type=${WorkerNodeInstanceType} --nodes ${WorkerNodeCount} --node-volume-size=${WorkerNodeVolumesize} --vpc-public-subnets &quot;$PubSubnet1&quot;,&quot;$PubSubnet2&quot;,&quot;$PubSubnet3&quot; --version ${KubernetesVersion} --ssh-access --ssh-public-key /root/.ssh/id_rsa.pub --with-oidc --external-dns-access --full-ecr-access --alb-ingress-access --dry-run &gt; myeks.yaml sed -i 's/certManager: false/certManager: true/g' myeks.yaml sed -i 's/awsLoadBalancerController: false/awsLoadBalancerController: true/g' myeks.yaml cat &lt;&lt;EOT &gt;&gt; myeks.yaml addons: - name: vpc-cni # no version is specified so it deploys the default version version: latest # auto discovers the latest available attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy configurationValues: |- enableNetworkPolicy: &quot;true&quot; - name: kube-proxy version: latest - name: coredns version: latest EOT cat &lt;&lt;EOT &gt; precmd.yaml preBootstrapCommands: - &quot;yum install nvme-cli links tree tcpdump sysstat -y&quot; EOT sed -i -n -e '/instanceType/r precmd.yaml' -e '1,$p' myeks.yaml nohup eksctl create cluster -f myeks.yaml --verbose 4 --kubeconfig &quot;/root/.kube/config&quot; 1&gt; /root/create-eks.log 2&gt;&amp;1 &amp; # Install krew curl -L https://github.com/kubernetes-sigs/krew/releases/download/v0.4.4/krew-linux_amd64.tar.gz -o /root/krew-linux_amd64.tar.gz tar zxvf krew-linux_amd64.tar.gz ./krew-linux_amd64 install krew export PATH=&quot;$PATH:/root/.krew/bin&quot; echo 'export PATH=&quot;$PATH:/root/.krew/bin&quot;' &gt;&gt; /etc/profile # Install kube-ps1 echo 'source &lt;(kubectl completion bash)' &gt;&gt; /root/.bashrc echo 'alias k=kubectl' &gt;&gt; /root/.bashrc echo 'complete -F __start_kubectl k' &gt;&gt; /root/.bashrc git clone https://github.com/jonmosco/kube-ps1.git /root/kube-ps1 cat &lt;&lt;&quot;EOT&quot; &gt;&gt; /root/.bashrc source /root/kube-ps1/kube-ps1.sh KUBE_PS1_SYMBOL_ENABLE=false function get_cluster_short() { echo &quot;$1&quot; | cut -d . -f1 } KUBE_PS1_CLUSTER_FUNCTION=get_cluster_short KUBE_PS1_SUFFIX=') ' PS1='$(kube_ps1)'$PS1 EOT # Install krew plugin kubectl krew install ctx ns get-all neat stern # ktop df-pv mtail tree # Install Docker amazon-linux-extras install docker -y systemctl start docker &amp;&amp; systemctl enable docker echo 'cloudinit End!'Outputs: eksctlhost: Value: !GetAtt EKSEC2.PublicIp","link":"/blog/2024/11/01/docs/vpccni/install/"},{"title":"vpccni-2","text":"ExternalDNS소개 : K8S 서비스/인그레스 생성 시 도메인을 설정하면, AWS(Route 53), Azure(DNS), GCP(Cloud DNS) 에 A 레코드(TXT 레코드)로 자동 생성/삭제 ExternalDNS CTRL 권한 주는 방법 3가지 : Node IAM Role, Static credentials, IRSA CoreDNS Topology Aware Routing1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 현재 노드 AZ 배포 확인kubectl get node --label-columns=topology.kubernetes.io/zoneNAME STATUS ROLES AGE VERSION ZONEip-192-168-1-225.ap-northeast-2.compute.internal Ready &lt;none&gt; 70m v1.24.11-eks-a59e1f0 ap-northeast-2aip-192-168-2-248.ap-northeast-2.compute.internal Ready &lt;none&gt; 70m v1.24.11-eks-a59e1f0 ap-northeast-2bip-192-168-3-228.ap-northeast-2.compute.internal Ready &lt;none&gt; 70m v1.24.11-eks-a59e1f0 ap-northeast-2c# 테스트를 위한 디플로이먼트와 서비스 배포cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: deploy-echospec: replicas: 3 selector: matchLabels: app: deploy-websrv template: metadata: labels: app: deploy-websrv spec: terminationGracePeriodSeconds: 0 containers: - name: websrv image: registry.k8s.io/echoserver:1.5 ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: svc-clusteripspec: ports: - name: svc-webport port: 80 targetPort: 8080 selector: app: deploy-websrv type: ClusterIPEOF# 확인kubectl get deploy,svc,ep,endpointsliceskubectl get pod -owidekubectl get svc,ep svc-clusteripkubectl get endpointslices -l kubernetes.io/service-name=svc-clusteripkubectl get endpointslices -l kubernetes.io/service-name=svc-clusterip -o yaml# 접속 테스트를 수행할 클라이언트 파드 배포cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: netshoot-podspec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0EOF# 확인kubectl get pod -owide 테스트 파드(netshoot-pod)에서 ClusterIP 접속 시 부하분산 확인 : AZ(zone) 상관없이 랜덤 확률 부하분산 동작 123456789101112131415# 디플로이먼트 파드가 배포된 AZ(zone) 확인kubectl get pod -l app=deploy-websrv -owide# 테스트 파드(netshoot-pod)에서 ClusterIP 접속 시 부하분산 확인kubectl exec -it netshoot-pod -- curl svc-clusterip | grep HostnameHostname: deploy-echo-859cc9b57d-sjzg9kubectl exec -it netshoot-pod -- curl svc-clusterip | grep HostnameHostname: deploy-echo-859cc9b57d-zsp42# 100번 반복 접속 : 3개의 파드로 AZ(zone) 상관없이 랜덤 확률 부하분산 동작(Administrator@myeks:N/A) [root@myeks-bastion ~]# kubectl exec -it netshoot-pod -- zsh -c &quot;for i in {1..100}; do curl -s svc-clusterip | grep Hostname; done | sort | uniq -c | sort -nr&quot; 36 Hostname: deploy-echo-859cc9b57d-gqkzz 33 Hostname: deploy-echo-859cc9b57d-sjzg9 31 Hostname: deploy-echo-859cc9b57d-zsp42 Using AWS Load Balancer Controller for blue/green deployment, canary deployment and A/B testing Weighted target group 가중치가 적용된 대상 그룹 AWS 고객이 블루/그린 및 카나리아 배포와 A/B 테스트 전략을 채택할 수 있도록 돕기 위해 AWS는 2019년 11월에 애플리케이션 로드 밸런서에 대한 가중 대상 그룹을 발표했습니다. 여러 대상 그룹을 리스너 규칙 의 동일한 전달 작업 에 연결 하고 각 그룹에 대한 가중치를 지정할 수 있습니다. 이를 통해 개발자는 트래픽을 여러 버전의 애플리케이션에 분산하는 방법을 제어할 수 있습니다. 예를 들어, 가중치가 8과 2인 두 개의 대상 그룹이 있는 규칙을 정의하면 로드 밸런서는 트래픽의 80%를 첫 번째 대상 그룹으로, 20%를 다른 대상 그룹으로 라우팅합니다. Advanced request routing 고급 요청 라우팅 AWS는 가중치가 적용된 대상 그룹 외에도 2019년에 고급 요청 라우팅 기능을 발표했습니다 . 고급 요청 라우팅은 개발자에게 표준 및 사용자 지정 HTTP 헤더와 메서드, 요청 경로, 쿼리 문자열, 소스 IP 주소를 기반으로 규칙을 작성하고 트래픽을 라우팅할 수 있는 기능을 제공합니다. 이 새로운 기능은 라우팅을 위한 프록시 플릿의 필요성을 없애 애플리케이션 아키텍처를 간소화하고, 로드 밸런서에서 원치 않는 트래픽을 차단하며, A/B 테스트를 구현할 수 있도록 합니다. AWS Load Balancer Controller AWS 로드 밸런서 컨트롤러 AWS Load Balancer Controller 는 Kubernetes 클러스터의 Elastic Load Balancer를 관리하는 데 도움이 되는 컨트롤러입니다. 애플리케이션 로드 밸런서를 프로비저닝하여 Kubernetes 인그레스 리소스를 충족합니다. Kubernetes 인그레스 객체에 주석을 추가하여 프로비저닝된 애플리케이션 로드 밸런서의 동작을 사용자 지정할 수 있습니다. 이를 통해 개발자는 애플리케이션 로드 밸런서를 구성하고 Kubernetes 기본 의미 체계를 사용하여 블루/그린, 카나리아 및 A/B 배포를 실현할 수 있습니다. 예를 들어, 다음 인그레스 주석은 애플리케이션 로드 밸런서를 구성하여 두 버전의 애플리케이션 간에 트래픽을 분할합니다. 1234567891011121314151617181920**annotations**: ... **alb.ingress.kubernetes.io/actions.blue-green**: | { &quot;type&quot;:&quot;forward&quot;, &quot;forwardConfig&quot;:{ &quot;**targetGroups**&quot;:[ { &quot;serviceName&quot;:&quot;**hello-kubernetes-v1**&quot;, &quot;servicePort&quot;:&quot;80&quot;, &quot;weight&quot;:**50** }, { &quot;serviceName&quot;:&quot;h**ello-kubernetes-v2**&quot;, &quot;servicePort&quot;:&quot;80&quot;, &quot;weight&quot;:**50** } ] } } Network Policies with VPC CNILink1 Link2 Link3 AWS EKS fully supports the upstream Kubernetes Network Policy API, ensuring compatibility and adherence to Kubernetes standards. eBPF로 패킷 필터링 동작 - Network Policy Controller, Node Agent, eBPF SDK 사전 조건 : EKS 1.25 버전 이상, AWS VPC CNI 1.14 이상, OS 커널 5.10 이상 EKS 최적화 AMI(AL2, Bottlerocket, Ubuntu) Network Policy Controller : v1.25 EKS 버전 이상 자동 설치, 통제 정책 모니터링 후 eBPF 프로그램을 생성 및 업데이트하도록 Node Agent에 지시 Node Agent : AWS VPC CNI 번들로 ipamd 플러그인과 함께 설치됨(aws-node 데몬셋). eBPF 프래그램을 관리 eBPF SDK : AWS VPC CNI에는 노드에서 eBPF 프로그램과 상호 작용할 수 있는 SDK 포함, eBPF 실행의 런타임 검사, 추적 및 분석 가능 사전정보확인 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Network Policy 기본 비활성화되어 있어, 활성화 필요 : 실습 환경은 미리 활성화 설정 추가되어 있음tail -n 11 myeks.yamladdons: - name: vpc-cni # no version is specified so it deploys the default version version: latest # auto discovers the latest available attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy configurationValues: |- enableNetworkPolicy: &quot;true&quot;# Node Agent 확인 : AWS VPC CNI 1.14 이상 버전 정보 확인kubectl get ds aws-node -n kube-system -o yaml | k neat... - args: - --enable-ipv6=false - --enable-network-policy=true... volumeMounts: - mountPath: /host/opt/cni/bin name: cni-bin-dir - mountPath: /sys/fs/bpf name: bpf-pin-path - mountPath: /var/log/aws-routed-eni name: log-dir - mountPath: /var/run/aws-node name: run-dir...kubectl get ds aws-node -n kube-system -o yaml | grep -i image:kubectl get pod -n kube-system -l k8s-app=aws-nodekubectl get ds -n kube-system aws-node -o jsonpath='{.spec.template.spec.containers[*].name}{&quot;\\n&quot;}'aws-node aws-eks-nodeagent# EKS 1.25 버전 이상 확인kubectl get nod# OS 커널 5.10 이상 확인ssh ec2-user@$N1 uname -r5.10.210-201.852.amzn2.x86_64# 실행 중인 eBPF 프로그램 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo /opt/cni/bin/aws-eks-na-cli ebpf progs; echo; done...Programs currently loaded : Type : 26 ID : 6 Associated maps count : 1========================================================================================Type : 26 ID : 8 Associated maps count : 1========================================================================================# 각 노드에 BPF 파일 시스템을 탑재 확인ssh ec2-user@$N1 mount | grep -i bpfnone on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)ssh ec2-user@$N1 df -a | grep -i bpfnone 0 0 0 - /sys/fs/bpf 샘플 애플리케이션 배포 및 네트워크 정책 적용 실습 - Link 12345678910111213141516#git clone &lt;https://github.com/aws-samples/eks-network-policy-examples.git&gt;cd eks-network-policy-examplestree advanced/manifests/kubectl apply -f advanced/manifests/# 확인kubectl get pod,svckubectl get pod,svc -n another-ns# 통신 확인kubectl exec -it client-one -- curl demo-appkubectl exec -it client-two -- curl demo-appkubectl exec -it another-client-one -n another-ns -- curl **demo-app**kubectl exec -it another-client-one -n another-ns -- curl demo-app.**default**kubectl exec -it another-client-two -n another-ns -- curl demo-app.default.svc 모든 트래픽 거부 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 모니터링# kubectl exec -it client-one -- curl demo-appwhile true; do kubectl exec -it client-one -- curl --connect-timeout 1 demo-app ; date; sleep 1; done# 정책 적용cat advanced/policies/01-deny-all-ingress.yamlkubectl apply -f advanced/policies/01-deny-all-ingress.yaml**kubectl get networkpolicy**# 실행 중인 eBPF 프로그램 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf progs**; echo; donefor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf loaded-ebpfdata**; echo; done...&gt;&gt; node 192.168.3.201 &lt;&lt;PinPath: /sys/fs/bpf/globals/aws/programs/demo-app-6fd76f694b-default_handle_ingressPod Identifier : **demo-app-6fd76f694b-default** Direction : **ingress** Prog ID: 9Associated Maps -&gt; Map Name: ingress_mapMap ID: **7**Map Name: policy_eventsMap ID: **6**Map Name: aws_conntrack_mapMap ID: **5**========================================================================================PinPath: /sys/fs/bpf/globals/aws/programs/demo-app-6fd76f694b-default_handle_egressPod Identifier : **demo-app-6fd76f694b-default** Direction : **egress** Prog ID: 10Associated Maps -&gt; Map Name: aws_conntrack_mapMap ID: 5Map Name: egress_mapMap ID: 8Map Name: policy_eventsMap ID: 6========================================================================================ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 5ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 9ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 10# 정책 다시 삭제kubectl delete -f advanced/policies/01-deny-all-ingress.yamlfor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf loaded-ebpfdata**; echo; done# 다시 적용kubectl apply -f advanced/policies/01-deny-all-ingress.yaml 동일 네임스페이스 + 클라이언트1 로부터의 수신 허용 1234567891011#cat advanced/policies/03-allow-ingress-from-samens-client-one.yaml kubectl apply -f advanced/policies/03-allow-ingress-from-samens-client-one.yamlkubectl get networkpolicyfor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf loaded-ebpfdata**; echo; donessh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 5ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 9ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 10# 클라이언트2 수신 확인kubectl exec -it client-two -- curl --connect-timeout 1 demo-app another-ns 네임스페이스로부터의 수신 허용 123456789101112131415# 모니터링# kubectl exec -it another-client-one -n another-ns -- curl --connect-timeout 1 demo-app.defaultwhile true; do kubectl exec -it another-client-one -n another-ns -- curl --connect-timeout 1 demo-app.default ; date; sleep 1; done#cat advanced/policies/04-allow-ingress-from-xns.yamlkubectl apply -f advanced/policies/04-allow-ingress-from-xns.yamlkubectl get networkpolicyfor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf loaded-ebpfdata**; echo; donessh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 5ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 9ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 10#kubectl exec -it another-client-two -n another-ns -- curl --connect-timeout 1 demo-app.default eBPF 관련 정보 확인 123456# 실행 중인 eBPF 프로그램 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf progs**; echo; done# eBPF 로그 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo cat /var/log/aws-routed-eni/ebpf-sdk.log; echo; donefor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo cat /var/log/aws-routed-eni/**network-policy-agent**; echo; done 송신 트래픽 거부 : 기본 네임스페이스의 클라이언트-1 포드에서 모든 송신 격리를 적용 1234567891011121314# 모니터링while true; do kubectl exec -it client-one -- curl --connect-timeout 1 google.com ; date; sleep 1; done#cat advanced/policies/06-deny-egress-from-client-one.yamlkubectl apply -f advanced/policies/06-deny-egress-from-client-one.yamlkubectl get networkpolicyfor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i **sudo /opt/cni/bin/aws-eks-na-cli ebpf loaded-ebpfdata**; echo; donessh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 5ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 9ssh ec2-user@$N3 sudo /opt/cni/bin/aws-eks-na-cli ebpf dump-maps 10#kubectl exec -it client-one -- nslookup demo-app 송신 트래픽 허용 : DNS 트래픽을 포함하여 여러 포트 및 네임스페이스에서의 송신을 허용 1234567# 모니터링while true; do kubectl exec -it client-one -- curl --connect-timeout 1 demo-app ; date; sleep 1; done#cat advanced/policies/08-allow-egress-to-demo-app.yaml | yhkubectl apply -f advanced/policies/08-allow-egress-to-demo-app.yamlkubectl get networkpolicy 실습 후 리소스 삭제 12kubectl delete networkpolicy --allkubectl delete -f advanced/manifests/ AWS VPC CNI + Cilium CNI : Hybrid mode구성 방안 : 각 CNI의 강점을 조합하여 사용 - AWS VPC CNI(IPAM, Routing 등), Cilium(LB, Network Policy, Encryption, Visibility) - Docs 제약 사항 : Layer 7 Policy (see GitHub issue 12454) , IPsec Transparent Encryption (see GitHub issue 15596) 다만, Cilium Full 기능 사용을 위해서는 AWS VPN CNI를 제거하고 Fully to Cilium 사용을 권장함 - Youtube (참고) Cilium CNI 설치 → 기존 배치 파드는 Restart 필요 123456789101112# Cilium CNI 설치helm repo add cilium https://helm.cilium.io/helm install cilium cilium/cilium --version 1.16.3 \\ --namespace kube-system \\ --set cni.chainingMode=aws-cni \\ --set cni.exclusive=false \\ --set enableIPv4Masquerade=false \\ --set routingMode=native \\ --set endpointRoutes.enabled=true## This will enable chaining with the AWS VPC CNI plugin. ## It will also disable tunneling, as it’s not required since ENI IP addresses can be directly routed in the VPC. ## For the same reason, masquerading can be disabled as well.","link":"/blog/2024/11/03/docs/vpccni/vpccni-2/"},{"title":"cluster-install","text":"kind - cluster install1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#cat &lt;&lt;EOT&gt; kind-svc-1w.yamlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4featureGates: &quot;InPlacePodVerticalScaling&quot;: true &quot;MultiCIDRServiceAllocator&quot;: truenodes:- role: control-plane labels: mynode: control-plane topology.kubernetes.io/zone: ap-northeast-2a extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: runtime-config: api/all=true controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0- role: worker labels: mynode: worker1 topology.kubernetes.io/zone: ap-northeast-2a- role: worker labels: mynode: worker2 topology.kubernetes.io/zone: ap-northeast-2b- role: worker labels: mynode: worker3 topology.kubernetes.io/zone: ap-northeast-2cnetworking: podSubnet: 10.10.0.0/16 serviceSubnet: 10.200.1.0/24EOT# k8s 클러스터 설치kind create cluster --config kind-svc-1w.yaml --name myk8s --image kindest/node:v1.31.0docker ps# 노드에 기본 툴 설치docker exec -it myk8s-control-plane sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'for i in worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i sh -c 'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'; echo; done 설치 확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# k8s v1.31.0 버전 확인kubectl get node# 노드 labels 확인kubectl get nodes -o jsonpath=&quot;{.items[*].metadata.labels}&quot; | grep mynodekubectl get nodes -o jsonpath=&quot;{.items[*].metadata.labels}&quot; | jq | grep mynodekubectl get nodes -o jsonpath=&quot;{.items[*].metadata.labels}&quot; | jq | grep 'topology.kubernetes.io/zone'# kind network 중 컨테이너(노드) IP(대역) 확인 : 172.18.0.2~ 부터 할당되며, control-plane 이 꼭 172.18.0.2가 안될 수 도 있음docker ps -q | xargs docker inspect --format '{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}'/myk8s-control-plane 172.18.0.4/myk8s-worker 172.18.0.3/myk8s-worker2 172.18.0.5/myk8s-worker3 172.18.0.2# 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용kubectl get cm -n kube-system kubeadm-config -oyaml | grep -i subnet podSubnet: 10.10.0.0/16 serviceSubnet: 10.200.1.0/24kubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot;# feature-gates 확인 : https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/kubectl describe pod -n kube-system | grep feature-gates --feature-gates=InPlacePodVerticalScaling=truekubectl describe pod -n kube-system | grep runtime-config --runtime-config=api/all=true# MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/kubectl get servicecidrNAME CIDRS AGEkubernetes 10.200.1.0/24 2m13s# 노드마다 할당된 dedicated subnet (podCIDR) 확인kubectl get nodes -o jsonpath=&quot;{.items[*].spec.podCIDR}&quot;10.10.0.0/24 10.10.4.0/24 10.10.3.0/24 10.10.1.0/24# kube-proxy configmap 확인kubectl describe cm -n kube-system kube-proxy...mode: iptablesiptables: localhostNodePorts: null masqueradeAll: false masqueradeBit: null minSyncPeriod: 1s syncPeriod: 0s...# 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ls /opt/cni/bin/; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i cat /etc/cni/net.d/10-kindnet.conflist; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c route; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c addr; echo; donefor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ip -c -4 addr show dev eth0; echo; done# iptables 정보 확인for i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; donefor i in filter nat mangle raw ; do echo &quot;&gt;&gt; IPTables Type : $i &lt;&lt;&quot;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done# 각 노드 bash 접속docker exec -it myk8s-control-plane bashdocker exec -it myk8s-worker bashdocker exec -it myk8s-worker2 bashdocker exec -it myk8s-worker3 bash----------------------------------------exit----------------------------------------# kind 설치 시 kind 이름의 도커 브리지가 생성된다 : 172.18.0.0/16 대역docker network lsdocker inspect kind# arp scan 해두기docker exec -it myk8s-control-plane arp-scan --interfac=eth0 --localnet# mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정docker run -d --rm --name mypc --network kind --ip 172.18.0.100 nicolaka/netshoot sleep infinitydocker ps## 만약 kind 네트워크 대역이 다를 경우 위 IP 지정이 실패할 수 있으니, 그냥 IP 지정 없이 mypc 컨테이너 기동 할 것## docker run -d --rm --name mypc --network kind nicolaka/netshoot sleep infinity# 통신 확인docker exec -it mypc ping -c 1 172.18.0.1for i in {1..5} ; do docker exec -it mypc ping -c 1 172.18.0.$i; donedocker exec -it mypc zsh-------------ifconfigping -c 1 172.18.0.2exit------------- kube-ops-view 설치12345678910111213141516171819202122232425helm repo add geek-cookbook https://geek-cookbook.github.io/charts/helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 --set env.TZ=&quot;Asia/Seoul&quot; --namespace kube-system# myk8s-control-plane 배치kubectl -n kube-system edit deploy kube-ops-view---spec: ... template: ... spec: nodeSelector: mynode: control-plane tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Equal&quot; effect: &quot;NoSchedule&quot;---# 설치 확인kubectl -n kube-system get pod -o wide -l app.kubernetes.io/instance=kube-ops-view# kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5&quot;echo -e &quot;KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2&quot; grafana 설치1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950helm repo add prometheus-community https://prometheus-community.github.io/helm-chartshelm repo update# 파라미터 파일 생성cat &lt;&lt;EOT &gt; monitor-values.yamlprometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false nodeSelector: mynode: control-plane tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Equal&quot; effect: &quot;NoSchedule&quot;grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: 1234 service: type: NodePort nodePort: 30002 nodeSelector: mynode: control-plane tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Equal&quot; effect: &quot;NoSchedule&quot;defaultRules: create: falsealertmanager: enabled: falseEOT# 배포kubectl create ns monitoringhelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 62.3.0 -f monitor-values.yaml --namespace monitoring# 확인helm list -n monitoring# Grafana 접속 계정 : admin / 1234 : macOS 사용자echo -e &quot;Grafana URL = http://localhost:30002&quot;# (참고) helm 삭제helm uninstall -n monitoring kube-prometheus-stack","link":"/blog/2024/09/29/docs/service/install/"},{"title":"nodeport","text":"통신 흐름외부 클라이언트가 노드IP:NodePort 접속 시 해당 노드의 iptables 룰에 의해서 SNAT/DNAT 되어 목적지 파드와 통신 후 리턴 트래픽은 최초 인입 노드를 경유해서 외부로 되돌아감 ClusterIp 서비스 생성과 동일하게 NodePort 서비스도 생성 시 kube-proxy에 의해서 모든 노드에 iptables규칙이 설정됨. 노드에 진입한 트래픽은 iptables에 의해 서비스에 연결된 pod들에 부하분산되어 접속됨. ![스크린샷 2024-09-29 오전 7.08.42](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2024-09-29 오전 7.08.42.png) 외부에서 클러스터의 ‘서비스(NodePort)’ 로 접근 가능 → 이후에는 Cluster IP 통신과 동일! 모드 노드(마스터 포함)에 iptables rule 이 설정되므로, 모든 노드에 NodePort 로 접속 시 iptables rule 에 의해서 분산 접속이 됨 Node 의 모든 Loca IP(Local host Interface IP : loopback 포함) 사용 가능 &amp; Local IP를 지정 가능 쿠버네티스 NodePort 할당 범위 기본 (30000-32767) 실습 구성목적지(backend) 디플로이먼트(Pod) 파일 생성 12345678910111213141516171819202122cat &lt;&lt;EOT&gt; echo-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: deploy-echospec: replicas: 3 selector: matchLabels: app: deploy-websrv template: metadata: labels: app: deploy-websrv spec: terminationGracePeriodSeconds: 0 containers: - name: kans-websrv image: mendhak/http-https-echo ports: - containerPort: 8080EOT 서비스(NodePort) 파일 생성 1234567891011121314cat &lt;&lt;EOT&gt; svc-nodeport.yamlapiVersion: v1kind: Servicemetadata: name: svc-nodeportspec: ports: - name: svc-webport port: 9000 # 서비스 ClusterIP 에 접속 시 사용하는 포트 port 를 의미 targetPort: 8080 # 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미 selector: app: deploy-websrv type: NodePortEOT 1234567891011121314151617181920212223242526272829303132333435363738# 생성kubectl apply -f echo-deploy.yaml,svc-nodeport.yaml# 모니터링watch -d 'kubectl get pod -owide;echo; kubectl get svc,ep svc-nodeport'# 확인kubectl get deploy,pod -o wide# 아래 31493은 서비스(NodePort) 정보!kubectl get svc svc-nodeportNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc-nodeport NodePort 10.200.1.167 &lt;none&gt; 9000:30396/TCP 9skubectl get endpoints svc-nodeportNAME ENDPOINTS AGEsvc-nodeport 10.10.1.5:8080,10.10.2.6:8080,10.10.3.4:8080 17s# Port , TargetPort , NodePort 주의kubectl describe svc svc-nodeportName: svc-nodeportNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=deploy-websrvType: NodePortIP Family Policy: SingleStackIP Families: IPv4IP: 10.200.1.167IPs: 10.200.1.167Port: svc-webport 9000/TCPTargetPort: 8080/TCPNodePort: svc-webport 30396/TCPEndpoints: 10.10.2.6:8080,10.10.3.4:8080,10.10.1.5:8080Session Affinity: NoneExternal Traffic Policy: ClusterInternal Traffic Policy: ClusterEvents: &lt;none&gt; 서비스(NodePort) 접속 확인외부 클라이언트(mypc 컨테이너)에서 접속 테스트 &amp; 서비스(NodePort) 부하분산 접속 확인 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# NodePort 확인 : 아래 NodePort 는 범위내 랜덤 할당으로 실습 환경마다 다릅니다kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}'30396# NodePort 를 변수에 지정NPORT=$(kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}')echo $NPORT# 현재 k8s 버전에서는 포트 Listen 되지 않고, iptables rules 처리됨for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i ss -tlnp; echo; done## (참고) 아래처럼 예전 k8s 환경에서 Service(NodePort) 생성 시, TCP Port Listen 되었었음root@myk8s-control-plane:/# ss -4tlnp | egrep &quot;(Process|$NPORT)&quot;State Recv-Q Send-Q Local Address:Port Peer Address:PortProcessLISTEN 0 1024 127.0.0.11:35413 0.0.0.0:*LISTEN 0 4096 127.0.0.1:34021 0.0.0.0:* users:((&quot;containerd&quot;,pid=104,fd=10))LISTEN 0 4096 127.0.0.1:10248 0.0.0.0:* users:((&quot;kubelet&quot;,pid=692,fd=12))LISTEN 0 4096 172.18.0.4:2379 0.0.0.0:* users:((&quot;etcd&quot;,pid=638,fd=9))LISTEN 0 4096 127.0.0.1:2379 0.0.0.0:* users:((&quot;etcd&quot;,pid=638,fd=8))LISTEN 0 4096 172.18.0.4:2380 0.0.0.0:* users:((&quot;etcd&quot;,pid=638,fd=7))# 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)kubectl logs -l app=deploy-websrv -f# 외부 클라이언트(mypc 컨테이너)에서 접속 시도를 해보자# 노드의 IP와 NodePort를 변수에 지정## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;## NODE1=&lt;노드1의 IP주소&gt;## NODE2=&lt;노드2의 IP주소&gt;## NODE3=&lt;노드3의 IP주소&gt;CNODE=172.18.0.4NODE1=172.18.0.2NODE2=172.18.0.3NODE3=172.18.0.5NPORT=$(kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}')echo $NPORT# 서비스(NodePort) 부하분산 접속 확인docker exec -it mypc curl -s $CNODE:$NPORT | jq # headers.host 주소는 왜 그런거죠?for i in $CNODE $NODE1 $NODE2 $NODE3 ; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it mypc curl -s $i:$NPORT; echo; done# 컨트롤플레인 노드에는 목적지 파드가 없는데도, 접속을 받아준다! 이유는?docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $CNODE:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE1:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE2:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE3:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;# 아래 반복 접속 실행 해두자docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;# NodePort 서비스는 ClusterIP 를 포함# CLUSTER-IP:PORT 로 접속 가능! &lt;- 컨트롤노드에서 아래 실행 해보자kubectl get svc svc-nodeportNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc-nodeport NodePort 10.111.1.238 &lt;none&gt; 9000:30158/TCP 3m3sCIP=$(kubectl get service svc-nodeport -o jsonpath=&quot;{.spec.clusterIP}&quot;)CIPPORT=$(kubectl get service svc-nodeport -o jsonpath=&quot;{.spec.ports[0].port}&quot;)echo $CIP $CIPPORTdocker exec -it myk8s-control-plane curl -s $CIP:$CIPPORT | jq# mypc에서 CLUSTER-IP:PORT 로 접속 가능할까?docker exec -it mypc curl -s $CIP:$CIPPORT# (옵션) 노드에서 Network Connectionconntrack -Econntrack -L --any-nat# (옵션) 패킷 캡쳐 확인tcpdump.. IPTABLES 정책 확인 컨트롤플레인 노드 - iptables 분석 (핵심 정책 확인) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162docker exec -it myk8s-control-plane bash----------------------------------------# 패킷 카운트 초기화iptables -t nat --zeroPREROUTING 정보 확인iptables -t nat -S | grep PREROUTING-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES...# 외부 클라이언트가 노드IP:NodePort 로 접속하기 때문에 --dst-type LOCAL 에 매칭되어서 -j KUBE-NODEPORTS 로 점프!iptables -t nat -S | grep KUBE-SERVICES-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS...# KUBE-NODEPORTS 에서 KUBE-EXT-# 로 점프!## -m nfacct --nfacct-name localhost_nps_accepted_pkts 추가됨 : 패킷 flow 카운팅 - 카운트 이름 지정 NPORT=$(kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}')echo $NPORTiptables -t nat -S | grep KUBE-NODEPORTS | grep &lt;NodePort&gt;iptables -t nat -S | grep KUBE-NODEPORTS | grep $NPORT-A KUBE-NODEPORTS -d 127.0.0.0/8 -p tcp -m comment --comment &quot;default/svc-nodeport:svc-webport&quot; -m tcp --dport 30898 -m nfacct --nfacct-name localhost_nps_accepted_pkts -j KUBE-EXT-VTR7MTHHNMFZ3OFS-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/svc-nodeport:svc-webport&quot; -m tcp --dport 30898 -j KUBE-EXT-VTR7MTHHNMFZ3OFS# (참고) nfacct 확인nfacct list## nfacct flush # 초기화## KUBE-EXT-# 에서 'KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000' 마킹 및 KUBE-SVC-# 로 점프!# docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot; 반복 접속 후 아래 확인watch -d 'iptables -v --numeric --table nat --list KUBE-EXT-VTR7MTHHNMFZ3OFS'iptables -t nat -S | grep &quot;A KUBE-EXT-VTR7MTHHNMFZ3OFS&quot;-A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment &quot;masquerade traffic for default/svc-nodeport:svc-webport external destinations&quot; -j KUBE-MARK-MASQ-A KUBE-EXT-VTR7MTHHNMFZ3OFS -j KUBE-SVC-VTR7MTHHNMFZ3OFS# KUBE-SVC-# 이후 과정은 Cluster-IP 와 동일! : 3개의 파드로 DNAT 되어서 전달iptables -t nat -S | grep &quot;A KUBE-SVC-VTR7MTHHNMFZ3OFS -&quot;-A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment &quot;default/svc-nodeport:svc-webport&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-Q5ZOWRTVDPKGFLOL-A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment &quot;default/svc-nodeport:svc-webport&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-MMWCMKTGOFHFMRIZ-A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment &quot;default/svc-nodeport:svc-webport&quot; -j KUBE-SEP-CQTAHW4MAKGGR6M2POSTROUTING 정보 확인# 마킹되어 있어서 출발지IP를 접속한 노드의 IP 로 SNAT(MASQUERADE) 처리함! , 최초 출발지Port는 랜덤Port 로 변경iptables -t nat -S | grep &quot;A KUBE-POSTROUTING&quot;-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN # 0x4000/0x4000 되어 있으니 여기에 매칭되지 않고 아래 Rule로 내려감-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE --random-fully# docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot; 반복 접속 후 아래 확인watch -d 'iptables -v --numeric --table nat --list KUBE-POSTROUTING;echo;iptables -v --numeric --table nat --list POSTROUTING'exit---------------------------------------- 서비스(NodePort) 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가되는지 확인 1234567#NPORT=$(kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}')docker exec -it myk8s-control-plane iptables -t nat -S | grep KUBE-NODEPORTS | grep $NPORT...for i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i iptables -t nat -S | grep KUBE-NODEPORTS | grep $NPORT; echo; done... externalTrafficPolicy 설정externalTrafficPolicy: Local : NodePort 로 접속 시 해당 노드에 배치된 파드로만 접속됨, 이때 SNAT 되지 않아서 외부 클라이언트 IP가 보존됨! 클라이언트 가상머신에서 노드1의 ip에 NodePort로 접속할 때, 노드1에 배포된 파드1로 접속된다. 이후 반복접속 시도하면 노드1의 파드1로만 접속된다. externalTrafficPolicy 설정은 다른 노드에 배포된 파드로는 접속되지 않는다. 외부 클라이언트의 IP 주소(아래 출발지IP: 50.1.1.1)가 노드의 IP로 SNAT 되지 않고 서비스(backend) 파드까지 전달됨! externalTrafficPolicy:Local 설정 시 통신흐름 [1] client 에서 파드가 배포되어있는 노드1에 NodePort로 접속시도 [2] 노드에 iptables NAT 테이블 규칙과 매핑되어 목적지 IP와 목적지 Ports는 변환되지만 출발지 IP와 Port는 변환되지 않고 목적지 파드에 전달된다. (파드 입장에서 클라이언트 ip가 그대로 존재) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 기본 정보 확인kubectl get svc svc-nodeport -o json | grep 'TrafficPolicy&quot;' externalTrafficPolicy: Cluster internalTrafficPolicy: Cluster# 기존 통신 연결 정보(conntrack) 제거 후 아래 실습 진행하자! : (모든 노드에서) conntrack -Ffor i in control-plane worker worker2 worker3; do echo &quot;&gt;&gt; node myk8s-$i &lt;&lt;&quot;; docker exec -it myk8s-$i conntrack -F; echo; donekubectl delete -f svc-nodeport.yamlkubectl apply -f svc-nodeport.yaml# externalTrafficPolicy: local 설정 변경kubectl patch svc svc-nodeport -p '{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;: &quot;Local&quot;}}'kubectl get svc svc-nodeport -o json | grep 'TrafficPolicy&quot;' &quot;externalTrafficPolicy&quot;: &quot;Local&quot;, &quot;internalTrafficPolicy&quot;: &quot;Cluster&quot;,# 파드 3개를 2개로 줄임kubectl scale deployment deploy-echo --replicas=2# 파드 존재하는 노드 정보 확인kubectl get pod -owide# 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)kubectl logs -l app=deploy-websrv -f# 외부 클라이언트(mypc)에서 접속 시도# 노드의 IP와 NodePort를 변수에 지정## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;## NODE1=&lt;노드1의 IP주소&gt;## NODE2=&lt;노드2의 IP주소&gt;## NODE3=&lt;노드3의 IP주소&gt;CNODE=172.18.0.4NODE1=172.18.0.2NODE2=172.18.0.3NODE3=172.18.0.5## NodePort 를 변수에 지정 : 서비스를 삭제 후 다시 생성하여서 NodePort가 변경되었음NPORT=$(kubectl get service svc-nodeport -o jsonpath='{.spec.ports[0].nodePort}')echo $NPORT# 서비스(NodePort) 부하분산 접속 확인 : 파드가 존재하지 않는 노드로는 접속 실패!, 파드가 존재하는 노드는 접속 성공 및 클라이언트 IP 확인!docker exec -it mypc curl -s --connect-timeout 1 $CNODE:$NPORT | jqfor i in $CNODE $NODE1 $NODE2 $NODE3 ; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; docker exec -it mypc curl -s --connect-timeout 1 $i:$NPORT; echo; done# 목적지 파드가 배치되지 않은 노드는 접속이 어떻게? 왜 그런가?docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $CNODE:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE1:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE2:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;docker exec -it mypc zsh -c &quot;for i in {1..100}; do curl -s $NODE3:$NPORT | grep hostname; done | sort | uniq -c | sort -nr&quot;# 파드가 배치된 노드에 NodePort로 아래 반복 접속 실행 해두자docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $NODE1:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;혹은docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $NODE2:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;혹은docker exec -it mypc zsh -c &quot;while true; do curl -s --connect-timeout 1 $NODE3:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ; sleep 1; done&quot;# (옵션) 노드에서 Network Connectionconntrack -Econntrack -L --any-nat# 패킷 캡쳐 확인 서비스(NodePort) 부족한 점외부에서 노드의 IP와 포트로 직접 접속이 필요함 → 내부망이 외부에 공개(라우팅 가능)되어 보안에 취약함 ⇒ LoadBalancer 서비스 타입으로 외부 공개 최소화 가능! 클라이언트 IP 보존을 위해서, externalTrafficPolicy: local 사용 시 파드가 없는 노드 IP로 NodePort 접속 시 실패 ⇒ LoadBalancer 서비스에서 헬스체크(Probe) 로 대응 가능! 파드 간 속도 측정iperf3 : 서버 모드로 동작하는 단말과 클라이언트 모드로 동작하는 단말로 구성해서 최대 네트워크 대역폭 측정 - TCP, UDP, SCTP 지원 macos 123456789101112131415161718192021222324252627282930313233343536373839# iperf3 설치 brew install iperf3# iperf3 테스트 1 : TCP 5201, 측정시간 10초iperf3 -s # 서버모드 실행iperf3 -c 127.0.0.1 # 클라이언트모드 실행# iperf3 테스트 2 : TCP 80, 측정시간 5초iperf3 -s -p 80iperf3 -c 127.0.0.1 -p 80 -t 5-----------------------------------------------------------Server listening on 80 (test #1)-----------------------------------------------------------Accepted connection from 127.0.0.1, port 58402[ 5] local 127.0.0.1 port 80 connected to 127.0.0.1 port 58403[ ID] Interval Transfer Bitrate[ 5] 0.00-1.00 sec 12.8 GBytes 110 Gbits/sec[ 5] 1.00-2.00 sec 12.8 GBytes 110 Gbits/sec[ 5] 2.00-3.00 sec 13.3 GBytes 114 Gbits/sec[ 5] 3.00-4.01 sec 14.6 GBytes 125 Gbits/sec[ 5] 4.01-5.00 sec 13.5 GBytes 116 Gbits/sec[ 5] 5.00-5.00 sec 20.9 MBytes 128 Gbits/sec# iperf3 테스트 3 : UDP 사용, 역방향 모드(-R)iperf3 -s iperf3 -c 127.0.0.1 -u -b 100G# iperf3 테스트 4 : 역방향 모드(-R)iperf3 -s iperf3 -c 127.0.0.1 -R# iperf3 테스트 5 : 쌍방향 모드(-R)iperf3 -s iperf3 -c 127.0.0.1 --bidir# iperf3 테스트 6 : TCP 다중 스트림(30개), -P(number of parallel client streams to run)iperf3 -s iperf3 -c 127.0.0.1 -P 2 -t 30 k8s 속도 측정 12345678# 배포kubectl apply -f https://raw.githubusercontent.com/gasida/PKOS/main/aews/k8s-iperf3.yaml# 확인 : 서버와 클라이언트가 다른 워커노드에 배포되었는지 확인kubectl get deploy,svc,pod -owide# 서버 파드 로그 확인 : 기본 5201 포트 Listenkubectl logs -l app=iperf3-server -f TCP 5201, 측정시간 5초 1234567891011121314151617181920# 클라이언트 파드에서 아래 명령 실행kubectl exec -it deploy/**iperf3-client** -- **iperf3 -c iperf3-server -t 5**# 서버 파드 로그 확인 : 기본 5201 포트 Listenkubectl logs -l **app=iperf3-server** -fServer listening on 5201 (test #1)-----------------------------------------------------------Accepted connection from 10.10.2.7, port 55190[ 5] local 10.10.1.6 port 5201 connected to 10.10.2.7 port 55192[ ID] Interval Transfer Bitrate[ 5] 0.00-1.00 sec 4.70 GBytes 40.4 Gbits/sec[ 5] 1.00-2.00 sec 5.11 GBytes 43.9 Gbits/sec[ 5] 2.00-3.00 sec 4.94 GBytes 42.4 Gbits/sec[ 5] 3.00-4.00 sec 5.07 GBytes 43.6 Gbits/sec[ 5] 4.00-5.00 sec 4.93 GBytes 42.4 Gbits/sec[ 5] 5.00-5.00 sec 1.75 MBytes 40.1 Gbits/sec- - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval Transfer Bitrate[ 5] 0.00-5.00 sec 24.8 GBytes 42.5 Gbits/sec receiver UDP 사용, 역방향 모드(-R) 12345# 클라이언트 파드에서 아래 명령 실행kubectl exec -it deploy/**iperf3-client** -- **iperf3 -c iperf3-server -u -b 20G**# 서버 파드 로그 확인 : 기본 5201 포트 Listenkubectl logs -l **app=iperf3-server** -f TCP, 쌍방향 모드(-R) 12345# 클라이언트 파드에서 아래 명령 실행kubectl exec -it deploy/**iperf3-client** -- **iperf3 -c iperf3-server -t 5 --bidir**# 서버 파드 로그 확인 : 기본 5201 포트 Listenkubectl logs -l **app=iperf3-server** -f TCP 다중 스트림(30개), -P(number of parallel client streams to run) 12345# 클라이언트 파드에서 아래 명령 실행kubectl exec -it deploy/**iperf3-client** -- **iperf3 -c iperf3-server -t 10 -P 2**# 서버 파드 로그 확인 : 기본 5201 포트 Listenkubectl logs -l **app=iperf3-server** -f https://seongtaekkim.github.io/blog/categories/serivce-clusterip/ https://seongtaekkim.github.io/blog/categories/serivce-nodeport/","link":"/blog/2024/09/29/docs/service/nodeport/"},{"title":"vxlan concept","text":"VxLAN은 VLAN을 확장한 개념으로 기존의 VLAN에서 만들 수 있는 네트워크보다 훨씬 더 많은 네트워크를 생성할 수 있다. VxLAN에서 x는 eXtensible으로 이름에서도 확장을 나타내고 있다. VLAN은 이더넷 프레임에 16bit(vlan, option, id)를 추가로 구성하여 Tag를 기반으로 동작하는데 이때 id로 사용할 수 있는 비트가 12bit이기 때문에 만들 수 있는 네트워크가 최대 4096개만 생성할 수 있었다. VxLAN은 VLAN의 문제를 해결하기 위해 50byte 헤더(Mac over IP, UDP Header, 24bit VLAN ID)를 추가로 구성하여 16,000,000개 이상의 VLAN을 제공할 수 있다. VNI (virtual Network Identifier) VLAN과 VxLAN segment간 mapping 구분자 같은 VLAN에 대해 여러 개의 VNI mapping이 가능하다. VTEP ( VXLAN Tunnel End Point ) VXLAN Tunnel 종단 역할을 수행한다 인캡슐레이션과 터미네이션 의 end point 역활을 수행한다. Original L2 frame 이외의 모든 헤더가 VXLAN 관련 헤더이다. VXLAN 헤더 에는 VNI 라는 24비트 VLAN 을 생성할 수 있다. 일반적인 VLAN ID 가 16비트의 기반이라면, VXLAN의 VNI 에서 표현되는 VLAN 은 24비트의 기반으로, 16,000,000 개의 VLAN 을 생성 할 수 있다. L2 over L3 : UDP 패킷 내부에 L2 프레임을 캡슐화 하는 터널링 기술 Mac in UDP VXLAN 패킷 사이즈 : 50byte VXLAN L2 정보 (MAC address)를 L3에 넣어서 통신 MTU 원래는 1500byte 를 사용하는데 VXLAN 패킷에 50byte를 썼으므로 1450byte 를 세팅해서 사용하도록 MTU를 세팅한다. referencehttps://skstp35.tistory.com/227 https://atthis.tistory.com/7?category=750237","link":"/blog/2024/11/24/docs/overlaynetwork/vxlan/"},{"title":"vpccni","text":"AWS VPC CNI 소개기본정보 확인 1234567891011121314151617181920212223242526272829303132# CNI 정보 확인kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d &quot;/&quot; -f 2# kube-proxy config 확인 : 모드 iptables 사용 &gt;&gt; ipvs 모드 사용하지 않는 이유???kubectl describe cm -n kube-system kube-proxy-config...mode: &quot;iptables&quot;...# 노드 IP 확인aws ec2 describe-instances --query &quot;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}&quot; --filters Name=instance-state-name,Values=running --output table-----------------------------------------------------------------| DescribeInstances |+----------------+-----------------+-----------------+----------+| InstanceName | PrivateIPAdd | PublicIPAdd | Status |+----------------+-----------------+-----------------+----------+| myeks-ng1-Node| 192.168.2.165 | 54.181.2.113 | running || myeks-ng1-Node| 192.168.3.238 | 3.35.49.101 | running || myeks-bastion | 192.168.1.100 | 52.79.247.213 | running || myeks-ng1-Node| 192.168.1.220 | 3.36.114.42 | running |+----------------+-----------------+-----------------+----------+# 파드 IP 확인kubectl get pod -n kube-system -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase# 파드 이름 확인kubectl get pod -A -o name# 파드 갯수 확인kubectl get pod -A -o name | wc -l 1234567891011121314# CNI 정보 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i tree /var/log/aws-routed-eni; echo; donessh ec2-user@$N1 sudo cat /var/log/aws-routed-eni/plugin.log | jqssh ec2-user@$N1 sudo cat /var/log/aws-routed-eni/ipamd.log | jqssh ec2-user@$N1 sudo cat /var/log/aws-routed-eni/egress-v6-plugin.log | jqssh ec2-user@$N1 sudo cat /var/log/aws-routed-eni/ebpf-sdk.log | jqssh ec2-user@$N1 sudo cat /var/log/aws-routed-eni/network-policy-agent.log | jq# 네트워크 정보 확인 : eniY는 pod network 네임스페이스와 veth pairfor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo ip -br -c addr; echo; donefor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo ip -c addr; echo; donefor i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo ip -c route; echo; donessh ec2-user@$N1 sudo iptables -t nat -Sssh ec2-user@$N1 sudo iptables -t nat -L -n -v 노드에서 기본 네트워크 정보 확인워커 노드1 기본 네트워크 구성 Network 네임스페이스는 호스트(Root)와 파드 별(Per Pod)로 구분된다 특정한 파드(kube-proxy, aws-node)는 호스트(Root)의 IP를 그대로 사용한다 ⇒ 파드의 Host Network 옵션 t3.medium 의 경우 ENI 마다 최대 6개의 IP를 가질 수 있다 ENI0, ENI1 으로 2개의 ENI는 자신의 IP 이외에 추가적으로 5개의 보조 프라이빗 IP를 가질수 있다 coredns 파드는 veth 으로 호스트에는 eniY@ifN 인터페이스와 파드에 eth0 과 연결되어 보조 IPv4 주소를 파드가 사용하는지 확인12345678# coredns 파드 IP 정보 확인kubectl get pod -n kube-system -l k8s-app=kube-dns -owideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-699d8c5988-pzsvc 1/1 Running 0 19m 192.168.1.181 ip-192-168-1-220.ap-northeast-2.compute.internal &lt;none&gt; &lt;none&gt;coredns-699d8c5988-tcktg 1/1 Running 0 19m 192.168.3.153 ip-192-168-3-238.ap-northeast-2.compute.internal &lt;none&gt; &lt;none&gt;# 노드의 라우팅 정보 확인 &gt;&gt; EC2 네트워크 정보의 '보조 프라이빗 IPv4 주소'와 비교해보자for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo ip -c route; echo; done 테스트용 파드1234567891011121314151617181920212223242526272829303132333435# 테스트용 파드 netshoot-pod 생성cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: netshoot-podspec: replicas: 3 selector: matchLabels: app: netshoot-pod template: metadata: labels: app: netshoot-pod spec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [&quot;tail&quot;] args: [&quot;-f&quot;, &quot;/dev/null&quot;] terminationGracePeriodSeconds: 0EOF# 파드 이름 변수 지정PODNAME1=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[0].metadata.name})PODNAME2=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[1].metadata.name})PODNAME3=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[2].metadata.name})# 파드 확인kubectl get pod -o widekubectl get pod -o=custom-columns=NAME:.metadata.name,IP:.status.podIP# 노드에 라우팅 정보 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo ip -c route; echo; done 파드가 생성되면, 워커 노드에 eniY@ifN 추가되고 라우팅 테이블에도 정보가 추가된다 테스트용 파드 eniY 정보 확인 - 워커 노드 EC2 1234567891011121314151617181920212223242526272829303132333435363738# 노드3에서 네트워크 인터페이스 정보 확인ssh ec2-user@$N3----------------ip -br -c addr showip -c linkip -c addrip route # 혹은 route -n# 마지막 생성된 네임스페이스 정보 출력 -t net(네트워크 타입)sudo lsns -o PID,COMMAND -t net | awk 'NR&gt;2 {print $1}' | tail -n 1# 마지막 생성된 네임스페이스 net PID 정보 출력 -t net(네트워크 타입)를 변수 지정MyPID=$(sudo lsns -o PID,COMMAND -t net | awk 'NR&gt;2 {print $1}' | tail -n 1)# PID 정보로 파드 정보 확인sudo nsenter -t $MyPID -n ip -c addrsudo nsenter -t $MyPID -n ip -c routeexit----------------[ec2-user@ip-192-168-3-238 ~]$ sudo nsenter -t $MyPID -n ip -c addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever3: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP group default link/ether 76:f6:91:af:2c:27 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.3.138/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::74f6:91ff:feaf:2c27/64 scope link valid_lft forever preferred_lft forever[ec2-user@ip-192-168-3-238 ~]$ sudo nsenter -t $MyPID -n ip -c routedefault via 169.254.1.1 dev eth0169.254.1.1 dev eth0 scope link 테스트용 파드 접속(exec) 후 확인 123456789101112131415161718192021222324252627282930# 테스트용 파드 접속(exec) 후 Shell 실행kubectl exec -it $PODNAME1 -- zsh# 아래부터는 pod-1 Shell 에서 실행 : 네트워크 정보 확인----------------------------ip -c addrip -c routeroute -nping -c 1 &lt;pod-2 IP&gt;pscat /etc/resolv.confexit---------------------------- dP dP dP 88 88 8888d888b. .d8888b. d8888P .d8888b. 88d888b. .d8888b. .d8888b. d8888P88' `88 88ooood8 88 Y8ooooo. 88' `88 88' `88 88' `88 8888 88 88. ... 88 88 88 88 88. .88 88. .88 88dP dP `88888P' dP `88888P' dP dP `88888P' `88888P' dPWelcome to Netshoot! (github.com/nicolaka/netshoot)Version: 0.13# 파드2 Shell 실행kubectl exec -it $PODNAME2 -- ip -c addr# 파드3 Shell 실행kubectl exec -it $PODNAME3 -- ip -br -c addr 노드 간 파드 통신 파드간 통신 시 tcpdump 내용을 확인하고 통신 과정을 알아본다 파드간 통신 흐름 : AWS VPC CNI 경우 별도의 오버레이(Overlay) 통신 기술 없이, VPC Native 하게 파드간 직접 통신이 가능하다 파드간 통신 테스트 및 확인12345678910111213141516171819202122232425262728293031323334# 파드 IP 변수 지정PODIP1=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[0].status.podIP})PODIP2=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[1].status.podIP})PODIP3=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[2].status.podIP})# 파드1 Shell 에서 파드2로 ping 테스트kubectl exec -it $PODNAME1 -- ping -c 2 $PODIP2# 파드2 Shell 에서 파드3로 ping 테스트kubectl exec -it $PODNAME2 -- ping -c 2 $PODIP3# 파드3 Shell 에서 파드1로 ping 테스트kubectl exec -it $PODNAME3 -- ping -c 2 $PODIP1**# 워커 노드 EC2 :** TCPDUMP 확인## For Pod to external (outside VPC) traffic, we will program iptables to SNAT using Primary IP address on the Primary ENI.sudo tcpdump -i **any** -nn icmpsudo tcpdump -i **eth1** -nn icmpsudo tcpdump -i **eth0** -nn icmpsudo tcpdump -i **eniYYYYYYYY** -nn icmp[워커 노드1]# routing policy database management 확인ip rule# routing table management 확인ip route show table local# 디폴트 네트워크 정보를 eth0 을 통해서 빠져나간다# ip route show table maindefault via 192.168.1.1 dev eth0169.254.169.254 dev eth0172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.100 파드에서 외부 통신파드에서 외부 통신 흐름 : iptable 에 SNAT 을 통하여 노드의 eth0 IP로 변경되어서 외부와 통신됨 VPC CNI 의 External source network address translation (SNAT) 설정에 따라, 외부(인터넷) 통신 시 SNAT 하거나 혹은 SNAT 없이 통신을 할 수 있다 - 링크 파드에서 외부 통신 테스트 및 확인 파드 shell 실행 후 외부로 ping 테스트 &amp; 워커 노드에서 tcpdump 및 iptables 정보 확인 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 작업용 EC2 : pod-1 Shell 에서 외부로 pingkubectl exec -it $PODNAME1 -- ping -c 1 www.google.comkubectl exec -it $PODNAME1 -- ping -i 0.1 www.google.com# 워커 노드 EC2 : TCPDUMP 확인sudo tcpdump -i any -nn icmpsudo tcpdump -i eth0 -nn icmp# 작업용 EC2 : 퍼블릭IP 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i curl -s ipinfo.io/ip; echo; echo; done# 작업용 EC2 : pod-1 Shell 에서 외부 접속 확인 - 공인IP는 어떤 주소인가?## The right way to check the weather - 링크for i in $PODNAME1 $PODNAME2 $PODNAME3; do echo &quot;&gt;&gt; Pod : $i &lt;&lt;&quot;; kubectl exec -it $i -- curl -s ipinfo.io/ip; echo; echo; donekubectl exec -it $PODNAME1 -- curl -s wttr.in/seoulWeather report: seoul \\ / Clear .-. 13 °C ― ( ) ― ↙ 4 km/h `-’ 10 km / \\ 0.0 mm kubectl exec -it $PODNAME1 -- curl -s wttr.in/seoul?format=3kubectl exec -it $PODNAME1 -- curl -s wttr.in/Moonkubectl exec -it $PODNAME1 -- curl -s wttr.in/:help# 워커 노드 EC2## 출력된 결과를 보고 어떻게 빠져나가는지 고민해보자!ip ruleip route show table mainsudo iptables -L -n -v -t natsudo iptables -t nat -S# 파드가 외부와 통신시에는 아래 처럼 'AWS-SNAT-CHAIN-0' 룰(rule)에 의해서 SNAT 되어서 외부와 통신!# 참고로 뒤 IP는 eth0(ENI 첫번째)의 IP 주소이다# --random-fully 동작 - 링크1 링크2sudo iptables -t nat -S | grep 'A AWS-SNAT-CHAIN'-A AWS-SNAT-CHAIN-0 ! -d 192.168.0.0/16 -m comment --comment &quot;AWS SNAT CHAIN&quot; -j RETURN-A AWS-SNAT-CHAIN-0 ! -o vlan+ -m comment --comment &quot;AWS, SNAT&quot; -m addrtype ! --dst-type LOCAL -j SNAT --to-source 192.168.1.251 --random-fully## 아래 'mark 0x4000/0x4000' 매칭되지 않아서 RETURN 됨!-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE --random-fully...# 카운트 확인 시 AWS-SNAT-CHAIN-0에 매칭되어, 목적지가 192.168.0.0/16 아니고 외부 빠져나갈때 SNAT 192.168.1.251(EC2 노드1 IP) 변경되어 나간다!sudo iptables -t filter --zero; sudo iptables -t nat --zero; sudo iptables -t mangle --zero; sudo iptables -t raw --zerowatch -d 'sudo iptables -v --numeric --table nat --list AWS-SNAT-CHAIN-0; echo ; sudo iptables -v --numeric --table nat --list KUBE-POSTROUTING; echo ; sudo iptables -v --numeric --table nat --list POSTROUTING'# conntrack 확인for i in $N1 $N2 $N3; do echo &quot;&gt;&gt; node $i &lt;&lt;&quot;; ssh ec2-user@$i sudo conntrack -L -n |grep -v '169.254.169'; echo; doneconntrack v1.4.5 (conntrack-tools): icmp 1 28 src=172.30.66.58 dst=8.8.8.8 type=8 code=0 id=34392 src=8.8.8.8 dst=172.30.85.242 type=0 code=0 id=50705 mark=128 use=1tcp 6 23 TIME_WAIT src=172.30.66.58 dst=34.117.59.81 sport=58144 dport=80 src=34.117.59.81 dst=172.30.85.242 sport=80 dport=44768 [ASSURED] mark=128 use=1 자원 삭제 1kubectl delete deploy netshoot-pod 노드에 파드 생성 갯수 제한사전 준비 : kube-ops-view 설치 123456# kube-ops-viewhelm repo add geek-cookbook https://geek-cookbook.github.io/charts/helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=LoadBalancer --set env.TZ=&quot;Asia/Seoul&quot; --namespace kube-system# kube-ops-view 접속 URL 확인 (1.5 배율)kubectl get svc -n kube-system kube-ops-view -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print &quot;KUBE-OPS-VIEW URL = http://&quot;$1&quot;:8080/#scale=1.5&quot;}' 워커 노드의 인스턴스 타입 별 파드 생성 갯수 제한 인스턴스 타입 별 ENI 최대 갯수와 할당 가능한 최대 IP 갯수에 따라서 파드 배치 갯수가 결정됨 단, aws-node 와 kube-proxy 파드는 호스트의 IP를 사용함으로 최대 갯수에서 제외함 123456789101112131415161718192021222324252627282930313233343536# t3 타입의 정보(필터) 확인aws ec2 describe-instance-types --filters Name=instance-type,Values=t3.* \\ --query &quot;InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}&quot; \\ --output table--------------------------------------| DescribeInstanceTypes |+----------+----------+--------------+| IPv4addr | MaxENI | Type |+----------+----------+--------------+| 15 | 4 | t3.2xlarge || 6 | 3 | t3.medium || 12 | 3 | t3.large || 15 | 4 | t3.xlarge || 2 | 2 | t3.micro || 2 | 2 | t3.nano || 4 | 3 | t3.small |+----------+----------+--------------+# c5 타입의 정보(필터) 확인aws ec2 describe-instance-types --filters Name=instance-type,Values=c5*.* \\ --query &quot;InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}&quot; \\ --output table# 파드 사용 가능 계산 예시 : aws-node 와 kube-proxy 파드는 host-networking 사용으로 IP 2개 남음((MaxENI * (IPv4addr-1)) + 2)t3.medium 경우 : ((3 * (6 - 1) + 2 ) = 17개 &gt;&gt; aws-node 와 kube-proxy 2개 제외하면 15개# 워커노드 상세 정보 확인 : 노드 상세 정보의 Allocatable 에 pods 에 17개 정보 확인kubectl describe node | grep Allocatable: -A6Allocatable: cpu: 1930m ephemeral-storage: 27905944324 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 3388360Ki pods: 17 최대 파드 생성 및 확인 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 워커 노드 EC2 - 모니터링while true; do ip -br -c addr show &amp;&amp; echo &quot;--------------&quot; ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; sleep 1; done# 작업용 EC2 - 터미널1watch -d 'kubectl get pods -o wide'# 작업용 EC2 - 터미널2# 디플로이먼트 생성curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/2/nginx-dp.yamlkubectl apply -f nginx-dp.yaml# 파드 확인kubectl get pod -o widekubectl get pod -o=custom-columns=NAME:.metadata.name,IP:.status.podIP# 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인kubectl scale deployment nginx-deployment --replicas=8# 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?kubectl scale deployment nginx-deployment --replicas=15# 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?kubectl scale deployment nginx-deployment --replicas=30# 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?kubectl scale deployment nginx-deployment --replicas=50# 파드 생성 실패!(Administrator@myeks:N/A) [root@myeks-bastion ~]# k get pod | grep Pendingnginx-deployment-6f999cfffb-5j7s4 0/1 Pending 0 22snginx-deployment-6f999cfffb-9q67j 0/1 Pending 0 22snginx-deployment-6f999cfffb-hknjj 0/1 Pending 0 22snginx-deployment-6f999cfffb-l7l6d 0/1 Pending 0 22snginx-deployment-6f999cfffb-mz592 0/1 Pending 0 22snginx-deployment-6f999cfffb-rspw4 0/1 Pending 0 22snginx-deployment-6f999cfffb-rxpzj 0/1 Pending 0 22snginx-deployment-6f999cfffb-vv8bm 0/1 Pending 0 22s(Administrator@myeks:N/A) [root@myeks-bastion ~]# k describe pod nginx-deployment-6f999cfffb-5j7s4 | grep Events: -A5Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 60s default-scheduler 0/3 nodes are available: 3 Too many pods. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.# 디플로이먼트 삭제kubectl delete deploy nginx-deployment 해결 방안 : Prefix Delegation, WARM &amp; MIN IP/Prefix Targets, Custom Network Service &amp; AWS LoadBalancer Controller 서비스 종류 ClusterIP 타입 NodePort 타입 LoadBalancer 타입 (기본 모드) : NLB 인스턴스 유형 Service (LoadBalancer Controller) : AWS Load Balancer Controller + NLB IP 모드 동작 with AWS VPC CNI 1NLB 모드 전체 정리 인스턴스 유형 externalTrafficPolicy : ClusterIP ⇒ 2번 분산 및 SNAT으로 Client IP 확인 불가능 ← LoadBalancer 타입 (기본 모드) 동작 externalTrafficPolicy : Local ⇒ 1번 분산 및 ClientIP 유지, 워커 노드의 iptables 사용함 상세 설명 통신 흐름 요약 : 외부 클라이언트가 ‘로드밸런서’ 접속 시 부하분산 되어 노드 도달 후 iptables 룰로 목적지 파드와 통신됨 노드는 외부에 공개되지 않고 로드밸런서만 외부에 공개되어, 외부 클라이언트는 로드밸랜서에 접속을 할 뿐 내부 노드의 정보를 알 수 없다 로드밸런서가 부하분산하여 파드가 존재하는 노드들에게 전달한다, iptables 룰에서는 자신의 노드에 있는 파드만 연결한다 (externalTrafficPolicy: local) DNAT 2번 동작 : 첫번째(로드밸런서 접속 후 빠져 나갈때), 두번째(노드의 iptables 룰에서 파드IP 전달 시) 외부 클라이언트 IP 보존(유지) : AWS NLB 는 타켓이 인스턴스일 경우 클라이언트 IP를 유지, iptables 룰 경우도 externalTrafficPolicy 로 클라이언트 IP를 보존 부하분산 최적화 : 노드에 파드가 없을 경우 ‘로드밸런서’에서 노드에 헬스 체크(상태 검사)가 실패하여 해당 노드로는 외부 요청 트래픽을 전달하지 않는다 3번째 인스턴스(Node3)은 상태 확인 실패로 외부 요청 트래픽 전달하지 않는다 IP 유형 ⇒ 반드시 AWS LoadBalancer 컨트롤러 파드 및 정책 설정이 필요함! Proxy Protocol v2 비활성화 ⇒ NLB에서 바로 파드로 인입, 단 ClientIP가 NLB로 SNAT 되어 Client IP 확인 불가능 Proxy Protocol v2 활성화 ⇒ NLB에서 바로 파드로 인입 및 ClientIP 확인 가능(→ 단 PPv2 를 애플리케이션이 인지할 수 있게 설정 필요) AWS LoadBalancer Controller 배포 - Link 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Helm Chart 설치helm repo add eks https://aws.github.io/eks-chartshelm repo updatehelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME## 설치 확인kubectl get crdkubectl get deployment -n kube-system aws-load-balancer-controllerkubectl describe deploy -n kube-system aws-load-balancer-controllerkubectl describe deploy -n kube-system aws-load-balancer-controller | grep 'Service Account' Service Account: aws-load-balancer-controller # 클러스터롤, 롤 확인kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebindingkubectl describe clusterroles.rbac.authorization.k8s.io aws-load-balancer-controller-role...PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- targetgroupbindings.elbv2.k8s.aws [] [] [create delete get list patch update watch] events [] [] [create patch] ingresses [] [] [get list patch update watch] services [] [] [get list patch update watch] ingresses.extensions [] [] [get list patch update watch] services.extensions [] [] [get list patch update watch] ingresses.networking.k8s.io [] [] [get list patch update watch] services.networking.k8s.io [] [] [get list patch update watch] endpoints [] [] [get list watch] namespaces [] [] [get list watch] nodes [] [] [get list watch] pods [] [] [get list watch] endpointslices.discovery.k8s.io [] [] [get list watch] ingressclassparams.elbv2.k8s.aws [] [] [get list watch] ingressclasses.networking.k8s.io [] [] [get list watch] ingresses/status [] [] [update patch] pods/status [] [] [update patch] services/status [] [] [update patch] targetgroupbindings/status [] [] [update patch] ingresses.elbv2.k8s.aws/status [] [] [update patch] pods.elbv2.k8s.aws/status [] [] [update patch] services.elbv2.k8s.aws/status [] [] [update patch] targetgroupbindings.elbv2.k8s.aws/status [] [] [update patch] ingresses.extensions/status [] [] [update patch] pods.extensions/status [] [] [update patch] services.extensions/status [] [] [update patch] targetgroupbindings.extensions/status [] [] [update patch] ingresses.networking.k8s.io/status [] [] [update patch] pods.networking.k8s.io/status [] [] [update patch] services.networking.k8s.io/status [] [] [update patch] targetgroupbindings.networking.k8s.io/status [] [] [update patch] 서비스/파드 배포 테스트 with NLB - 링크 NLB 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 모니터링watch -d kubectl get pod,svc,ep# 작업용 EC2 - 디플로이먼트 &amp; 서비스 생성curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/2/echo-service-nlb.yamlcat echo-service-nlb.yamlkubectl apply -f echo-service-nlb.yaml# 확인kubectl get deploy,podkubectl get svc,ep,ingressclassparams,targetgroupbindingskubectl get targetgroupbindings -o json | jq# (옵션) 빠른 실습을 위해서 등록 취소 지연(드레이닝 간격) 수정 : 기본값 300초vi echo-service-nlb.yaml..apiVersion: v1kind: Servicemetadata: name: svc-nlb-ip-type annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: &quot;8080&quot; service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: &quot;true&quot; service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: deregistration_delay.timeout_seconds=60...:wq!kubectl apply -f echo-service-nlb.yaml# AWS ELB(NLB) 정보 확인aws elbv2 describe-load-balancers | jqaws elbv2 describe-load-balancers --query 'LoadBalancers[*].State.Code' --output textALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-default-svcnlbip`) == `true`].LoadBalancerArn' | jq -r '.[0]')aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jqTARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN | jq{ &quot;TargetHealthDescriptions&quot;: [ { &quot;Target&quot;: { &quot;Id&quot;: &quot;192.168.2.153&quot;, &quot;Port&quot;: 8080, &quot;AvailabilityZone&quot;: &quot;ap-northeast-2b&quot; }, &quot;HealthCheckPort&quot;: &quot;8080&quot;, &quot;TargetHealth&quot;: { &quot;State&quot;: &quot;initial&quot;, &quot;Reason&quot;: &quot;Elb.RegistrationInProgress&quot;, &quot;Description&quot;: &quot;Target registration is in progress&quot; } },...# 웹 접속 주소 확인kubectl get svc svc-nlb-ip-type -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print &quot;Pod Web URL = http://&quot;$1 }'# 파드 로깅 모니터링kubectl logs -l app=deploy-websrv -f# 분산 접속 확인NLB=$(kubectl get svc svc-nlb-ip-type -o jsonpath={.status.loadBalancer.ingress[0].hostname})curl -s $NLBfor i in {1..100}; do curl -s $NLB | grep Hostname ; done | sort | uniq -c | sort -nr 52 Hostname: deploy-echo-55456fc798-2w65p 48 Hostname: deploy-echo-55456fc798-cxl7z# 지속적인 접속 시도 : 아래 상세 동작 확인 시 유용(패킷 덤프 등)while true; do curl -s --connect-timeout 1 $NLB | egrep 'Hostname|client_address'; echo &quot;----------&quot; ; date &quot;+%Y-%m-%d %H:%M:%S&quot; ; sleep 1; done AWS NLB의 대상 그룹 확인 : IP를 확인해보자 파드 2개 → 1개 → 3개 설정 시 동작 : auto discovery ← 어떻게 가능할까? 12345678910111213141516171819202122232425262728# (신규 터미널) 모니터링while true; do aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN --output text; echo; done# 작업용 EC2 - 파드 1개 설정 kubectl scale deployment deploy-echo --replicas=1# 확인kubectl get deploy,pod,svc,epcurl -s $NLBfor i in {1..100}; do curl -s --connect-timeout 1 $NLB | grep Hostname ; done | sort | uniq -c | sort -nr# 작업용 EC2 - 파드 3개 설정 kubectl scale deployment deploy-echo --replicas=3# 확인 : NLB 대상 타켓이 아직 initial 일 때 100번 반복 접속 시 어떻게 되는지 확인해보자!kubectl get deploy,pod,svc,epcurl -s $NLBfor i in {1..100}; do curl -s --connect-timeout 1 $NLB | grep Hostname ; done | sort | uniq -c | sort -nr# kubectl describe deploy -n kube-system aws-load-balancer-controller | grep -i 'Service Account' Service Account: aws-load-balancer-controller# [AWS LB Ctrl] 클러스터 롤 바인딩 정보 확인kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebinding# [AWS LB Ctrl] 클러스터롤 확인 kubectl describe clusterroles.rbac.authorization.k8s.io aws-load-balancer-controller-role Ingress인그레스 소개 : 클러스터 내부의 서비스(ClusterIP, NodePort, Loadbalancer)를 외부로 노출(HTTP/HTTPS) - Web Proxy 역할 AWS Load Balancer Controller + Ingress (ALB) IP 모드 동작 with AWS VPC CNI 서비스/파드 배포 테스트 with Ingress(ALB) - ALB 1234567891011121314151617181920212223242526272829# 게임 파드와 Service, Ingress 배포curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/ingress1.yamlcat ingress1.yamlkubectl apply -f ingress1.yaml# 모니터링watch -d kubectl get pod,ingress,svc,ep -n game-2048# 생성 확인kubectl get-all -n game-2048kubectl get ingress,svc,ep,pod -n game-2048kubectl get targetgroupbindings -n game-2048# ALB 생성 확인aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-game2048`) == `true`]' | jqALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-game2048`) == `true`].LoadBalancerArn' | jq -r '.[0]')aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARNTARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN | jq# Ingress 확인kubectl describe ingress -n game-2048 ingress-2048kubectl get ingress -n game-2048 ingress-2048 -o jsonpath=&quot;{.status.loadBalancer.ingress[*].hostname}{'\\n'}&quot;# 게임 접속 : ALB 주소로 웹 접속kubectl get ingress -n game-2048 ingress-2048 -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print &quot;Game URL = http://&quot;$1 }'# 파드 IP 확인kubectl get pod -n game-2048 -owide ALB 대상 그룹에 등록된 대상 확인 : ALB에서 파드 IP로 직접 전달 파드 3개로 증가 123456# 터미널1watch kubectl get pod -n game-2048while true; do aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN --output text; echo; done# 터미널2 : 파드 3개로 증가kubectl scale deployment -n game-2048 deployment-2048 --replicas 3 파드 1개로 감소 12# 터미널2 : 파드 1개로 감소kubectl scale deployment -n game-2048 deployment-2048 --replicas 1 실습 리소스 삭제 12kubectl delete ingress ingress-2048 -n game-2048kubectl delete svc service-2048 -n game-2048 &amp;&amp; kubectl delete deploy deployment-2048 -n game-2048 &amp;&amp; kubectl delete ns game-2048","link":"/blog/2024/11/03/docs/vpccni/vpccni/"}],"tags":[],"categories":[{"name":"k8s&#x2F;kind","slug":"k8s-kind","link":"/blog/categories/k8s-kind/"},{"name":"container&#x2F;cgroup","slug":"container-cgroup","link":"/blog/categories/container-cgroup/"},{"name":"k8s&#x2F;flannel","slug":"k8s-flannel","link":"/blog/categories/k8s-flannel/"},{"name":"calico&#x2F;concept","slug":"calico-concept","link":"/blog/categories/calico-concept/"},{"name":"gateway&#x2F;gatewayapi","slug":"gateway-gatewayapi","link":"/blog/categories/gateway-gatewayapi/"},{"name":"calico&#x2F;networkmode","slug":"calico-networkmode","link":"/blog/categories/calico-networkmode/"},{"name":"gateway&#x2F;gloo","slug":"gateway-gloo","link":"/blog/categories/gateway-gloo/"},{"name":"calico&#x2F;communication","slug":"calico-communication","link":"/blog/categories/calico-communication/"},{"name":"k8s&#x2F;PAUSE-Container","slug":"k8s-PAUSE-Container","link":"/blog/categories/k8s-PAUSE-Container/"},{"name":"istio&#x2F;bookinfo","slug":"istio-bookinfo","link":"/blog/categories/istio-bookinfo/"},{"name":"istio&#x2F;istio-traffic","slug":"istio-istio-traffic","link":"/blog/categories/istio-istio-traffic/"},{"name":"istio&#x2F;istio-outline","slug":"istio-istio-outline","link":"/blog/categories/istio-istio-outline/"},{"name":"istio&#x2F;envoy","slug":"istio-envoy","link":"/blog/categories/istio-envoy/"},{"name":"container&#x2F;namespace","slug":"container-namespace","link":"/blog/categories/container-namespace/"},{"name":"istio&#x2F;istio-test","slug":"istio-istio-test","link":"/blog/categories/istio-istio-test/"},{"name":"java&#x2F;Enumeration","slug":"java-Enumeration","link":"/blog/categories/java-Enumeration/"},{"name":"java&#x2F;flyweight","slug":"java-flyweight","link":"/blog/categories/java-flyweight/"},{"name":"java&#x2F;interface","slug":"java-interface","link":"/blog/categories/java-interface/"},{"name":"cilium&#x2F;install","slug":"cilium-install","link":"/blog/categories/cilium-install/"},{"name":"cilium&#x2F;cilium","slug":"cilium-cilium","link":"/blog/categories/cilium-cilium/"},{"name":"loadbaleancer&#x2F;ipvs-install","slug":"loadbaleancer-ipvs-install","link":"/blog/categories/loadbaleancer-ipvs-install/"},{"name":"loadbaleancer&#x2F;ipvs","slug":"loadbaleancer-ipvs","link":"/blog/categories/loadbaleancer-ipvs/"},{"name":"cilium&#x2F;communication","slug":"cilium-communication","link":"/blog/categories/cilium-communication/"},{"name":"loadbaleancer&#x2F;metalLB-install","slug":"loadbaleancer-metalLB-install","link":"/blog/categories/loadbaleancer-metalLB-install/"},{"name":"loadbaleancer&#x2F;metalLB","slug":"loadbaleancer-metalLB","link":"/blog/categories/loadbaleancer-metalLB/"},{"name":"container&#x2F;overlayFS","slug":"container-overlayFS","link":"/blog/categories/container-overlayFS/"},{"name":"serivce&#x2F;clusterip","slug":"serivce-clusterip","link":"/blog/categories/serivce-clusterip/"},{"name":"ingress&#x2F;ingress","slug":"ingress-ingress","link":"/blog/categories/ingress-ingress/"},{"name":"container&#x2F;overlaynetwork","slug":"container-overlaynetwork","link":"/blog/categories/container-overlaynetwork/"},{"name":"vpccni&#x2F;vpccni-install","slug":"vpccni-vpccni-install","link":"/blog/categories/vpccni-vpccni-install/"},{"name":"vpccni&#x2F;vpccni2","slug":"vpccni-vpccni2","link":"/blog/categories/vpccni-vpccni2/"},{"name":"serivce&#x2F;install","slug":"serivce-install","link":"/blog/categories/serivce-install/"},{"name":"serivce&#x2F;nodeport","slug":"serivce-nodeport","link":"/blog/categories/serivce-nodeport/"},{"name":"vpccni&#x2F;vpccni","slug":"vpccni-vpccni","link":"/blog/categories/vpccni-vpccni/"}],"pages":[]}